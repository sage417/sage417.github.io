{"pages":[{"title":"about","text":"to be continue…","link":"/about/index.html"},{"title":"categories","text":"","link":"/categories/index.html"},{"title":"tags","text":"","link":"/tags/index.html"}],"posts":[{"title":"根据权限查询时避免角色切换的一种思路","text":"1. 问题背景权限系统现状UC权限系统基于角色访问控制技术RBAC（Role Based Access Control） 。具体来说，就是赋予用户某个角色，角色给与角色对应的权限能访问及操作不同范围的资源。 什么是数据权限代表一个角色对应某个权限所能操作的数据范围，比如gitlab组管理员能看到组下的所有项目代码，我们可以这样配置： 创建组管理员 分配给组管理员查看项目代码的权限 查看项目代码权限设置约束条件，约束为自己组下的项目 实际产生遇到的问题对绝大多数简单的系统来说一个用户对应一个系统只会有一个角色，一个角色只有一个数据权限范围（即使有多个，也可以合并成一个）。但是随着产品的功能迭代，用户的变更和系统设计的原因，总有一些特殊且重要的用户在同一个系统中拥有多个角色。在多角色和数据权限的组合下，一个用户可以拥有复数的数据权限范围。 考虑到实现复杂性，大多数系统选择使用角色切换的手段简化系统实现，同时对用户暴露出了他们不熟悉的角色这一概念，造成这些用户在系统使用中的各种不便。 本文重点讨论在避免角色切换的前提下，进行多角色数据范围查询的一种思路。 具体需要解决的需求我们的数据报表后台，不同的角色拥有不同的数据查看范围（不同角色所能看到的员工数据字段也各不相同），例如： 薪酬管理员：查看非高职级员工数据 高级薪酬管理员： 查看高职级员工数据 长期激励管理员：查看有长期激励员工数据 等等 简单来说，拥有长期激励管理员和高级薪酬管理员的用户能否直接看到高职级员工数据和长期激励员工数据？至少在直觉上是可行的。 2.多角色数据范围查询直觉的做法单角色单数据范围可以使用一句sql查询出结果，那多角色多数据范围是不是使用多句sql查询出结果合并就可以了？ 深入思考 多角色数据范围对行的影响 查询条件合并还是结果合并？ —-结果合并 如何排序？ —–外部排序，或先内部排序,limit,再外部排序 有重复数据怎么办？ —-使用groupby去重 查询性能有影响吗？—-有 具体体现： 1234567select * from ((select id, sortvalue from table_1 where t_name = &apos;a&apos; order by sortvalue desc limit 20) -- 先内部排序,limitunion all -- 结果合并(select id, sortvalue from table_1 where t_name = &apos;b&apos; order by sortvalue desc limit 20) -- 先内部排序,limitorder by sortvalue desc -- 外部排序) a group by id -- 使用groupby去重limit 10, 10 深入思考 多角色数据范围对列的影响 薪酬管理员： 查看员工薪酬字段 长期激励管理员：查看员工长期激励字段 如何解决？方法有很多！ 综合思考，给出一种解决方案123graph LR A(查询行及角色信息) --&gt; B(根据角色查询对应列字段) B --&gt; C(结果) 步骤： 查询多角色数据范围下的数据，附带角色信息 1234567select id, GROUP_CONCAT(a.role) as roles from ((select id, &apos;role_a&apos; as role from table_1 where sortvalue &gt; 10 order by `sortvalue` desc limit 2)union all(select id, &apos;role_b&apos; as role from table_1 where sortvalue &gt; 20 order by `sortvalue` desc limit 2) order by `sortvalue` desc ) a group by idlimit 0, 2 结果： id roles 1 薪酬管理员 5 薪酬管理员，长期激励管理员 根据每一行不同的角色，查询出可见的字段，例如id=1的行只能查看ROLE_B对应字段，而id=5的行可以看到ROLE_A,ROLE_B对应的两个角色的字段 3.总结和延伸多角色数据范围写操作？遍历角色直到找到满足条件的权限即可。 收获自己不行动，等于等着被别人安排哈哈 还有疑问？自己想。还可以点这里","link":"/2018/11/06/2018-11-06-biz/"},{"title":"再谈最长公共子串","text":"序言这次遇到贝壳花名的需求，需要使用最长公共子串对花名做校验。这种算法在面试题中算是必会题，各种四层循环，三层循环，两层循环的代码在我脑中闪过，但是今天就是要带你实现不一样的最长公共子串！ 教科书式实现使用动态规划，两层循环，使用二维数组存储状态，时间复杂度O(n^2^)，空间复杂度O(n^2^)或O(2n) 一张图解释原理： 12345678910111213141516 先 横 向 处 理 +---------------------------&gt; e a b c b c f + +---+---+---+---+---+---+---+ | a | 0 | 1 | 0 | 0 | 0 | 0 | 0 |纵 | +---------------------------+向 | b | 0 | 0 | 2 | 0 | 1 | 0 | 0 |累 | +---------------------------+加 | c | 0 | 0 | 0 | 3 | 0 | 2 | 0 | | +---------------------------+ | d | 0 | 0 | 0 | 0 | 0 | 0 | 0 | | +---------------------------+ | e | 1 | 0 | 0 | 0 | 0 | 0 | 0 | v +---+---+---+---+---+---+---+ e a b c b c f 优化空间复杂度至O(1)从上图可以发现，在纵向累加时实际只需要左上方的计数器即可, O(n^2^)的空间白白被浪费了,最优的空间复杂度应该是O(1)。那么该如何处理呢？ 一张图解释原理： e a b c b c f +---+ +---+---+---+---+---+---+---+ a | 0 | | 1 | 0 | 0 | 0 | 0 | 0 | a +-------+ +-----------------------+ b | 0 | 0 | | 2 | 0 | 1 | 0 | 0 | b +-----------+ --------------------+ c | 0 | 0 | 0 | | 3 | 0 | 2 | 0 | c +---------------+ ----------------+ d | 0 | 0 | 0 | 0 | | 0 | 0 | 0 | d +-------------------+ +-----------+ e | 1 | 0 | 0 | 0 | 0 | | 0 | 0 | e +---+---+---+---+---+-------+ +---+---+ e a b c b c f答案就是沿着等长对着线处理。 有意思的代码抽象大家可以根据上面思路写一下，一般会把算法分成两部分：处理长方形的左下部分和处理长方形的右上部分，两部分都是双层循环，时间复杂度和空间负载度已经变为了O(n^2^) ，O(1)。 肯定有人已经发现自己的代码处理左下角的代码和处理右上角的代码不能复用，一个是从中间向左下角处理，一个是从中间向右上角处理，明明很类似，但是就是没发合并。 那么有没有方法把这两部分处理抽象成公共代码呢？不卖关子了，直接上图： 12345678910111213141516 + e | +--+ e a b c b c f a | 1|+---+---+---+---+---+---+---+ +-----+ | 1 | 0 | 0 | 0 | 0 | 0 | a b | 0| 2| +-----------------------+ +--------+ | 2 | 0 | 1 | 0 | 0 | b c | 0| 0| 3| --------------------+ 翻 折 +-----------+ | 3 | 0 | 2 | 0 | c +------------&gt; b | 0| 1| 0| 0| ----------------+ +--------------+ | 0 | 0 | 0 | d c | 0| 0| 2| 0| 0| +-----------+ +--------------+ | 0 | 0 | e f | 0| 0| 0| 0| 0| +---+---+ +--------------+ a b c d e 如果你想使用公共代码同时实现处理左下角和右上角是不可能的了。所以你需要把右上角的三角翻折，然后你就得到了两个三角： 12345678910111213141516 + e | +--+ a | 1| +---+ +-----+a | 0 | b | 0| 2| +-------+ +--------+b | 0 | 0 | c | 0| 0| 3| +-----------+ +-----------+c | 0 | 0 | 0 | b | 0| 1| 0| 0| +---------------+ +--------------+d | 0 | 0 | 0 | 0 | c | 0| 0| 2| 0| 0| +-------------------+ +--------------+e | 1 | 0 | 0 | 0 | 0 | f | 0| 0| 0| 0| 0| +---+---+---+---+---+------ +--------------+ e a b c b c f a b c d e 这样就变成了处理两遍左下角了，代码也可以完美复用！！！ 最终实现我的完整思考过程已经分析完毕，这样沿对着线处理还有一个小小的优点：提前结束搜索。这一点大家可以自行思考，这里不做过多解释。 直接干货上场： 1234567891011121314151617181920212223242526272829public class Solution { /** * @param A: A string * @param B: A string * @return: the length of the longest common substring. */ public int longestCommonSubstring(String A, String B) { // write your code here char[] achars = A.toCharArray(), bchars = B.toCharArray(); return getMaxLength(bchars, achars, getMaxLength(achars, bchars, 0, 0), 1); } private static int getMaxLength(char[] s1, char[] s2, int maxLength, int startIndex) { for (int start = startIndex; start &lt; s1.length; start++) { int upper = Math.min(s2.length, s1.length - start); // if (upper &lt;= maxLength) break; //提前结束搜索 for (int currentLineLength = 0, x = 0, y = start; x &lt; upper; x++, y++) { if (s1[y] == s2[x]) maxLength = Math.max(maxLength, ++currentLineLength); else { // if (upper - x - 1 &lt;= maxLength) break; //提前结束搜索 currentLineLength = 0; }; } } return maxLength; }} 结尾怎么样，经历这次优化过程是否感觉自己对最长公共子串的认识又更深了一步呢？虽然不能保证是首创（也可能是首创？），但是这次一步一步真切思考优化直到获得成果让我无比兴奋。 说了这么多，我就是要给我们贝壳招聘开发组打个广告&gt;_&gt;，期待更多爱思考优秀的同学加入！ ![](/Users/sage/Desktop/屏幕快照 2018-11-10 13.33.20.png)","link":"/2018/11/11/2018-11-11-lcs/"},{"title":"代码外的生存之道-读书笔记-职业篇","text":"职业发展的驱动力应该来自自身，工作属于公司，职业生涯属于自己。 第一要务 拥有正确的心态大多数人形成的错误的心态: 认为在为公司打工，没有把自己的职业生涯当作生意来看待。铭记在心，开始积极主动的管理自己的职业生涯吧。 像企业一样思考自己能提供什么:自己的能力就是创造软件自己需要做什么: 持续不断地改进和完善自己的产品 传达自己的价值，和千万同行的独特之处 一头扎进工作不可能非同凡响，你应该： 专注于你正在提供怎样的服务， 以及如何营销这项服务； 想方设 提升你的服务； 思考你可以专注为哪一 特定类型的客户或行业提供特定的服务； 集中精力成为一位专家，专门为某一特定类型的客户提供专业的整体服务（ 记住， 作为一个软件开发 人员， 你 只有真正专注 于一类客户，才能找到非常好的工作）。 更好的宣传自己的产品，更好的找到你的客户 第二要务 设定自己的目标无论因为何种原因你没有为自己的职业生涯设定目标， 现在都是时候设定目标了。 不是明天， 也不是下周， 就是现在。 没有明确的方向， 你走的每一步都是徒劳的。 如何设定目标？先在心中树立一个大目标，然后分解成多个小目标 追踪你的目标定期核对自己的目标，必要时还要调整。 人际交往能力","link":"/2018/12/27/2018-12-27-softablity/"},{"title":"《重构-改善既有代码设计》读书笔记","text":"1.1 一个简单的例子一个计算顾客租赁影片费用的程序，能容易写成面条式的代码（流水账）：顾客类调用影片类和租赁时长计算费用 对机器来言只要能运行正确没有好代码和坏代码之分，但是对（维护的）人来说很难找到修改点，容易引入bug 1.2重构前先写测试保证重构结果利用单元测试保证重构正确性 1.3以微小的步伐修改程序，保证问题快速发现解决不要为修改变量名感到羞耻，只有写出人能理解的代码才是好程序员 重构完可能 性能变差，但同时会带来更多的机会来优化 1.4干货－用多态代替switchswitch需要关心具体条件，多态具有switch不具备的优势：不需要关心具体类型 2.1重构的定义重构：不改变运行结果下 提高理解性 降低修改成本 2.2重构的原因 代码结构的流失是累积性的，越难看懂代码设计意图，越难保护其设计 消除重复代码，方便修改 我们编写代码时很容易忘记读者的感受，造成他人时间的浪费 重构时犯错可以加深对代码意图的理解，可以帮助发现bug 好的结构设计是加速开发的根本 2.3重构的时机 添加功能时重构，在修改过程中把结构理清，也可以更简单的添加功能 修复错误时重构 复审代码时重构 2.4 重构的价值程序有两面价值，今天可以为你做什么和明天可以为你做什么为了满足明天的需要，你会遇到： 难以阅读，逻辑重复 添加新功能许修改以前代码 复杂的条件逻辑等代码 而你希望看到的是： 容易阅读，所有逻辑在唯一指定地点， 新的修改不会危及现有行为 尽可能简单表达逻辑条件 重构就是把程序转变为这些特征的工具 2.5如何告诉（对付）经理很多经理都是进度驱动，所以更加需要重构带来的好处，所以不要告诉他们 他们不会理解的 2.5.1引入间接层与重构的关系间接层优点： 允许逻辑共享 增加解释意图和实现的机会－多了类名和函数名 隔离变化 多态封装条件逻辑 2.5.2何时不应该重构 软件根本不工作 最后期限已近 未完成的重构可以称之为债务，迟早要还 2.5.3重构与预先设计的关系重构可以节约不必要的时间精力花在预先设计上，让软件设计向简化发展 3代码的坏味道 重复代码 过长函数 过大类，过长参数 修改一处程序的原因过多/一个原因修改过多的程序 数据依赖过多 重复的字段和参数 总是放在一起的字段 switch语句 平行继承关系 不是所有分支下都需要的临时变量 过度耦合调用链 不必要的委托 失血数据类 频繁重写父类方法 过多注释 4~12具体如何做自己看书吧","link":"/2018/09/08/2018-9-8-reading/"},{"title":"2018下半年书单","text":"经济金融类： 《斯坦福极简经济学》分微观和宏观经济学 没有教科书式的介绍 比较好读，推荐 《随机漫步的傻瓜》 经验之谈 观点可以接受 《买晾衣杆的小贩为何不会倒》 贴近生活 过于简单 不推荐 《富爸爸 穷爸爸》小白入门首选 推荐 《小岛经济学》 深入浅出 最后映射中国和美国关系 后面有点看不懂 推荐 小说类： 《解忧杂货店》 个人感觉一般 后半部分剧情我都猜到了 《人间失格》 遭受到了社会的毒打 很现实 有点致郁 心里承受能力低的不推荐 《三体》 第一部可以的，后面一部不如一部 但是比起其他网上大众喜欢的爽文好多了 技术类： 《java编程的逻辑》 温故而知新，基础好的不用看了 小白推荐 《kafka权威指南》正在看 但是真的写的不错 推荐 《mybatis从入门到精通》 一般般，只有入门吧 《netty权威指南》 一般般 《redis设计与实现》 真设计与实现 推荐 《深入理解java虚拟机》多读几遍也不为过 推荐 《SpringBoot实战》 读的比较早没影响了 不推荐 《算法图解》 程序=数据结构+算法 小白推荐（我就是小白） 心理类： 《乌合之众》 人在小范围为私利行动 在民族范畴会抛开私利做出些过于崇高或粗鲁的行为 推荐 《原生家庭》 一个人受童年家庭的影响是最大的，这本书能看懂是一回事，能做好是另一回事","link":"/2018/09/04/2018booklist/"},{"title":"[片段]使用TypeToken在运行期保存泛型信息","text":"一般来说可以使用getGenericSuperclass 获取子类范型信息，但是泛型有嵌套的话想获取完整信息还是有点复杂的。例如：Message&lt;List&gt; 有两个泛型信息。 guava中有强大的TypeToken帮助你保存复杂泛型信息，可以参考： 123ParameterizedTypeReference&lt;Message&lt;T&gt;&gt; responseTypeRef = ParameterizedTypeReferenceBuilder.fromTypeToken( new TypeToken&lt;Message&lt;T&gt;&gt;() {}.where(new TypeParameter&lt;T&gt;() {}, new TypeToken&lt;List&lt;OrgSugVOV1&gt;&gt;() {})); 如果需要在spring框架中使用，需要一个适配器： 123456789101112131415161718192021222324252627282930313233343536public class ParameterizedTypeReferenceBuilder { public static &lt;T&gt; ParameterizedTypeReference&lt;T&gt; fromTypeToken(TypeToken&lt;T&gt; typeToken) { return new TypeTokenParameterizedTypeReference&lt;&gt;(typeToken); } private static class TypeTokenParameterizedTypeReference&lt;T&gt; extends ParameterizedTypeReference&lt;T&gt; { private final Type type; private TypeTokenParameterizedTypeReference(TypeToken&lt;T&gt; typeToken) { this.type = typeToken.getType(); } @Override public Type getType() { return type; } @Override public boolean equals(Object obj) { return (this == obj || (obj instanceof ParameterizedTypeReference &amp;&amp; this.type.equals(((ParameterizedTypeReference&lt;?&gt;) obj).getType()))); } @Override public int hashCode() { return this.type.hashCode(); } @Override public String toString() { return \"ParameterizedTypeReference&lt;\" + this.type + \"&gt;\"; } }} 关于java的泛型我就不多做吐槽了。","link":"/2019/02/26/2019-02-26-java-genic-type/"},{"title":"[项目感悟] 读《再谈敏捷开发与延期风控》","text":"本人本身不太喜欢方法论，感觉都是套路，生搬硬套不适合自己，敏捷开发就是其中让我保持谨慎态度的方法论之一。 敏捷开发与Scrum对于一个项目来说，能够即快又好地完成当然是非常棒的，但是众所周知，受限于项目管理三要素：时间、质量、成本，只能折衷选择。因此「敏捷」作为一种方法论（虽然Agile自称为Culture）被提出，其中Scrum(/skrʌm/，一种球类比赛)是比较知名的实现类之一。 在Scum中，它主要强调将瀑布式开发流程转为多阶段小迭代，可以理解为CPU的多级流水线(Instruction pipeline)设计，流水线设计将CPU的计算逻辑拆分，实现了复用计算模块，进而提高了时钟频率，同时也引入了寄存器/分支预测等管理模块增加了复杂度。 类似于CPU流水线机制，敏捷开发本质是在保持时间、质量不变的情况下，通过投入管理成本降低开发过程的空转成本，进而提高时钟周期的方法。 用白话来说，可以把软件开放比作流水车间，把PM，SE比作流水线工人。 我见过的的假敏捷然而到了现实，由于各种原因，却很容易成为假敏捷 将工位的隔栏拆开变成网吧“敏捷岛” 强行将Release计划拆成一个月一版，将Sprint拆成2周就看作快速迭代，照着人月神话反着搞 招聘一堆无责任心的开发让你去“敏捷”，永远无法实现“全功能部队” 客户难沟通，PO低估工作量，SE设计缺陷，编码质量低等原因，最终导致延期上述任何一个问题，都可能导致最终项目一锅粥，导致高层焦虑，中层跑路，底层混日子的结果。 敏捷能够提供强大高效的方法论，但是前提是需要本身基础过硬的团队，敏捷只能帮助存在进步瓶颈的团队。如果项目已经空心化，债务多，这不是敏捷方法论应该解决的问题。","link":"/2019/04/01/2019-04-01-[项目感悟] 读《再谈敏捷开发与延期风控》/"},{"title":"[片段] Mybatis ResultSetHandler 实践","text":"这次拦截的方法是handleResultSets(Statement stmt)，用来批量解密用@Encrypted注解的String字段，可能还有一些坑。 12345678910111213141516171819202122232425262728293031323334353637@Overridepublic List&lt;Object&gt; handleResultSets(Statement stmt) throws SQLException { ErrorContext.instance().activity(\"handling results\").object(mappedStatement.getId()); final List&lt;Object&gt; multipleResults = new ArrayList&lt;Object&gt;(); int resultSetCount = 0; ResultSetWrapper rsw = getFirstResultSet(stmt); List&lt;ResultMap&gt; resultMaps = mappedStatement.getResultMaps(); int resultMapCount = resultMaps.size(); validateResultMapsCount(rsw, resultMapCount); while (rsw != null &amp;&amp; resultMapCount &gt; resultSetCount) { ResultMap resultMap = resultMaps.get(resultSetCount); handleResultSet(rsw, resultMap, multipleResults, null); rsw = getNextResultSet(stmt); cleanUpAfterHandlingResultSet(); resultSetCount++; } String[] resultSets = mappedStatement.getResultSets(); if (resultSets != null) { while (rsw != null &amp;&amp; resultSetCount &lt; resultSets.length) { ResultMapping parentMapping = nextResultMaps.get(resultSets[resultSetCount]); if (parentMapping != null) { String nestedResultMapId = parentMapping.getNestedResultMapId(); ResultMap resultMap = configuration.getResultMap(nestedResultMapId); handleResultSet(rsw, resultMap, null, parentMapping); } rsw = getNextResultSet(stmt); cleanUpAfterHandlingResultSet(); resultSetCount++; } } return collapseSingleResultList(multipleResults);} 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123package app.pooi.common.encrypt;import app.pooi.common.encrypt.anno.CipherSpi;import app.pooi.common.encrypt.anno.Encrypted;import lombok.Getter;import org.apache.ibatis.executor.resultset.ResultSetHandler;import org.apache.ibatis.plugin.*;import org.apache.ibatis.reflection.MetaObject;import org.apache.ibatis.reflection.SystemMetaObject;import java.lang.reflect.Field;import java.sql.Statement;import java.util.*;import java.util.function.Function;import java.util.logging.Logger;import java.util.stream.Collectors;@Intercepts({ @Signature(type = ResultSetHandler.class, method = \"handleResultSets\", args = {Statement.class}),})public class EncryptInterceptor implements Interceptor { private static final Logger logger = Logger.getLogger(EncryptInterceptor.class.getName()); private CipherSpi cipherSpi; public EncryptInterceptor(CipherSpi cipherSpi) { this.cipherSpi = cipherSpi; } @Override public Object intercept(Invocation invocation) throws Throwable { final Object proceed = invocation.proceed(); if (proceed == null) { return proceed; } List&lt;?&gt; results = (List&lt;?&gt;) proceed; if (results.isEmpty()) { return proceed; } final Object first = results.iterator().next(); final Class&lt;?&gt; modelClazz = first.getClass(); final List&lt;String&gt; fieldsNeedDecrypt = Arrays.stream(modelClazz.getDeclaredFields()) .filter(f -&gt; f.getAnnotation(Encrypted.class) != null) .filter(f -&gt; { boolean isString = f.getType() == String.class; if (!isString) { logger.warning(f.getName() + \"is not String, actual type is \" + f.getType().getSimpleName() + \" ignored\"); } return isString; }) .map(Field::getName) .collect(Collectors.toList()); final List&lt;List&lt;String&gt;&gt; partition = partition(fieldsNeedDecrypt, 20); for (Object r : results) { final MetaObject metaObject = SystemMetaObject.forObject(r); for (List&lt;String&gt; fields : partition) { final Map&lt;String, String&gt; fieldValueMap = fields.stream().collect(Collectors.toMap(Function.identity(), f -&gt; (String) metaObject.getValue(f))); final ArrayList&lt;String&gt; values = new ArrayList&lt;&gt;(fieldValueMap.values()); Map&lt;String, String&gt; decryptValues = cipherSpi.decrypt(values); fieldValueMap.entrySet() .stream() .map(e -&gt; Tuple2.of(e.getKey(), decryptValues.getOrDefault(e.getValue(), \"\"))) .forEach(e -&gt; metaObject.setValue(e.getT1(), e.getT2())); } } return results; } private &lt;T&gt; List&lt;List&lt;T&gt;&gt; partition(List&lt;T&gt; list, int batchCount) { if (!(batchCount &gt; 0)) { throw new IllegalArgumentException(\"batch count must greater than zero\"); } List&lt;List&lt;T&gt;&gt; partitionList = new ArrayList&lt;&gt;(list.size() / (batchCount + 1)); for (int i = 0; i &lt; list.size(); i += batchCount) { partitionList.add(list.stream().skip(i).limit(batchCount).collect(Collectors.toList())); } return partitionList; } @Override public Object plugin(Object target) { return Plugin.wrap(target, this); } @Override public void setProperties(Properties properties) { }}@Getterclass Tuple2&lt;T1, T2&gt; { private final T1 t1; private final T2 t2; Tuple2(T1 t1, T2 t2) { this.t1 = t1; this.t2 = t2; } static &lt;T1, T2&gt; Tuple2&lt;T1, T2&gt; of(T1 t1, T2 t2) { return new Tuple2&lt;&gt;(t1, t2); }}","link":"/2019/01/09/2019-01-09-Mybatis ResultSetHandler/"},{"title":"[片段] Mybatis ResultSetHandler 实践-续","text":"这次拦截的方法是handleResultSets(Statement stmt)，用来批量解密用@Encrypted注解的String字段。 上次的局限是只能批量解密一个对象的所有加密字段，对批量数据来说稍显不足，这个主要改进了这一点。 12345678910111213141516171819202122232425262728293031323334353637@Overridepublic List&lt;Object&gt; handleResultSets(Statement stmt) throws SQLException { ErrorContext.instance().activity(\"handling results\").object(mappedStatement.getId()); final List&lt;Object&gt; multipleResults = new ArrayList&lt;Object&gt;(); int resultSetCount = 0; ResultSetWrapper rsw = getFirstResultSet(stmt); List&lt;ResultMap&gt; resultMaps = mappedStatement.getResultMaps(); int resultMapCount = resultMaps.size(); validateResultMapsCount(rsw, resultMapCount); while (rsw != null &amp;&amp; resultMapCount &gt; resultSetCount) { ResultMap resultMap = resultMaps.get(resultSetCount); handleResultSet(rsw, resultMap, multipleResults, null); rsw = getNextResultSet(stmt); cleanUpAfterHandlingResultSet(); resultSetCount++; } String[] resultSets = mappedStatement.getResultSets(); if (resultSets != null) { while (rsw != null &amp;&amp; resultSetCount &lt; resultSets.length) { ResultMapping parentMapping = nextResultMaps.get(resultSets[resultSetCount]); if (parentMapping != null) { String nestedResultMapId = parentMapping.getNestedResultMapId(); ResultMap resultMap = configuration.getResultMap(nestedResultMapId); handleResultSet(rsw, resultMap, null, parentMapping); } rsw = getNextResultSet(stmt); cleanUpAfterHandlingResultSet(); resultSetCount++; } } return collapseSingleResultList(multipleResults);} 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130package app.pooi.common.encrypt;import app.pooi.common.encrypt.anno.CipherSpi;import app.pooi.common.encrypt.anno.Encrypted;import lombok.Getter;import org.apache.ibatis.executor.resultset.ResultSetHandler;import org.apache.ibatis.plugin.*;import org.apache.ibatis.reflection.MetaObject;import org.apache.ibatis.reflection.SystemMetaObject;import java.lang.reflect.Field;import java.sql.Statement;import java.util.*;import java.util.function.Function;import java.util.logging.Logger;import java.util.stream.Collectors;@Intercepts({ @Signature(type = ResultSetHandler.class, method = \"handleResultSets\", args = {Statement.class}),})public class DecryptInterceptor implements Interceptor { private static final Logger logger = Logger.getLogger(DecryptInterceptor.class.getName()); private CipherSpi cipherSpi; public DecryptInterceptor(CipherSpi cipherSpi) { this.cipherSpi = cipherSpi; } @Override public Object intercept(Invocation invocation) throws Throwable { final Object proceed = invocation.proceed(); if (proceed == null) { return proceed; } List&lt;?&gt; results = (List&lt;?&gt;) proceed; if (results.isEmpty()) { return proceed; } final Object first = results.iterator().next(); final Class&lt;?&gt; modelClazz = first.getClass(); final List&lt;String&gt; decryptFields = getDecryptFields(modelClazz); if (decryptFields.isEmpty()) { return proceed; } final List&lt;List&lt;String&gt;&gt; secret = Flux.fromIterable(results) .map(SystemMetaObject::forObject) .flatMapIterable(mo -&gt; decryptFields.stream().map(mo::getValue).collect(Collectors.toList())) .cast(String.class) .buffer(1000) .collectList() .block(); final Map&lt;String, String&gt; secretMap = secret.stream() .map(secrets -&gt; { try { return cipherSpi.batchDecrypt(secrets); } catch (Exception e) { e.printStackTrace(); return Maps.&lt;String, String&gt;newHashMap(); } }).reduce(Maps.newHashMap(), (m1, m2) -&gt; { m1.putAll(m2); return m1; }); secretMap.put(\"\", \"0\"); for (Object r : results) { final MetaObject metaObject = SystemMetaObject.forObject(r); decryptFields.forEach(f -&gt; metaObject.setValue(f, secretMap.get(metaObject.getValue(f)))); } return results; } @NotNull private List&lt;String&gt; getDecryptFields(Class&lt;?&gt; modelClazz) { return Arrays.stream(modelClazz.getDeclaredFields()) .filter(f -&gt; f.getAnnotation(Decrypted.class) != null) .filter(f -&gt; { boolean isString = f.getType() == String.class; if (!isString) { logger.warning(f.getName() + \"is not String, actual type is \" + f.getType().getSimpleName() + \" ignored\"); } return isString; }) .map(Field::getName) .collect(Collectors.toList()); } @Override public Object plugin(Object target) { return Plugin.wrap(target, this); } @Override public void setProperties(Properties properties) { }}@Getterclass Tuple2&lt;T1, T2&gt; { private final T1 t1; private final T2 t2; Tuple2(T1 t1, T2 t2) { this.t1 = t1; this.t2 = t2; } static &lt;T1, T2&gt; Tuple2&lt;T1, T2&gt; of(T1 t1, T2 t2) { return new Tuple2&lt;&gt;(t1, t2); }}","link":"/2019/04/04/2019-04-04-Mybatis ResultSetHandler续/"},{"title":"[片段] Java收集方法参数+Spring DataBinder","text":"收集参数目前是使用了spring aop 来拦截方法调用，把方法参数包装成Map形式 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758@Target(ElementType.METHOD)@Retention(RetentionPolicy.RUNTIME)@Documentedpublic @interface CollectArguments {}@Aspectpublic class ArgumentsCollector { private static final ThreadLocal&lt;Map&lt;String, Object&gt;&gt; ARGUMENTS = ThreadLocal.withInitial(ImmutableMap::of); static Map&lt;String, Object&gt; getArgs() { return ARGUMENTS.get(); } private Object[] args(Object[] args, int exceptLength) { if (exceptLength == args.length) { return args; } return Arrays.copyOf(args, exceptLength); } @Pointcut(\"@annotation(CollectArguments)\") void collectArgumentsAnnotationPointCut() { } @Before(\"collectArgumentsAnnotationPointCut()\") public void doAccessCheck(JoinPoint joinPoint) { final String[] parameterNames = ((MethodSignature) joinPoint.getSignature()).getParameterNames(); final Object[] args = args(joinPoint.getArgs(), parameterNames.length); ARGUMENTS.set(Collections.unmodifiableMap((IntStream.range(0, parameterNames.length - 1) .mapToObj(idx -&gt; Tuple2.of(parameterNames[idx], args[idx])) .collect(HashMap::new, (m, t) -&gt; m.put(t.getT1(), t.getT2()), HashMap::putAll)))); } @After(\"collectArgumentsAnnotationPointCut()\") public void remove() { ARGUMENTS.remove(); } @Data private static class Tuple2&lt;T1, T2&gt; { private T1 t1; private T2 t2; Tuple2(T1 t1, T2 t2) { this.t1 = t1; this.t2 = t2; } public static &lt;T1, T2&gt; Tuple2&lt;T1, T2&gt; of(T1 t1, T2 t2) { return new Tuple2&lt;&gt;(t1, t2); } }} 通过Map构造对象123456789101112public class BinderUtil { BinderUtil() { } @SuppressWarnings(\"unchecked\") public static &lt;T&gt; T getTarget(Class&lt;T&gt; beanClazz) { final DataBinder binder = new DataBinder(BeanUtils.instantiate(beanClazz)); binder.bind(new MutablePropertyValues(ArgumentsCollector.getArgs())); return (T) binder.getTarget(); }}","link":"/2019/01/22/2019-01-22-Spring_aop_argument_collector/"},{"title":"[项目] 根据权限查询时避免角色切换遇到的坑","text":"前情概要 1. 问题背景使用多个角色查询列表时，会遇到两个维度的不同点： 行维度：多个角色能够看到行的并集，sql需要多次查询取并集，之后还要去重分页排序 列维度：如果不同角色可见列不同，计算出当前行能看到列的并集 举一个例子： 假设存在一个登录员工拥有两个角色： 长期激励负责人：能看到拥有长期激励的人（行维度），能看到基本信息和长期激励信息（列维度） 薪酬负责人：能看到低职级的人（行维度），能看到基本信息和薪酬信息（列维度） 那么，在列表中他能看见： 基本信息 薪酬信息 长期激励信息 低职级/无长期激励 √ √ x 低职级/长期激励 √ √ √ 高职级/无长期激励 x x x 高职级/长期激励 √ x √ 2. 实际遇到的问题（困难重重）基本思路已经在前期概要里介绍，本人已经实践了一段时间，挖了两个深坑正在解决中。 性能问题（已解决）最开始的实现中数据是一条一条读取的，同时薪酬字段属于加密信息，使用了第三方微服务提供解密，读取字段多+解密字段多 导致了在百条分页的情况下接口在超时的边缘不断试探。。。 解决方案： 合并查询sql,批量查询数据 合并解密请求,批量调用解密微服务 因为之前为了方便我们解密使用了mybatis的TypeHandler做到字段隐式加解密，目前我们的做法是对于单条数据的加解密，还是保持原来的typeHandler做法，而对批量数据处理，重新写一套数据实体，同时使用mybatis的拦截器对查询的批量数据做批量解密的处理。具体做法可以参见我的另一片文章：【片段】 Mybatis ResultSetHandler 实践-续 批量查询带来的问题批量查询返回的列表中列字段都是一致的，而我们的需求是不同的行能看见不同的列字段，把批量查询出来的列表直接返回是有问题的，这个问题因为疏忽导致了线上的一次故障。 所以目前的思路是先做一次数据批量预取，之后在对列字段做处理，隐藏掉不能看见的字段。 3. 总结没有想到当时想解决权限查询时避免角色切换这个问题时会遇到这么多困难，想法是正确的，在实际执行时还是困难重重。值得欣慰的在最开始的时候思路和方向都是正确的，同时也把其中遇到的各种问题和心得记录了下来，经过层层积累，才到达现在的高度。 []: https://blog.yamato.moe/2018/11/06/2018-11-06-biz/ “根据权限查询时避免角色切换的一种思路”[]: https://blog.yamato.moe/2019/04/04/Mybatis%20ResultSetHandler_2019-04-04%20%E7%BB%AD/ “【片段】 Mybatis ResultSetHandler 实践-续”[]: https://blog.yamato.moe/2019/01/09/Mybatis%20ResultSetHandler_2019-01-09/ “【片段】 Mybatis ResultSetHandler 实践”","link":"/2019/05/17/2019-05-17-根据权限查询时避免角色切换遇到的坑/"},{"title":"季度总结 如何管理自我时间","text":"最近这个季度最近比较忙，算了下自己可以利用的空闲时间，忙时一天可能只有1到3个小时空闲时间，甚至一天没有空余时间。一周大约只有30小时时间是可以利用的，如果算上玩手机时间只会更短，如果再把这些时间再浪费掉，可能最近半年的成长就只有一些项目经验而已了。所幸平常关注的博客和《软技能》这本书里有提及一些关于自我时间管理相关内容，我简单实践了2个月，分享下这方面的心得感悟。 计划和休息《软技能》中作者是如何做计划的？ 季度计划 明确宏观目标，以及如何实现 月计划 估算当月能完成多少工作量 周计划 为每周安排必须完成的强制性任务 日计划 排除干扰，按需调整 休息 每隔一段时间休息，只做强制性任务 很惭愧，工作刚开始一两年我少有计划，单纯凭借好奇心学习到了不少东西。大约从去年写年度总结开始才刚刚做一些计划。目前我使用微软to-do跟踪自己的计划进度和deadline效果显著，治好了我的拖延症。 简单来说《软技能》中阐述的几个观点我感觉十分受益： 要有计划 完成计划时保持专注 使用番茄工作法可以保证保证一段时间的专注， 同时还可以确定自己的工作效率， 总结不足提高自身效率，从而帮助自己精确且高效的指定计划 只有你完成了计划的工作，接下来的休息时间才能安心 这边年读书情况这半年看的书比较少，但是刷题和博客总结写的多了些 技术类：《sql反模式》推荐， 应该叫数据库结构设计的最佳实践《软技能，代码之外的生存指南》 有很多事比代码更重要的多，推荐 心理类：《你的灯亮着吗》 解决问题前，先要搞清楚问题的本质。 一般般","link":"/2019/09/24/2019-06-24自我时间管理/"},{"title":"[项目] 多角色权限展示数据的一种实现","text":"多角色权限如果遇到不同角色能看到不同的列可以怎么做 逐行读取 最简单的解决方法，实现简单。但是在微服务中调用接口次数太多，性能很差。 批量读取 实现较复杂，但是性能好很多，下面主要介绍这种方法的思路 批量读取以分页读取数据为例： 读取第一页数据，包含需要展示数据的id和所属权限（多个） 为什么需要所属权限这个字段呢？ 因为决定能否看到这行是有你所拥有的所有权限决定的，而决定能否看到哪个列是由这行所拥有的权限决定的。 如何获取该行所拥有的权限呢，我的做法是分不同的权限查询结果通过union 组合起来 将第一页数据原始顺序保存， 然后按行拥有权限分组 记录原始顺序是因为后面分组后会打乱， 为什么要分组？分组后同样的查询才能聚合在一起，可以简化代码 根据权限分组多次查询所需要的字段，然后将查询结果合并 这里我使用的graphql来选择需要查询的字段 最后还原成原来的顺序 可以使用guava Ordering工具类方便生成Compartor []: https://blog.yamato.moe/2018/11/06/2018-11-06-biz/ “根据权限查询时避免角色切换的一种思路”[]: https://blog.yamato.moe/2019/04/04/Mybatis%20ResultSetHandler_2019-04-04%20%E7%BB%AD/ “【片段】 Mybatis ResultSetHandler 实践-续”[]: https://blog.yamato.moe/2019/01/09/Mybatis%20ResultSetHandler_2019-01-09/ “【片段】 Mybatis ResultSetHandler 实践”","link":"/2019/07/29/2019-07-29多角色权限展示数据的一种实现/"},{"title":"LeetCode 最长回文子串算法","text":"Manacher 算法 容易理解，实现起来也没什么大坑，复杂度还是 O(n)的， 花半个小时实现下很有意思 12345678910111213141516171819202122232425262728293031323334353637class Solution { public String longestPalindrome(String s) { StringBuilder sb = new StringBuilder(2 + 2 * s.length()); sb.append(&quot;^&quot;); for (int i = 0; i &lt; s.length(); i++) { sb.append(&quot;#&quot;).append(s.charAt(i)); } sb.append(&quot;#$&quot;); String s2 = sb.toString(); int maxStart = 1, max = 0, rC = 1, rR = 1; int[] p = new int[s2.length()]; for (int i = 1; i &lt; s2.length() - 1; i++ ) { p[i] = i &lt; rR ? Math.min(p[2 * rC - i], rR - i) : 0; while (s2.charAt(i+p[i]+1) == s2.charAt(i - p[i] - 1)) { p[i] = p[i]+1; } if (i + p[i] &gt; rR) { rC = i; rR = i + p[i]; } if (p[i] &gt; max) { maxStart = i - p[i]; max = p[i]; } } return s2.substring(maxStart,maxStart + 2*max+1).replace(&quot;#&quot;, &quot;&quot;); }}","link":"/2019/08/10/2019-08-10序列算法 - 最长回文子串/"},{"title":"LeetCode两个经典的排序算法","text":"LeetCode两个经典的排序算法这回是标题党， 记录下两个分而治之的排序算法（手写），分而治之的算法很容易改造成并行算法，肯定是未来的潮流， leetcode已通过， 两个算法都使用了原地(inplace)更新。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263class Solution { public List&lt;Integer&gt; sortArray(int[] nums) { // merge(nums, 0, nums.length - 1); sort(nums, 0, nums.length - 1); return IntStream.of(nums).boxed().collect(Collectors.toList()); } //-------------------归并排序-------------------// private void merge(int[] nums, int start, int end) { if (start &gt;= end) return; int midIdx = start + (end - start) / 2; merge(nums, start, midIdx); merge(nums, midIdx+1, end); concat(nums, start, midIdx, end); } private void concat(int[] nums, int start, int midIdx, int end) { int[] tmp = new int[end - start + 1]; int lp = start, rp = midIdx + 1, i = 0; while (lp &lt;= midIdx &amp;&amp; rp &lt;= end) { tmp[i++] = nums[lp] &lt; nums[rp] ? nums[lp++] : nums[rp++]; } while (lp &lt;= midIdx) { tmp[i++] = nums[lp++]; } while (rp &lt;= end) { tmp[i++] = nums[rp++]; } System.arraycopy(tmp, 0, nums, start, tmp.length); } //-------------------快速排序-------------------// private void sort(int[] nums, int start, int end) { if (start &gt;= end) return; int bIdx = partition(nums, start, end); sort(nums, start, bIdx - 1); sort(nums, bIdx + 1, end); } private int partition(int[] nums, int start, int end) { int idx = start, base = nums[end]; for (int i = start; i &lt; end; i++) { if (nums[i] &lt; base) { swap(nums, idx++, i); } } swap(nums, idx, end); return idx; } private void swap(int[] nums, int i, int j) { int tmp = nums[i]; nums[i] = nums[j]; nums[j] = tmp; }}","link":"/2019/11/20/2019-11-20 两个经典的排序算法/"},{"title":"缓存专题(三) Write Through（读穿 写穿）策略","text":"这个策略的核心原则是用户只与缓存打交道，由缓存和数据库通信，写入或者读取数据。这就好比你在汇报工作的时候只对你的直接上级汇报，再由你的直接上级汇报给他的上级，你是不能越级汇报的。 Write Through 的策略是这样的：先查询要写入的数据在缓存中是否已经存在，如果已经存在，则更新缓存中的数据，并且由缓存组件同步更新到数据库中，如果缓存中数据不存在，我们把这种情况叫做“Write Miss（写失效）”。 一般来说，我们可以选择两种“Write Miss”方式：一个是“Write Allocate（按写分配）”，做法是写入缓存相应位置，再由缓存组件同步更新到数据库中；另一个是“No-write allocate（不按写分配）”，做法是不写入缓存中，而是直接更新到数据库中。 在 Write Through 策略中，我们一般选择“No-write allocate”方式，原因是无论采用哪种“Write Miss”方式，我们都需要同步将数据更新到数据库中，而“No-write allocate”方式相比“Write Allocate”还减少了一次缓存的写入，能够提升写入的性能。 Read Through 策略就简单一些，它的步骤是这样的：先查询缓存中数据是否存在，如果存在则直接返回，如果不存在，则由缓存组件负责从数据库中同步加载数据。 下面是 Read Through/Write Through 策略的示意图： Read Through/Write Through 策略的特点是由缓存节点而非用户来和数据库打交道，在我们开发过程中相比 Cache Aside 策略要少见一些，原因是我们经常使用的分布式缓存组件，无论是 Memcached 还是 Redis 都不提供写入数据库，或者自动加载数据库中的数据的功能。而我们在使用本地缓存的时候可以考虑使用这种策略，比如说在上一节中提到的本地缓存 Guava Cache 中的 Loading Cache 就有 Read Through 策略的影子。 我们看到 Write Through 策略中写数据库是同步的，这对于性能来说会有比较大的影响，因为相比于写缓存，同步写数据库的延迟就要高很多了。那么我们可否异步地更新数据库？这就是我们接下来要提到的“Write Back”策略。 Write Back（写回）策略这个策略的核心思想是在写入数据时只写入缓存，并且把缓存块儿标记为“脏”的。而脏块儿只有被再次使用时才会将其中的数据写入到后端存储中。 需要注意的是，在“Write Miss”的情况下，我们采用的是“Write Allocate”的方式，也就是在写入后端存储的同时要写入缓存，这样我们在之后的写请求中都只需要更新缓存即可，而无需更新后端存储了，我将 Write back 策略的示意图放在了下面： 发现了吗？其实这种策略不能被应用到我们常用的数据库和缓存的场景中，它是计算机体系结构中的设计，比如我们在向磁盘中写数据时采用的就是这种策略。无论是操作系统层面的 Page Cache，还是日志的异步刷盘，亦或是消息队列中消息的异步写入磁盘，大多采用了这种策略。因为这个策略在性能上的优势毋庸置疑，它避免了直接写磁盘造成的随机写问题，毕竟写内存和写磁盘的随机 I/O 的延迟相差了几个数量级呢。 但因为缓存一般使用内存，而内存是非持久化的，所以一旦缓存机器掉电，就会造成原本缓存中的脏块儿数据丢失。所以你会发现系统在掉电之后，之前写入的文件会有部分丢失，就是因为 Page Cache 还没有来得及刷盘造成的。 当然，你依然可以在一些场景下使用这个策略，在使用时，我想给你的落地建议是：你在向低速设备写入数据的时候，可以在内存里先暂存一段时间的数据，甚至做一些统计汇总，然后定时地刷新到低速设备上。比如说，你在统计你的接口响应时间的时候，需要将每次请求的响应时间打印到日志中，然后监控系统收集日志后再做统计。但是如果每次请求都打印日志无疑会增加磁盘 I/O，那么不如把一段时间的响应时间暂存起来，经过简单的统计平均耗时，每个耗时区间的请求数量等等，然后定时地，批量地打印到日志中。","link":"/2019/12/01/2019-12-01-Write Through（读穿 写穿）策略/"},{"title":"缓存专题(二) Cache Aside（旁路缓存）策略","text":"我们来考虑一种最简单的业务场景，比方说在你的电商系统中有一个用户表，表中只有 ID 和年龄两个字段，缓存中我们以 ID 为 Key 存储用户的年龄信息。那么当我们要把 ID 为 1 的用户的年龄从 19 变更为 20，要如何做呢？ 你可能会产生这样的思路：先更新数据库中 ID 为 1 的记录，再更新缓存中 Key 为 1 的数据。 这个思路会造成缓存和数据库中的数据不一致。比如，A 请求将数据库中 ID 为 1 的用户年龄从 19 变更为 20，与此同时，请求 B 也开始更新 ID 为 1 的用户数据，它把数据库中记录的年龄变更为 21，然后变更缓存中的用户年龄为 21。紧接着，A 请求开始更新缓存数据，它会把缓存中的年龄变更为 20。此时，数据库中用户年龄是 21，而缓存中的用户年龄却是 20。 为什么产生这个问题呢？因为变更数据库和变更缓存是两个独立的操作，而我们并没有对操作做任何的并发控制。那么当两个线程并发更新它们的时候，就会因为写入顺序的不同造成数据的不一致。 另外，直接更新缓存还存在另外一个问题就是丢失更新。还是以我们的电商系统为例，假如电商系统中的账户表有三个字段：ID、户名和金额，这个时候缓存中存储的就不只是金额信息，而是完整的账户信息了。当更新缓存中账户金额时，你需要从缓存中查询完整的账户数据，把金额变更后再写入到缓存中。 这个过程中也会有并发的问题，比如说原有金额是 20，A 请求从缓存中读到数据，并且把金额加 1，变更成 21，在未写入缓存之前又有请求 B 也读到缓存的数据后把金额也加 1，也变更成 21，两个请求同时把金额写回缓存，这时缓存里面的金额是 21，但是我们实际上预期是金额数加 2，这也是一个比较大的问题。 那我们要如何解决这个问题呢？其实，我们可以在更新数据时不更新缓存，而是删除缓存中的数据，在读取数据时，发现缓存中没了数据之后，再从数据库中读取数据，更新到缓存中。 这个策略就是我们使用缓存最常见的策略，Cache Aside 策略（也叫旁路缓存策略），这个策略数据以数据库中的数据为准，缓存中的数据是按需加载的。它可以分为读策略和写策略，其中读策略的步骤是： 从缓存中读取数据； 如果缓存命中，则直接返回数据； 如果缓存不命中，则从数据库中查询数据； 查询到数据后，将数据写入到缓存中，并且返回给用户。 写策略的步骤是： 更新数据库中的记录； 删除缓存记录。 你也许会问了，在写策略中，能否先删除缓存，后更新数据库呢？答案是不行的，因为这样也有可能出现缓存数据不一致的问题，我以用户表的场景为例解释一下。 假设某个用户的年龄是 20，请求 A 要更新用户年龄为 21，所以它会删除缓存中的内容。这时，另一个请求 B 要读取这个用户的年龄，它查询缓存发现未命中后，会从数据库中读取到年龄为 20，并且写入到缓存中，然后请求 A 继续更改数据库，将用户的年龄更新为 21，这就造成了缓存和数据库的不一致。 那么像 Cache Aside 策略这样先更新数据库，后删除缓存就没有问题了吗？其实在理论上还是有缺陷的。假如某个用户数据在缓存中不存在，请求 A 读取数据时从数据库中查询到年龄为 20，在未写入缓存中时另一个请求 B 更新数据。它更新数据库中的年龄为 21，并且清空缓存。这时请求 A 把从数据库中读到的年龄为 20 的数据写入到缓存中，造成缓存和数据库数据不一致。 不过这种问题出现的几率并不高，原因是缓存的写入通常远远快于数据库的写入，所以在实际中很难出现请求 B 已经更新了数据库并且清空了缓存，请求 A 才更新完缓存的情况。而一旦请求 A 早于请求 B 清空缓存之前更新了缓存，那么接下来的请求就会因为缓存为空而从数据库中重新加载数据，所以不会出现这种不一致的情况。 Cache Aside 策略是我们日常开发中最经常使用的缓存策略，不过我们在使用时也要学会依情况而变。比如说当新注册一个用户，按照这个更新策略，你要写数据库，然后清理缓存（当然缓存中没有数据给你清理）。可当我注册用户后立即读取用户信息，并且数据库主从分离时，会出现因为主从延迟所以读不到用户信息的情况。 而解决这个问题的办法恰恰是在插入新数据到数据库之后写入缓存，这样后续的读请求就会从缓存中读到数据了。并且因为是新注册的用户，所以不会出现并发更新用户信息的情况。 Cache Aside 存在的最大的问题是当写入比较频繁时，缓存中的数据会被频繁地清理，这样会对缓存的命中率有一些影响。如果你的业务对缓存命中率有严格的要求，那么可以考虑两种解决方案： 一种做法是在更新数据时也更新缓存，只是在更新缓存前先加一个分布式锁，因为这样在同一时间只允许一个线程更新缓存，就不会产生并发问题了。当然这么做对于写入的性能会有一些影响； 另一种做法同样也是在更新数据时更新缓存，只是给缓存加一个较短的过期时间，这样即使出现缓存不一致的情况，缓存的数据也会很快地过期，对业务的影响也是可以接受。","link":"/2019/12/01/2019-12-01-Cache Aside（旁路缓存）策略/"},{"title":"缓存专题(一) 缓存概述","text":"缓存分类在我们日常开发中，常见的缓存主要就是静态缓存、分布式缓存和热点本地缓存这三种。 静态缓存在 Web 1.0 时期是非常著名的，它一般通过生成 Velocity 模板或者静态 HTML 文件来实现静态缓存，在 Nginx 上部署静态缓存可以减少对于后台应用服务器的压力。例如，我们在做一些内容管理系统的时候，后台会录入很多的文章，前台在网站上展示文章内容，就像新浪，网易这种门户网站一样。 当然，我们也可以把文章录入到数据库里面，然后前端展示的时候穿透查询数据库来获取数据，但是这样会对数据库造成很大的压力。即使我们使用分布式缓存来挡读请求，但是对于像日均 PV 几十亿的大型门户网站来说，基于成本考虑仍然是不划算的。 所以我们的解决思路是每篇文章在录入的时候渲染成静态页面，放置在所有的前端 Nginx 或者 Squid 等 Web 服务器上，这样用户在访问的时候会优先访问 Web 服务器上的静态页面，在对旧的文章执行一定的清理策略后，依然可以保证 99% 以上的缓存命中率。 这种缓存只能针对静态数据来缓存，对于动态请求就无能为力了。那么我们如何针对动态请求做缓存呢？这时你就需要分布式缓存了。 分布式缓存的大名可谓是如雷贯耳了，我们平时耳熟能详的 Memcached、Redis 就是分布式缓存的典型例子。它们性能强劲，通过一些分布式的方案组成集群可以突破单机的限制。所以在整体架构中，分布式缓存承担着非常重要的角色。 对于静态的资源的缓存你可以选择静态缓存，对于动态的请求你可以选择分布式缓存，那么什么时候要考虑热点本地缓存呢？ 答案是当我们遇到极端的热点数据查询的时候。热点本地缓存主要部署在应用服务器的代码中，用于阻挡热点查询对于分布式缓存节点或者数据库的压力。 比如某一位明星在微博上有了热点话题，“吃瓜群众”会到他 (她) 的微博首页围观，这就会引发这个用户信息的热点查询。这些查询通常会命中某一个缓存节点或者某一个数据库分区，短时间内会形成极高的热点查询。 那么我们会在代码中使用一些本地缓存方案，如 HashMap，Guava Cache 或者是 Ehcache 等，它们和应用程序部署在同一个进程中，优势是不需要跨网络调度，速度极快，所以可以来阻挡短时间内的热点查询。来看个例子。 比方说你的垂直电商系统的首页有一些推荐的商品，这些商品信息是由编辑在后台录入和变更。你分析编辑录入新的商品或者变更某个商品的信息后，在页面的展示是允许有一些延迟的，比如说 30 秒的延迟，并且首页请求量最大，即使使用分布式缓存也很难抗住，所以你决定使用 Guava Cache 来将所有的推荐商品的信息缓存起来，并且设置每隔 30 秒重新从数据库中加载最新的所有商品。 首先，我们初始化 Guava 的 Loading Cache： 123456789CacheBuilder&lt;String, List&lt;Product&gt;&gt; cacheBuilder = CacheBuilder.newBuilder().maximumSize(maxSize).recordStats(); // 设置缓存最大值cacheBuilder = cacheBuilder.refreshAfterWrite(30, TimeUnit.Seconds); // 设置刷新间隔 LoadingCache&lt;String, List&lt;Product&gt;&gt; cache = cacheBuilder.build(new CacheLoader&lt;String, List&lt;Product&gt;&gt;() { @Override public List&lt;Product&gt; load(String k) throws Exception { return productService.loadAll(); // 获取所有商品 }}); 这样，你在获取所有商品信息的时候可以调用 Loading Cache 的 get 方法，就可以优先从本地缓存中获取商品信息，如果本地缓存不存在，会使用 CacheLoader 中的逻辑从数据库中加载所有的商品。 由于本地缓存是部署在应用服务器中，而我们应用服务器通常会部署多台，当数据更新时，我们不能确定哪台服务器本地中了缓存，更新或者删除所有服务器的缓存不是一个好的选择，所以我们通常会等待缓存过期。因此，这种缓存的有效期很短，通常为分钟或者秒级别，以避免返回前端脏数据。 缓存的不足通过了解上面的内容，你不难发现，缓存的主要作用是提升访问速度，从而能够抗住更高的并发。那么，缓存是不是能够解决一切问题？显然不是。事物都是具有两面性的，缓存也不例外，我们要了解它的优势的同时也需要了解它有哪些不足，从而扬长避短，将它的作用发挥到最大。 首先，缓存比较适合于读多写少的业务场景，并且数据最好带有一定的热点属性。这是因为缓存毕竟会受限于存储介质不可能缓存所有数据，那么当数据有热点属性的时候才能保证一定的缓存命中率。比如说类似微博、朋友圈这种 20% 的内容会占到 80% 的流量。所以，一旦当业务场景读少写多时或者没有明显热点时，比如在搜索的场景下，每个人搜索的词都会不同，没有明显的热点，那么这时缓存的作用就不明显了。 其次，缓存会给整体系统带来复杂度，并且会有数据不一致的风险。当更新数据库成功，更新缓存失败的场景下，缓存中就会存在脏数据。对于这种场景，我们可以考虑使用较短的过期时间或者手动清理的方式来解决。 再次，之前提到缓存通常使用内存作为存储介质，但是内存并不是无限的。因此，我们在使用缓存的时候要做数据存储量级的评估，对于可预见的需要消耗极大存储成本的数据，要慎用缓存方案。同时，缓存一定要设置过期时间，这样可以保证缓存中的会是热点数据。 最后，缓存会给运维也带来一定的成本，运维需要对缓存组件有一定的了解，在排查问题的时候也多了一个组件需要考虑在内。 虽然有这么多的不足，但是缓存对于性能的提升是毋庸置疑的，我们在做架构设计的时候也需要把它考虑在内，只是在做具体方案的时候需要对缓存的设计有更细致的思考，才能最大化的发挥缓存的优势。","link":"/2019/12/01/2019-12-01-如何使用缓存/"},{"title":"NoSql相对于关系型数据库的优势","text":"使用 NoSQL 提升写入性能数据库系统大多使用的是传统的机械磁盘，对于机械磁盘的访问方式有两种：一种是随机 IO；另一种是顺序 IO。随机 IO 就需要花费时间做昂贵的磁盘寻道，一般来说，它的读写效率要比顺序 IO 小两到三个数量级，所以我们想要提升写入的性能就要尽量减少随机 IO。 以 MySQL 的 InnoDB 存储引擎来说，更新 binlog、redolog、undolog 都是在做顺序 IO，而更新 datafile 和索引文件则是在做随机 IO，而为了减少随机 IO 的发生，关系数据库已经做了很多的优化，比如说写入时先写入内存，然后批量刷新到磁盘上，但是随机 IO 还是会发生。 索引在 InnoDB 引擎中是以 B+ 树方式来组织的，而 MySQL 主键是聚簇索引（一种索引类型，数据与索引数据放在一起），既然数据和索引数据放在一起，那么在数据插入或者更新的时候，我们需要找到要插入的位置，再把数据写到特定的位置上，这就产生了随机的 IO。而且一旦发生了页分裂，就不可避免会做数据的移动，也会极大地损耗写入性能。 NoSQL 数据库是怎么解决这个问题的呢？ 它们有多种的解决方式，这里我给你讲一种最常见的方案，就是很多 NoSQL 数据库都在使用的基于 LSM 树的存储引擎，这种算法使用最多，所以在这里着重剖析一下。 LSM 树（Log-Structured Merge Tree）牺牲了一定的读性能来换取写入数据的高性能，Hbase、Cassandra、LevelDB 都是用这种算法作为存储的引擎。 它的思想很简单，数据首先会写入到一个叫做 MemTable 的内存结构中，在 MemTable 中数据是按照写入的 Key 来排序的。为了防止 MemTable 里面的数据因为机器掉电或者重启而丢失，一般会通过写 Write Ahead Log 的方式将数据备份在磁盘上。 MemTable 在累积到一定规模时，它会被刷新生成一个新的文件，我们把这个文件叫做 SSTable（Sorted String Table）。当 SSTable 达到一定数量时，我们会将这些 SSTable 合并，减少文件的数量，因为 SSTable 都是有序的，所以合并的速度也很快。 当从 LSM 树里面读数据时，我们首先从 MemTable 中查找数据，如果数据没有找到，再从 SSTable 中查找数据。因为存储的数据都是有序的，所以查找的效率是很高的，只是因为数据被拆分成多个 SSTable，所以读取的效率会低于 B+ 树索引。 和 LSM 树类似的算法有很多，比如说 TokuDB 使用的名为 Fractal tree 的索引结构，它们的核心思想就是将随机 IO 变成顺序的 IO，从而提升写入的性能。 提升扩展性另外，在扩展性方面，很多 NoSQL 数据库也有着先天的优势。还是以你的垂直电商系统为例，你已经为你的电商系统增加了评论系统，开始你的评估比较乐观，觉得电商系统的评论量级不会增长很快，所以就为它分了 8 个库，每个库拆分成 16 张表。 但是评论系统上线之后，存储量级增长的异常迅猛，你不得不将数据库拆分成更多的库表，而数据也要重新迁移到新的库表中，过程非常痛苦，而且数据迁移的过程也非常容易出错。 这时，你考虑是否可以考虑使用 NoSQL 数据库来彻底解决扩展性的问题，经过调研你发现它们在设计之初就考虑到了分布式和大数据存储的场景，比如像 MongoDB 就有三个扩展性方面的特性。 其一是 Replica，也叫做副本集，你可以理解为主从分离，也就是通过将数据拷贝成多份来保证当主挂掉后数据不会丢失。同时呢，Replica 还可以分担读请求。Replica 中有主节点来承担写请求，并且把对数据变动记录到 oplog 里（类似于 binlog）；从节点接收到 oplog 后就会修改自身的数据以保持和主节点的一致。一旦主节点挂掉，MongoDB 会从从节点中选取一个节点成为主节点，可以继续提供写数据服务。 其二是 Shard，也叫做分片，你可以理解为分库分表，即将数据按照某种规则拆分成多份，存储在不同的机器上。MongoDB 的 Sharding 特性一般需要三个角色来支持，一个是 Shard Server，它是实际存储数据的节点，是一个独立的 Mongod 进程；二是 Config Server，也是一组 Mongod 进程，主要存储一些元信息，比如说哪些分片存储了哪些数据等；最后是 Route Server，它不实际存储数据，仅仅作为路由使用，它从 Config Server 中获取元信息后，将请求路由到正确的 Shard Server 中。 其三是负载均衡，就是当 MongoDB 发现 Shard 之间数据分布不均匀，会启动 Balancer 进程对数据做重新的分配，最终让不同 Shard Server 的数据可以尽量的均衡。当我们的 Shard Server 存储空间不足需要扩容时，数据会自动被移动到新的 Shard Server 上，减少了数据迁移和验证的成本。 你可以看到，NoSQL 数据库中内置的扩展性方面的特性可以让我们不再需要对数据库做分库分表和主从分离，也是对传统数据库一个良好的补充。","link":"/2019/12/01/2019-12-01-nosql相对关系型数据库的优势/"},{"title":"消息队列(一)-如何解决消息丢失","text":"消息会丢失的环节消息从被写入到消息队列，到被消费者消费完成，这个链路上会有哪些地方存在丢失消息的可能呢？其实，主要存在三个场景： 消息从生产者写入到消息队列的过程。 消息在消息队列中的存储场景。 消息被消费者消费的过程。 1. 在消息生产的过程中丢失消息消息的生产者一般是我们的业务服务器，消息队列是独立部署在单独的服务器上的。两者之间的网络虽然是内网，但是也会存在抖动的可能，而一旦发生抖动，消息就有可能因为网络的错误而丢失。 针对这种情况，我建议你采用的方案是消息重传：也就是当你发现发送超时后你就将消息重新发一次，但是你也不能无限制地重传消息。一般来说，如果不是消息队列发生故障，或者是到消息队列的网络断开了，重试 2～3 次就可以了。 不过，这种方案可能会造成消息的重复，从而导致在消费的时候会重复消费同样的消息。比方说，消息生产时由于消息队列处理慢或者网络的抖动，导致虽然最终写入消息队列成功，但在生产端却超时了，生产者重传这条消息就会形成重复的消息。 2. 在消息队列中丢失消息拿 Kafka 举例，消息在 Kafka 中是存储在本地磁盘上的，而为了减少消息存储时对磁盘的随机 I/O，我们一般会将消息先写入到操作系统的 Page Cache 中，然后再找合适的时机刷新到磁盘上。 比如，Kafka 可以配置当达到某一时间间隔，或者累积一定的消息数量的时候再刷盘，也就是所说的异步刷盘。 不过，如果发生机器掉电或者机器异常重启，那么 Page Cache 中还没有来得及刷盘的消息就会丢失了。那么怎么解决呢？ 你可能会把刷盘的间隔设置很短，或者设置累积一条消息就就刷盘，但这样频繁刷盘会对性能有比较大的影响，而且从经验来看，出现机器宕机或者掉电的几率也不高，所以我不建议你这样做。 如果你的系统对消息丢失的容忍度很低，那么你可以考虑以集群方式部署 Kafka 服务，通过部署多个副本备份数据，保证消息尽量不丢失。 那么它是怎么实现的呢？ Kafka 集群中有一个 Leader 负责消息的写入和消费，可以有多个 Follower 负责数据的备份。Follower 中有一个特殊的集合叫做 ISR（in-sync replicas），当 Leader 故障时，新选举出来的 Leader 会从 ISR 中选择，默认 Leader 的数据会异步地复制给 Follower，这样在 Leader 发生掉电或者宕机时，Kafka 会从 Follower 中消费消息，减少消息丢失的可能。 由于默认消息是异步地从 Leader 复制到 Follower 的，所以一旦 Leader 宕机，那些还没有来得及复制到 Follower 的消息还是会丢失。为了解决这个问题，Kafka 为生产者提供一个选项叫做“acks”，当这个选项被设置为“all”时，生产者发送的每一条消息除了发给 Leader 外还会发给所有的 ISR，并且必须得到 Leader 和所有 ISR 的确认后才被认为发送成功。这样，只有 Leader 和所有的 ISR 都挂了，消息才会丢失。 从上面这张图来看，当设置“acks=all”时，需要同步执行 1，3，4 三个步骤，对于消息生产的性能来说也是有比较大的影响的，所以你在实际应用中需要仔细地权衡考量。我给你的建议是： 如果你需要确保消息一条都不能丢失，那么建议不要开启消息队列的同步刷盘，而是需要使用集群的方式来解决，可以配置当所有 ISR Follower 都接收到消息才返回成功。 如果对消息的丢失有一定的容忍度，那么建议不部署集群，即使以集群方式部署，也建议配置只发送给一个 Follower 就可以返回成功了。 我们的业务系统一般对于消息的丢失有一定的容忍度，比如说以上面的红包系统为例，如果红包消息丢失了，我们只要后续给没有发送红包的用户补发红包就好了。 3. 在消费的过程中存在消息丢失的可能我还是以 Kafka 为例来说明。一个消费者消费消息的进度是记录在消息队列集群中的，而消费的过程分为三步：接收消息、处理消息、更新消费进度。 这里面接收消息和处理消息的过程都可能会发生异常或者失败，比如说，消息接收时网络发生抖动，导致消息并没有被正确的接收到；处理消息时可能发生一些业务的异常导致处理流程未执行完成，这时如果更新消费进度，那么这条失败的消息就永远不会被处理了，也可以认为是丢失了。 所以，在这里你需要注意的是，一定要等到消息接收和处理完成后才能更新消费进度，但是这也会造成消息重复的问题，比方说某一条消息在处理之后，消费者恰好宕机了，那么因为没有更新消费进度，所以当这个消费者重启之后，还会重复地消费这条消息。","link":"/2019/12/08/2019-12-08-如何解决消息丢失/"},{"title":"Boyer–Moore 字符搜索算法","text":"因为字符比较是从右往左比较的，所以第一层循环 needle.length + 1 &lt;= i &lt; haystack.length。 123456789101112 start=needle.length - 1 end=haystack.length - 1 + + | | | | v-------------------------------------------------------------------v-&gt; 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+| t | h | i | s | | i | s | | a | | s | i | m | p | l | e | | e | x | a | m | p | l | e |+-------------------------------+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---++---------------------------+| e | x | a | m | p | l | e |+---+---+---+---+---+---+---+ 第二层循环中i变量表示坏字符的位置、j表示搜索坏字符开始位置 123456789101112131415 i + | v 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+| t | h | i | s | | i | s | | a | | s | i | m | p | l | e | | e | x | a | m | p | l | e |+-------------------------------+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---++---------------------------+| e | x | a | m | p | l | e |+---+---+---+---+---+---+-+-+ ^ | + j i,j指针向左搜索,如果完全匹配直接返回i即可 12345for (j = needle.length - 1; needle[j] == haystack[i]; --i, --j) { if (j == 0) { return i; }} 坏字符规则和好字符规则坏字符:从右向左第一个不匹配的字符好字符:从坏字符下一个字符直到最后的字符 可以认为字符移动有两种策略：坏字符对齐和好字符对齐，然后选择字符移动距离大的策略即可 wiki实现123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687/** * Returns the index within this string of the first occurrence of the * specified substring. If it is not a substring, return -1. * * There is no Galil because it only generates one match. * * @param haystack The string to be scanned * @param needle The target string to search * @return The start index of the substring */public static int indexOf(char[] haystack, char[] needle) { if (needle.length == 0) { return 0; } int charTable[] = makeCharTable(needle); int offsetTable[] = makeOffsetTable(needle); for (int i = needle.length - 1, j; i &lt; haystack.length;) { for (j = needle.length - 1; needle[j] == haystack[i]; --i, --j) { if (j == 0) { return i; } } // i += needle.length - j; // For naive method i += Math.max(offsetTable[needle.length - 1 - j], charTable[haystack[i]]); } return -1;}/** * Makes the jump table based on the mismatched character information. */private static int[] makeCharTable(char[] needle) { final int ALPHABET_SIZE = Character.MAX_VALUE + 1; // 65536 int[] table = new int[ALPHABET_SIZE]; for (int i = 0; i &lt; table.length; ++i) { table[i] = needle.length; } for (int i = 0; i &lt; needle.length - 2; ++i) { table[needle[i]] = needle.length - 1 - i; } return table;}/** * Makes the jump table based on the scan offset which mismatch occurs. * (bad character rule). */private static int[] makeOffsetTable(char[] needle) { int[] table = new int[needle.length]; int lastPrefixPosition = needle.length; for (int i = needle.length; i &gt; 0; --i) { if (isPrefix(needle, i)) { lastPrefixPosition = i; } table[needle.length - i] = lastPrefixPosition - i + needle.length; } for (int i = 0; i &lt; needle.length - 1; ++i) { int slen = suffixLength(needle, i); table[slen] = needle.length - 1 - i + slen; } return table;}/** * Is needle[p:end] a prefix of needle? */private static boolean isPrefix(char[] needle, int p) { for (int i = p, j = 0; i &lt; needle.length; ++i, ++j) { if (needle[i] != needle[j]) { return false; } } return true;}/** * Returns the maximum length of the substring ends at p and is a suffix. * (good suffix rule) */private static int suffixLength(char[] needle, int p) { int len = 0; for (int i = p, j = needle.length - 1; i &gt;= 0 &amp;&amp; needle[i] == needle[j]; --i, --j) { len += 1; } return len;}","link":"/2020/01/10/2020-01-10_Boyer–Moore 字符搜索算法/"},{"title":"消息队列(二)-消息幂等","text":"什么是幂等幂等是一个数学上的概念，它的含义是多次执行同一个操作和执行一次操作，最终得到的结果是相同的。 如果我们消费一条消息的时候，要给现有的库存数量减 1，那么如果消费两条相同的消息就会给库存数量减 2，这就不是幂等的。而如果消费一条消息后，处理逻辑是将库存的数量设置为 0，或者是如果当前库存数量是 10 时则减 1，这样在消费多条消息时，所得到的结果就是相同的，这就是幂等的。 说白了，你可以这么理解“幂等”：一件事儿无论做多少次都和做一次产生的结果是一样的，那么这件事儿就具有幂等性。 在生产、消费过程中增加消息幂等性的保证消息在生产和消费的过程中都可能会产生重复，所以你要做的是，在生产过程和消费过程中增加消息幂等性的保证，这样就可以认为从最终结果上来看，消息实际上是只被消费了一次的。 在消息生产过程中，在 Kafka0.11 版本和 Pulsar 中都支持“producer idempotency”的特性，翻译过来就是生产过程的幂等性，这种特性保证消息虽然可能在生产端产生重复，但是最终在消息队列存储时只会存储一份。 它的做法是给每一个生产者一个唯一的 ID，并且为生产的每一条消息赋予一个唯一 ID，消息队列的服务端会存储 &lt; 生产者 ID，最后一条消息 ID&gt; 的映射。当某一个生产者产生新的消息时，消息队列服务端会比对消息 ID 是否与存储的最后一条 ID 一致，如果一致，就认为是重复的消息，服务端会自动丢弃。 而在消费端，幂等性的保证会稍微复杂一些，你可以从通用层和业务层两个层面来考虑。 在通用层面，你可以在消息被生产的时候，使用发号器给它生成一个全局唯一的消息 ID，消息被处理之后，把这个 ID 存储在数据库中，在处理下一条消息之前，先从数据库里面查询这个全局 ID 是否被消费过，如果被消费过就放弃消费。 你可以看到，无论是生产端的幂等性保证方式，还是消费端通用的幂等性保证方式，它们的共同特点都是为每一个消息生成一个唯一的 ID，然后在使用这个消息的时候，先比对这个 ID 是否已经存在，如果存在，则认为消息已经被使用过。所以这种方式是一种标准的实现幂等的方式，你在项目之中可以拿来直接使用，它在逻辑上的伪代码就像下面这样： 12345678910111213boolean isIDExisted = selectByID(ID); // 判断 ID 是否存在if(isIDExisted) { return; // 存在则直接返回} else { process(message); // 不存在，则处理消息 saveID(ID); // 存储 ID} 不过这样会有一个问题：如果消息在处理之后，还没有来得及写入数据库，消费者宕机了重启之后发现数据库中并没有这条消息，还是会重复执行两次消费逻辑，这时你就需要引入事务机制，保证消息处理和写入数据库必须同时成功或者同时失败，但是这样消息处理的成本就更高了，所以，如果对于消息重复没有特别严格的要求，可以直接使用这种通用的方案，而不考虑引入事务。 在业务层面怎么处理呢？这里有很多种处理方式，其中有一种是增加乐观锁的方式。比如，你的消息处理程序需要给一个人的账号加钱，那么你可以通过乐观锁的方式来解决。 具体的操作方式是这样的：你给每个人的账号数据中增加一个版本号的字段，在生产消息时先查询这个账户的版本号，并且将版本号连同消息一起发送给消息队列。消费端在拿到消息和版本号后，在执行更新账户金额 SQL 的时候带上版本号，类似于执行： 1update user set amount = amount + 20, version=version+1 where userId=1 and version=1; 你看，我们在更新数据时给数据加了乐观锁，这样在消费第一条消息时，version 值为 1，SQL 可以执行成功，并且同时把 version 值改为了 2；在执行第二条相同的消息时，由于 version 值不再是 1，所以这条 SQL 不能执行成功，也就保证了消息的幂等性。","link":"/2019/12/08/2019-12-08-消息幂等/"},{"title":"IM系统关键点梳理","text":"","link":"/2020/03/23/2020-02-23_IM系统关键点梳理/"},{"title":"清醒思考的艺术-checklist","text":"幸存者偏差：高估了成功的概率 看清楚自己 是否高估自己 是否陷入从众心理 是否与沉没成本难舍难分 是否是互惠互利陷阱 摆脱确认偏差，不要证明自己是正确的 是否屈服于权威 是否依据对比来判断（请不要怎么做） 现成偏见 是否故意营造一种危机气氛 明确判断他人的能力范围 警惕控制错觉 警惕激励过敏 是否是平均值会起作用？ 警惕公地悲剧 切勿只以结果判断决定 面对选择，遵守自己的标准 排除对他人好感的影响 禀赋效应 let it go 无巧不成书，如果从不发生才令人感到意外 唱反调的人也许是最重要的人 0风险偏误 稀有性谬论 概率偏误-前车之鉴后事之师 独立事件-赌徒谬论 锚定效应，你的锚在哪里？ 警惕归纳法，警惕黑天鹅事件 厌恶风险，但不要无视损失 社会性懈怠 赢家诅咒-你会得到什么、失去什么 是否高估了人的影响 相互关系不等同于因果关系 避免光环效应过度影响总体印象 合理不一定真实 不要被框架效应限制注意力 遇到不明情况，就会发生行动偏误 不作为偏误，如果不解决问题，你就是问题的一部分 自利偏误，为什么你从来不自责 避免长时间负面影响 自我选择偏误 联想偏误，不要欠拟合也不要过拟合 认知失调，是否在自我安慰 即使行乐，只限周末，不要为了眼前的利益破坏未来的利益","link":"/2020/02/01/2020-02-01-清醒思考的艺术/"},{"title":"构建大型支付系统时分布式架构的关键点","text":"SLA在构建大型系统时，常常会遇到各种错误。在计划构建一个系统时，定义系统的“健康状态”十分重要。 “健康状态”必须是可度量的，一般做法是使用SLAs来度量系统的“健康状态”。最常见的SLA为 可达性 从时间维度衡量（99.999%可达性，每年下线50分钟） 准确性 对于数据的丢失或失真是否可以接受？可以达到多少百分比？对于支付系统来说，不接受任何数据的丢失和失真 容量 系统支持并发 延迟 响应延迟，一般衡量95%请求的响应时间和99%请求响应时间 确保新系统比被替代系统“更好”，可以使用上面四个SLA指标来衡量，可达性是最重要的需求。 水平和垂直伸缩随着新业务的增长，负载也会增加。最常见的伸缩策略是垂直和水平伸缩。 水平伸缩就是增加更多的机器或节点，对于分布式系统来说水平伸缩是最常有的方式。 垂直伸缩基本上就是买更大/好的机器。 一致性可达性对于任何系统都是很重要的，但是分布式系统一般都构建在低可达性的机器上（比如：服务的可达性要求99.999% 机器的可达性为99.9%）。简单的做法是维护一组机器组成集群，这样服务的可达性不依赖单独的机器。 一致性是在高可用系统中最需要关心的。一个一致性系统在所有的节点上看到和返回的数据在同一时间是相同的。如果使用一组机器来组成集群，它们还需要互相发送消息来保持同步，但是发送消息可能失败，这样一些节点就会因为不一致而不可达。 一致性有多个模型，在分布式系统最常用的是强一致性，弱一致性和最终一致性。一般来说，一致性要求越低，系统可以工作的更快，但是返回的数据不一定是最新的。 系统中的数据需要是一致的，但是到底是怎样的一致？对于一些系统，需要强一致性，比如一次支付必须是强一致的存储下来。对于没那么重要的部分，最终一致性是可以考虑的权衡。比如列出最近的交易。 数据持久性持久性表示一旦数据成功添加到数据存储，它就永远可以访问到。不同的分布式数据库拥有不同级别的数据持久性。一般使用副本来增加数据持久性。 对于支付系统来说，数据不允许丢失。我们构建的分布式数据存储需要支持集群级别的数据持久型。目前Cassandra, MongoDB, HDFS和Dynamodb 都支持多种级别的数据持久性。 消息保持与持久性分布式系统中的节点执行计算，存储数据，互相发送消息。发送消息的关键是消息的可靠到达。对于关键系统，经常需要消息零丢失。 对于分布式系统，发送消息一般石油分布式消息服务发送，如RabbitMQ，Kafka。这些消息服务支持不同级别的消息投递可靠性。 消息保持表示当节点处理消息失败时，在错误被解决前消息一直被保持着。消息的持久性一般在消息队列层被使用。如果在消息发送的时候队列或节点下线了，那在它们重新上线是还能接收到消息。 在支付系统中我们需要每一条消息投递一次，在构建系统中保证投递一次和投递至少一次在实现上是有区别的。最后我们使用了kafka来保证投递至少一次。 幂等性在分布式系统中，很多东西都可能出错，连接会丢包或超时，客户端经常会重试这些请求。一个幂等的系统保证无论多少特定的请求被执行，一个请求实际的操作只会执行一次。比如支付请求，如果客户端请求支付并且请求已经成功，但是客户端超时了，客户端是能够重试相同的请求的。对于一个幂等的系统，一个个人的支付是不能被收取两次的。 对幂等的设计，分布式系统需要某种分布式锁机制。假设我们想要使用乐观锁来实现幂等性，这时系统需要强一致性的锁来执行操作，我们可以使用同一个版本的乐观锁来检查是否有启动了额外的操作。 根据系统的一致性和操作的类型，有很多方式来实现幂等性。在设计分布式系统时，幂等性时最容易忽略的。在支付系统中，幂等操作时最重要的，它避免了双花和双收问题。消息系统已经保证了消息至少消费一次，我们只需要保证所有重复的消息保证幂等性即可。我们选择使用乐观锁，并使用强一致性存储作为乐观锁的数据源。 分片和法定人数分布式系统经常需要存储大量的数据，远超一台节点的容量。一般的解决方案时使用分片，数据使用某种hash算法被水平分区。尽管很多分布式数据库屏蔽了分片的实现，但是分片还是挺有意思的，特别是关于重新分片。 许多分布式系统在多个拥有数据和统计信息。为保证对数据操作的一致性，使用基于投票的方式是不行的，只有超过一定数量的节点操作成功，这个操作才是成功的，这个叫做法定人数。 Actor模型描述分布式系统最普遍的做法是使用Actor模型，还有一种方法是CSP。 Actor模型基于actor互相发送消息并作出回应。每一个actor只能做少量的动作，创建其他actors, 发送消息或者决定如何处理下个消息。通过这些简单的规则，复杂的分布式系统可以被准确描述，可以在actor崩溃后自我修复。 使用akka提供了标准的分布式模型，避免我们重复造轮子。 反应式架构当构建大型分布式系统时，目标常常是它们的弹性，伸缩性，和扩展性。反应式架构是在这个领域最流行和最通用的方案。","link":"/2018/11/19/Distributed architecture concepts I learned while building a large payments system/"},{"title":"奇怪的知识又增加了- BLNJ导致索引有序性失效","text":"先来看表结构： 1234567891011121314CREATE TABLE a ( `id`bigint AUTO_INCREMENT , `a` int, `b` int, PRIMARY KEY (`id`), KEY `idx_a_b` (`a`,`b`));CREATE TABLE b ( `id`bigint AUTO_INCREMENT , `b` int, `c` int, PRIMARY KEY (`id`)) 看一下join语句，因为b上没有索引，所以mysql用的BLNJ： 1234explain select * from a join b using(b)where a = 1order by a, b; id select_type table partitions type possible_keys key key_len ref rows filtered extra 1 SIMPLE a null ref idx_a_b idx_a_b 4 const 5206 100.00 Using temporary; Using filesort 1 SIMPLE b null ALL null null null Null 1000 100.00 Using where; Using join buffer (Block Nested Loop) 如果b表有索引的话： 1234567CREATE TABLE b ( `id`bigint AUTO_INCREMENT , `b` int, `c` int, PRIMARY KEY (`id`), KEY `idx_b` (`b`)) id select_type table partitions type possible_keys key key_len ref rows filtered extra 1 SIMPLE a null ref idx_a_b idx_a_b 8 Const 5206 100.00 Using index condition 1 SIMPLE b null Ref idx_b Idx_b 4 b.b 50 100.00 null 可以发现a表idx_a_b有序性没有利用上，至于原因，先看一下BNLJ执行的流程图: 执行过程为： 扫描表 t1，顺序读取数据行放入 join_buffer 中，直到 join_buffer 满了，继续第 2 步； 扫描表 t2，把 t2 中的每一行取出来，跟 join_buffer 中的数据做对比，满足 join 条件的，作为结果集的一部分返回； 清空 join_buffer； 继续扫描表 t1，顺序读取之后数据放入 join_buffer 中，继续执行第 2 步，直到所有数据读取完毕。 其中隐含的问题在于第二步：即使t1表的数据是有序读取到join_buffer中的，由于是先扫描t2表再关联join_buffer数据，导致join_buffer中的有序性失效。 如果表b有索引idx_b,那么使用BKA算法第二步的关联顺序与BNLJ相反，是先扫描join_buffer后通过索引关联t2,则可以利用join_buffer中的有序数据。","link":"/2020/03/12/2020-03-12-奇怪的知识又增加了-BLNJ导致索引有序性失效/"},{"title":"IO Modle","text":"操作系统IO模型与Java IOJava IO模型和操作系统IO模型息息相关，之前阻塞/非阻塞，同步/非同步之间的关系一直分不清，所以很有必要了解下操作系统(linux)提供了哪些接口来进行IO。目前我们只需要了解即可，使用相关可以直接查看java io教程。 最基础的知识以使用IO读取数据为例，一般操作系统分为两个独立的阶段进行操作： 等待数据准备完成，可以是从磁盘拷贝到内核空间，或者是网卡接受到数据后拷贝到内核空间。 从内核空间将数据拷贝至请求数据的进程。如果是java可能还需从进程拷贝至jvm堆内存。 Blocking I/O Model这个是最常用的模型，望文生义就是阻塞IO，进行IO的两个阶段会都阻塞程序，直到读取到数据或者返回错误才会返回。 具体来说，通过调用系统recvfrom函数，而recvfrom函数会等到出错或者把数据拷贝到进程完成时才会返回。之后我们程序只需要处理错误或者处理数据就可以了。 阻塞模型对应java中绝大部分IO操作，比如网络请求api，io stream api，该模型优点在于简单直观，缺点在长时间阻塞很难支持大量并发IO请求。 Nonblocking I/O Model该模型在java中没有对应，所以这里只做简单介绍。 使用轮询方式调用系统recvfrom函数，recvfrom函数在第一阶段完成前一直返回错误，直到第一阶段完成后，阻塞至第二阶段完成。 这个模型稍显鸡肋，特点是在第一阶段是非阻塞的（进程不会被切换），代码相比阻塞模型来说也更复杂。 I/O Multiplexing Model非常著名的IO模型，可以支持大量并发IO。通过调用select或者pull并阻塞，而不是在实际调用系统IO时阻塞。使用select阻塞在第一阶段和Blocking I/O的阻塞不太一样，Blocking I/O阻塞在当前IO操作第一阶段，而I/O复用则可以注册多个I/O在select函数，当有一个I/O就绪时select函数就会返回，如果所有I/O处于第一阶段阻塞状态则select函数阻塞。 相比较Blocking I/O Model和Nonblocking I/O Model，I/O Multiplexing Model明显能在短时间内处理更多的I/O。如果使用多线程+Blocking I/O Model也能达到类似的效果，但是有可能消耗过多线程资源。 I/O Multiplexing Model对应java NIO的Selector等api Signal-Driven I/O Model该模型在java中没有对应，所以这里只做简单介绍。 该模型特点是第一阶段调用sigaction函数非阻塞返回，在第一阶段完成后发送信号SIGIO至进程，之后在signal handler中进行第二阶段处理。相当于对Nonblocking I/O Model的一种改进。 Asynchronous I/O ModelAsynchronous I/O Model相比较Signal-Driven I/O Model的区别在于通知的时机不同：Asynchronous I/O Model在第一和第二阶段都完成时通过信号通知进程操作完成。 Asynchronous I/O Model对应java中AsynchronousSocketChannel，AsynchronousServerSocketChannel 和 AsynchronousFileChannel等api。 各个模型比较","link":"/2019/05/20/IO-Modle/"},{"title":"[片段] @CreatedBy / @ModifiedBy 拦截器实现","text":"拦截器实现： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859package app.pooi.common.entity;import app.pooi.common.entity.anno.CreatedBy;import app.pooi.common.entity.anno.ModifiedBy;import lombok.Data;import org.apache.ibatis.executor.Executor;import org.apache.ibatis.mapping.MappedStatement;import org.apache.ibatis.plugin.Intercepts;import org.apache.ibatis.plugin.Invocation;import org.apache.ibatis.plugin.Plugin;import org.apache.ibatis.plugin.Signature;import java.util.Arrays;import java.util.Properties;import java.util.function.Supplier;@Data@Intercepts({ @Signature(type = Executor.class, method = \"update\", args = {MappedStatement.class, Object.class}),})public class EntityInterceptor implements org.apache.ibatis.plugin.Interceptor { private Supplier&lt;Long&gt; auditorAware; @Override public Object intercept(Invocation invocation) throws Throwable { Executor executor = (Executor) invocation.getTarget(); MappedStatement ms = (MappedStatement) invocation.getArgs()[0]; Object o = invocation.getArgs()[1]; Arrays.stream(o.getClass().getDeclaredFields()) .forEach(field -&gt; { final CreatedBy createdBy = field.getAnnotation(CreatedBy.class); final ModifiedBy modifiedBy = field.getAnnotation(ModifiedBy.class); if (createdBy != null || modifiedBy != null) { field.setAccessible(true); try { field.set(o, auditorAware.get()); } catch (IllegalAccessException ignore) { } } }); return invocation.proceed(); } @Override public Object plugin(Object target) { return Plugin.wrap(target, this); } @Override public void setProperties(Properties properties) { }} 配置： 1234567891011121314@Configurationstatic class MybatisInterceptorConfig { @Bean public Interceptor[] configurationCustomizer(CipherSpi cipherSpi) { final EntityInterceptor entityInterceptor = new EntityInterceptor(); entityInterceptor.setAuditorAware(() -&gt; { final String header = ((ServletRequestAttributes) RequestContextHolder.getRequestAttributes()).getRequest().getHeader(XHeaders.LOGIN_USER_ID); return Long.valueOf(header); }); return new Interceptor[]{new DecryptInterceptor(cipherSpi), entityInterceptor}; }}","link":"/2019/02/11/【片段】@CreatedBy  @ModifiedBy/"},{"title":"[感悟] 2018我的所见所闻所感","text":"1. 我就喜欢写代码行不行？如果刚刚投入工作，工作还无法完全胜任，那我的建议是多写写代码，打怪升级完成自己的年度绩效。 如果已经能够胜任当前工作，下一步应该集中在提效上。这一步和之前自己积累的经验和知识密不可分，只有真正了解代码懂代码，才能从同龄人中的“熟练工”脱颖而出，两者的提效虽然结果一样，但是本质却完全不一样。 如果已经摆脱了熟练工的称号，实际上已经完成了自我提效，提前完成自己的本职任务，下一步可以计划推动团队的效率。 如果太过专注技术，专心自己的一亩三分田，相当于给自己设限成毕业两三年的水平。这种人应该被打上不胜任的标签，在寒冬中很容易被优化掉。专注技术还有一个误区，就是容易把“实施细节”和“技术”两者混淆，特别在软件行业“实施细节”很容易随时代改变，基本三年就会大变样，而“技术”类似于“知识”不会过时。看到这里大家可以自己思考下，自己学到的到底是“实施细节”还是真正的“技术”。 2.如何推动团队进步恭喜你跨过了第一道坎，推动团队进步不是一个人的事，一般推动团队分为两部分：个人影响团队，团队自我驱动。 个人影响团队比较简单，就是把自我提效所积累的经验和知识共享给整个团队，完成的手段可以是博客分享，会议分享，文档分享。 团队自我驱动，实际上我把整个团队拟作了一个人，一个人找出别人的缺点很容易，但是找出一个团队的问题却没那么容易，同时也会受到公司和领导的局限，比如一些项目管理的领导就是二传手，只催你进度的那种，这时候就需要你主动找他讨论。 如何找到团队的缺点？可以通过下面两个大类的套路：管理手段和技术手段。 管理手段可以从知识管理、代码规范、需求分析三处着手： 知识管理： 建立知识库，避免重复的培训，重复的解答问题 代码规范：借助代码缺陷检查工具，具体到负责人提升代码规范 需求分析：避免低效，无效会议，避免各种妥协下产生的需求 技术手段比较简单： 重构升级：弃用老的架构，拥抱新技术，带领团队提升技术 内部造轮子：内部定制工具，帮助团队高效完成任务 找到了团队的缺点接下来可以制定度量，衡量团队的推进程度，可以从两个角度进行度量： 跟自己比较：比如这次做需求提测bug数减少，需求delay少了，满足需求不需要上线只要配置上线，等等 和别的团队比较：这个比较凶残，我也不知道用什么度量比，但是有的大公司就是这样做的。 3.总结与晋升从发现缺点到最后得到成果完成团队推进目标，是时候写个ppt在领导面前吹一波了，这个就不用我教了。","link":"/2018/12/21/【归档】2018我的所见所闻所感/"},{"title":"resilience4j-retry源码阅读","text":"resilience4j 源码还是比较清晰简单的，比较适合阅读。 放一张主要类的结构图： Retry入口Retry接口是提供重试功能的入口，主要提供了方法模版，具体校验结构，失败后处理由Context子类实现。 1234567891011121314151617181920212223/** * Creates a retryable supplier. * * @param retry the retry context * @param supplier the original function * @param &lt;T&gt; the type of results supplied by this supplier * @return a retryable function */static &lt;T&gt; Supplier&lt;T&gt; decorateSupplier(Retry retry, Supplier&lt;T&gt; supplier) { return () -&gt; { Retry.Context&lt;T&gt; context = retry.context(); do try { T result = supplier.get(); final boolean validationOfResult = context.onResult(result); if (!validationOfResult) { context.onSuccess(); return result; } } catch (RuntimeException runtimeException) { context.onRuntimeError(runtimeException); } while (true); };} 这里摘抄了一段核心代码，作用是循环直到context.onResult(result)返回true为止，需要留意context.onResult/onRuntimeError/onError可能执行多次， onSuccess只会执行一次，这里每次进入重试都是一个新的context对象。 Retry.ContextImpl1234567891011121314151617181920212223public boolean onResult(T result) { if (null != resultPredicate &amp;&amp; resultPredicate.test(result)) { int currentNumOfAttempts = numOfAttempts.incrementAndGet(); if (currentNumOfAttempts &gt;= maxAttempts) { return false; } else { waitIntervalAfterFailure(currentNumOfAttempts, null); return true; } } return false;}public void onRuntimeError(RuntimeException runtimeException) { if (exceptionPredicate.test(runtimeException)) { lastRuntimeException.set(runtimeException); throwOrSleepAfterRuntimeException(); } else { failedWithoutRetryCounter.increment(); publishRetryEvent(() -&gt; new RetryOnIgnoredErrorEvent(getName(), runtimeException)); throw runtimeException; }} 先关注onResult，它负责判断是否需要继续重试，如果通过校验或者重试超过此数，会停止重试。 onRuntimeError/onError, 负责把catch的异常存储在lastRuntimeException中。 12345678910public void onSuccess() { int currentNumOfAttempts = numOfAttempts.get(); if (currentNumOfAttempts &gt; 0) { succeededAfterRetryCounter.increment(); Throwable throwable = Option.of(lastException.get()).getOrElse(lastRuntimeException.get()); publishRetryEvent(() -&gt; new RetryOnSuccessEvent(getName(), currentNumOfAttempts, throwable)); } else { succeededWithoutRetryCounter.increment(); }} onSuccess负责统计和发送事件。 总结总体来说retry比较简单，需要注意的点有一个如果设置了结果校验，如果一直校验不通过，将返回未通过的结果，而不是返回失败。","link":"/2019/04/18/resilience4j-retry源码阅读/"},{"title":"[片段] SpringBoot Mybatis配置","text":"纯记录，供自己参考🤣。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140private final MybatisProperties properties;private final Interceptor[] interceptors;private final ResourceLoader resourceLoader;private final DatabaseIdProvider databaseIdProvider;private final List&lt;ConfigurationCustomizer&gt; configurationCustomizers;public DataSourceConfig(MybatisProperties properties, ObjectProvider&lt;Interceptor[]&gt; interceptorsProvider, ResourceLoader resourceLoader, ObjectProvider&lt;DatabaseIdProvider&gt; databaseIdProvider, ObjectProvider&lt;List&lt;ConfigurationCustomizer&gt;&gt; configurationCustomizersProvider) { this.properties = properties; this.interceptors = interceptorsProvider.getIfAvailable(); this.resourceLoader = resourceLoader; this.databaseIdProvider = databaseIdProvider.getIfAvailable(); this.configurationCustomizers = configurationCustomizersProvider.getIfAvailable();}/** * 普通数据源 * 主数据源，必须配置，spring启动时会执行初始化数据操作（无论是否真的需要），选择查找DataSource class类型的数据源 * * @return {@link DataSource} */@Primary@Bean(name = BEANNAME_DATASOURCE_COMMON)@ConfigurationProperties(prefix = \"com.lianjia.confucius.bridge.boot.datasource.common\")public DataSource createDataSourceCommon() { return DataSourceBuilder.create().build();}/** * 只读数据源 * * @return {@link DataSource} */@Bean(name = BEANNAME_DATASOURCE_READONLY)@ConfigurationProperties(prefix = \"com.lianjia.confucius.bridge.boot.datasource.readonly\")public DataSource createDataSourceReadonly() { return DataSourceBuilder.create().build();}private SqlSessionFactory sqlSessionFactory(DataSource dataSource) throws Exception { SqlSessionFactoryBean factory = new SqlSessionFactoryBean(); factory.setDataSource(dataSource); factory.setVfs(SpringBootVFS.class); if (StringUtils.hasText(this.properties.getConfigLocation())) { factory.setConfigLocation(this.resourceLoader.getResource(this.properties.getConfigLocation())); } org.apache.ibatis.session.Configuration configuration = this.properties.getConfiguration(); if (configuration == null &amp;&amp; !StringUtils.hasText(this.properties.getConfigLocation())) { configuration = new org.apache.ibatis.session.Configuration(); } if (configuration != null &amp;&amp; !CollectionUtils.isEmpty(this.configurationCustomizers)) { for (ConfigurationCustomizer customizer : this.configurationCustomizers) { customizer.customize(configuration); } } factory.setConfiguration(configuration); if (this.properties.getConfigurationProperties() != null) { factory.setConfigurationProperties(this.properties.getConfigurationProperties()); } if (!ObjectUtils.isEmpty(this.interceptors)) { factory.setPlugins(this.interceptors); } if (this.databaseIdProvider != null) { factory.setDatabaseIdProvider(this.databaseIdProvider); } if (StringUtils.hasLength(this.properties.getTypeAliasesPackage())) { factory.setTypeAliasesPackage(this.properties.getTypeAliasesPackage()); } if (StringUtils.hasLength(this.properties.getTypeHandlersPackage())) { factory.setTypeHandlersPackage(this.properties.getTypeHandlersPackage()); } if (!ObjectUtils.isEmpty(this.properties.resolveMapperLocations())) { factory.setMapperLocations(this.properties.resolveMapperLocations()); } return factory.getObject();}public SqlSessionTemplate sqlSessionTemplate(SqlSessionFactory sqlSessionFactory) { ExecutorType executorType = this.properties.getExecutorType(); if (executorType != null) { return new SqlSessionTemplate(sqlSessionFactory, executorType); } else { return new SqlSessionTemplate(sqlSessionFactory); }}@Bean@Primarypublic SqlSessionFactory primarySqlSessionFactory() throws Exception { return this.sqlSessionFactory(this.createDataSourceCommon());}@Beanpublic SqlSessionFactory secondarySqlSessionFactory() throws Exception { return this.sqlSessionFactory(this.createDataSourceReadonly());}/** * 实例普通的 sqlSession * * @return SqlSession * @throws Exception when any exception occured */@Bean(name = BEANNAME_SQLSESSION_COMMON)public SqlSession initSqlSessionCommon() throws Exception { return this.sqlSessionTemplate(this.primarySqlSessionFactory());}/** * 实例只读的 sqlSession * * @return SqlSession * @throws Exception when any exception occured */@Bean(name = BEANNAME_SQLSESSION_READONLY)public SqlSession initSqlSessionReadonly() throws Exception { return this.sqlSessionTemplate(this.secondarySqlSessionFactory());}@MapperScan(annotationClass = PrimaryMapper.class, sqlSessionTemplateRef = BEANNAME_SQLSESSION_COMMON, basePackageClasses = ITalentApplicationSpringBootStart.class)static class PrimaryMapperConfiguration {}@MapperScan(annotationClass = SecondaryMapper.class, sqlSessionTemplateRef = BEANNAME_SQLSESSION_READONLY, basePackageClasses = ITalentApplicationSpringBootStart.class)static class SecondaryMapperConfiguration {}","link":"/2019/03/13/【片段】SpringBoot Mybatis配置/"},{"title":"LeetCode二叉树排序树基础算法","text":"修剪二叉搜索树12345678910111213141516171819/** * Definition for a binary tree node. * public class TreeNode { * int val; * TreeNode left; * TreeNode right; * TreeNode(int x) { val = x; } * } */class Solution { public TreeNode trimBST(TreeNode root, int L, int R) { if (root == null) return null; if (root.val &lt; L) return trimBST(root.right, L, R); if (root.val &gt; R) return trimBST(root.left, L, R); root.left = trimBST(root.left, L, R); root.right = trimBST(root.right, L, R); return root; }} 二叉搜索树中第K小的元素12345678910111213141516171819202122232425262728293031/** * Definition for a binary tree node. * public class TreeNode { * int val; * TreeNode left; * TreeNode right; * TreeNode(int x) { val = x; } * } */class Solution { private int cnt; private int val; public int kthSmallest(TreeNode root, int k) { search(root, k); return val; } private void search(TreeNode root, int k) { if (root == null) return; // kthSmallest(root.left, k); cnt++; if (cnt == k) { val = root.val; return; } kthSmallest(root.right, k); }} 把二叉搜索树转换为累加树1234567891011121314151617181920212223242526272829303132/** * Definition for a binary tree node. * public class TreeNode { * int val; * TreeNode left; * TreeNode right; * TreeNode(int x) { val = x; } * } */class Solution { private int sum; public TreeNode convertBST(TreeNode root) { // 中序遍历 但是是从右往左遍历 // visit(root); return root; } private void visit(TreeNode root) { if (root == null) return; visit(root.right); sum += root.val; root.val = sum; visit(root.left); }} 二叉搜索树的最近公共祖先12345678910111213141516171819/** * Definition for a binary tree node. * public class TreeNode { * int val; * TreeNode left; * TreeNode right; * TreeNode(int x) { val = x; } * } */class Solution { public TreeNode lowestCommonAncestor(TreeNode root, TreeNode p, TreeNode q) { // 公共祖先在左边 if (root.val &gt; p.val &amp;&amp; root.val &gt; q.val) return lowestCommonAncestor(root.left, p, q); // 公共祖先在右边 if (root.val &lt; p.val &amp;&amp; root.val &lt; q.val) return lowestCommonAncestor(root.right, p, q); // 公共祖先在这 return root; }} 二叉树的最近公共祖先12345678910111213141516171819202122/** * Definition for a binary tree node. * public class TreeNode { * int val; * TreeNode left; * TreeNode right; * TreeNode(int x) { val = x; } * } */class Solution { public TreeNode lowestCommonAncestor(TreeNode root, TreeNode p, TreeNode q) { if (root == null || p == root || q == root) return root; TreeNode left = lowestCommonAncestor(root.left, p, q); TreeNode right = lowestCommonAncestor(root.right, p, q); // 左右都是父节点， 上一级就是公共父节点 if(left != null &amp;&amp; right != null) return root; if (left == null &amp;&amp; right == null) return null; if (left != null) return left; return right; }} 将有序数组转换为二叉搜索树二叉树中序遍历 1234567891011121314151617181920212223242526/** * Definition for a binary tree node. * public class TreeNode { * int val; * TreeNode left; * TreeNode right; * TreeNode(int x) { val = x; } * } */class Solution { public TreeNode sortedArrayToBST(int[] nums) { return build(nums, 0, nums.length -1); } private TreeNode build(int[] nums, int start, int end) { if(start&gt; end) return null; TreeNode node = new TreeNode(nums[(start+end)/2]); node.left = build(nums, start, (start+end)/2 -1); node.right = build(nums, (start+end)/2+1, end); return node; }} 有序链表转换二叉搜索树链表转数组 1234567891011121314151617181920212223242526272829303132333435363738394041/** * Definition for singly-linked list. * public class ListNode { * int val; * ListNode next; * ListNode(int x) { val = x; } * } *//** * Definition for a binary tree node. * public class TreeNode { * int val; * TreeNode left; * TreeNode right; * TreeNode(int x) { val = x; } * } */class Solution { public TreeNode sortedListToBST(ListNode head) { List&lt;Integer&gt; list = new LinkedList(); ListNode now = head; while (now != null) { list.add(now.val); now = now.next; } return build(list, 0, list.size() - 1); } private TreeNode build(List&lt;Integer&gt; nums, int start, int end) { if(start&gt; end) return null; TreeNode node = new TreeNode(nums.get((start+end)/2)); node.left = build(nums, start, (start+end)/2 -1); node.right = build(nums, (start+end)/2+1, end); return node; }} 还可以使用双指针找到链表中间节点，缺点是重复遍历节点 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162/** * Definition for singly-linked list. * public class ListNode { * int val; * ListNode next; * ListNode(int x) { val = x; } * } *//** * Definition for a binary tree node. * public class TreeNode { * int val; * TreeNode left; * TreeNode right; * TreeNode(int x) { val = x; } * } */class Solution { public TreeNode sortedListToBST(ListNode head) {// List&lt;Integer&gt; list = new LinkedList();// ListNode now = head;// while (now != null) {// list.add(now.val);// now = now.next;// } // return build(list, 0, list.size() - 1); return sortedListToBST(head, null); } private TreeNode sortedListToBST(ListNode head, ListNode tail) { if (head == tail) return null; ListNode mid = head, end = head; while (end != tail &amp;&amp; end.next != tail) { mid = mid.next; end = end.next.next; } TreeNode root = new TreeNode(mid.val); root.right = sortedListToBST(mid.next, tail); root.left = sortedListToBST(head, mid); return root; } private TreeNode build(List&lt;Integer&gt; nums, int start, int end) { if(start&gt; end) return null; TreeNode node = new TreeNode(nums.get((start+end)/2)); node.left = build(nums, start, (start+end)/2 -1); node.right = build(nums, (start+end)/2+1, end); return node; } } 两数之和 IV - 输入 BST自己写两次遍历搜索二叉树，注意要排除自身节点 123456789101112131415161718192021222324252627282930313233343536/** * Definition for a binary tree node. * public class TreeNode { * int val; * TreeNode left; * TreeNode right; * TreeNode(int x) { val = x; } * } */class Solution { private boolean res; private TreeNode r; private TreeNode current; public boolean findTarget(TreeNode root, int k) { r = root; visit(root, k); return res; } private void visit(TreeNode root, int val) { if (root == null) return; visit(root.left, val); current = root; if (find(r, val - root.val)) {res = true; return;} visit(root.right, val); } private boolean find(TreeNode root, int value) { if (root == null) return false; if (root == current) return false; if (root.val == value ) return true; return (value &gt; root.val) ? find(root.right, value): find(root.left, value); }} 正经思路， 中序遍历转化为排序数组， 使用双指针查找 1234567891011121314151617181920212223242526272829303132333435363738/** * Definition for a binary tree node. * public class TreeNode { * int val; * TreeNode left; * TreeNode right; * TreeNode(int x) { val = x; } * } */class Solution { private List&lt;Integer&gt; res = new ArrayList&lt;&gt;(64); public boolean findTarget(TreeNode root, int k) { visitTree(root); int low = 0, high = res.size() - 1; while (low &lt; high) { int sum = res.get(low) + res.get(high); if (sum == k) return true; if (sum &lt; k) low++; else high--; } return false; } private void visitTree(TreeNode root) { if (root == null) return; visitTree(root.left); res.add(root.val); visitTree(root.right); }}","link":"/2019/06/15/2019-06-30树算法 - 二叉排序树/"},{"title":"缓存专题(四) 分布式缓存高可用","text":"分布式缓存的高可用方案主要选择的方案有客户端方案、中间代理层方案和服务端方案三大类： 客户端方案就是在客户端配置多个缓存的节点，通过缓存写入和读取算法策略来实现分布式，从而提高缓存的可用性。 中间代理层方案是在应用代码和缓存节点之间增加代理层，客户端所有的写入和读取的请求都通过代理层，而代理层中会内置高可用策略，帮助提升缓存系统的高可用。 服务端方案就是 Redis 2.4 版本后提出的 Redis Sentinel 方案。 掌握这些方案可以帮助你，抵御部分缓存节点故障导致的，缓存命中率下降的影响，增强你的系统的鲁棒性。 客户端方案在客户端方案中，你需要关注缓存的写和读两个方面： 写入数据时，需要把被写入缓存的数据分散到多个节点中，即进行数据分片； 读数据时，可以利用多组的缓存来做容错，提升缓存系统的可用性。关于读数据，这里可以使用主从和多副本两种策略，两种策略是为了解决不同的问题而提出的。 下面我就带你一起详细地看一下到底要怎么做。 1. 缓存数据如何分片 单一的缓存节点受到机器内存、网卡带宽和单节点请求量的限制，不能承担比较高的并发，因此我们考虑将数据分片，依照分片算法将数据打散到多个不同的节点上，每个节点上存储部分数据。 这样在某个节点故障的情况下，其他节点也可以提供服务，保证了一定的可用性。这就好比不要把鸡蛋放在同一个篮子里，这样一旦一个篮子掉在地上，摔碎了，别的篮子里还有没摔碎的鸡蛋，不至于一个不剩。 一般来讲，分片算法常见的就是 Hash 分片算法和一致性 Hash 分片算法两种。 Hash 分片的算法就是对缓存的 Key 做哈希计算，然后对总的缓存节点个数取余。你可以这么理解： 比如说，我们部署了三个缓存节点组成一个缓存的集群，当有新的数据要写入时，我们先对这个缓存的 Key 做比如 crc32 等 Hash 算法生成 Hash 值，然后对 Hash 值模 3，得出的结果就是要存入缓存节点的序号。 这个算法最大的优点就是简单易理解，缺点是当增加或者减少缓存节点时，缓存总的节点个数变化造成计算出来的节点发生变化，从而造成缓存失效不可用。所以我建议你，如果采用这种方法，最好建立在你对于这组缓存命中率下降不敏感，比如下面还有另外一层缓存来兜底的情况下。 当然了，用一致性 Hash 算法可以很好地解决增加和删减节点时，命中率下降的问题。在这个算法中，我们将整个 Hash 值空间组织成一个虚拟的圆环，然后将缓存节点的 IP 地址或者主机名做 Hash 取值后，放置在这个圆环上。当我们需要确定某一个 Key 需要存取到哪个节点上的时候，先对这个 Key 做同样的 Hash 取值，确定在环上的位置，然后按照顺时针方向在环上“行走”，遇到的第一个缓存节点就是要访问的节点。比方说下面这张图里面，Key 1 和 Key 2 会落入到 Node 1 中，Key 3、Key 4 会落入到 Node 2 中，Key 5 落入到 Node 3 中，Key 6 落入到 Node 4 中。 这时如果在 Node 1 和 Node 2 之间增加一个 Node 5，你可以看到原本命中 Node 2 的 Key 3 现在命中到 Node 5，而其它的 Key 都没有变化；同样的道理，如果我们把 Node 3 从集群中移除，那么只会影响到 Key 5 。所以你看，在增加和删除节点时，只有少量的 Key 会“漂移”到其它节点上，而大部分的 Key 命中的节点还是会保持不变，从而可以保证命中率不会大幅下降。 不过，事物总有两面性。虽然这个算法对命中率的影响比较小，但它还是存在问题： 缓存节点在圆环上分布不平均，会造成部分缓存节点的压力较大；当某个节点故障时，这个节点所要承担的所有访问都会被顺移到另一个节点上，会对后面这个节点造成压力。 一致性 Hash 算法的脏数据问题。 极端情况下，比如一个有三个节点 A、B、C 承担整体的访问，每个节点的访问量平均，A 故障后，B 将承担双倍的压力（A 和 B 的全部请求），当 B 承担不了流量 Crash 后，C 也将因为要承担原先三倍的流量而 Crash，这就造成了整体缓存系统的雪崩。 说到这儿，你可能觉得很可怕，但也不要太担心，我们程序员就是要能够创造性地解决各种问题，所以你可以在一致性 Hash 算法中引入虚拟节点的概念。 它将一个缓存节点计算多个 Hash 值分散到圆环的不同位置，这样既实现了数据的平均，而且当某一个节点故障或者退出的时候，它原先承担的 Key 将以更加平均的方式分配到其他节点上，从而避免雪崩的发生。 其次，就是一致性 Hash 算法的脏数据问题。为什么会产生脏数据呢？比方说，在集群中有两个节点 A 和 B，客户端初始写入一个 Key 为 k，值为 3 的缓存数据到 Cache A 中。这时如果要更新 k 的值为 4，但是缓存 A 恰好和客户端连接出现了问题，那这次写入请求会写入到 Cache B 中。接下来缓存 A 和客户端的连接恢复，当客户端要获取 k 的值时，就会获取到存在 Cache A 中的脏数据 3，而不是 Cache B 中的 4。 所以，在使用一致性 Hash 算法时一定要设置缓存的过期时间，这样当发生漂移时，之前存储的脏数据可能已经过期，就可以减少存在脏数据的几率。 很显然，数据分片最大的优势就是缓解缓存节点的存储和访问压力，但同时它\b也让缓存的使用\b更加复杂。在 MultiGet（批量获取）场景下，单个节点的访问量并没有减少，同时节点数太多会造成缓存访问的 SLA（即“服务等级协议”，SLA 代表了网站服务可用性）得不到很好的保证，因为根据木桶原则，SLA 取决于最慢、最坏的节点的情况，节点数过多也会增加出问题的概率，因此我推荐 4 到 6 个节点为佳。 中间代理层方案虽然客户端方案已经能解决大部分的问题，但是只能在单一语言系统之间复用。例如微博使用 Java 语言实现了这么一套逻辑，我使用 PHP 就难以复用，需要重新写一套，很麻烦。而中间代理层的方案就可以解决这个问题。你可以将客户端解决方案的经验移植到代理层中，通过通用的协议（如 Redis 协议）来实现在其他语言中的复用。 如果你来自研缓存代理层，你就可以将客户端方案中的高可用逻辑封装在代理层代码里面，这样用户在使用你的代理层的时候就不需要关心缓存的高可用是如何做的，只需要依赖你的代理层就好了。 除此以外，业界也有很多中间代理层方案，比如 Facebook 的Mcrouter，Twitter 的Twemproxy，豌豆荚的Codis。它们的原理基本上可以由一张图来概括： 看这张图你有什么发现吗？ 所有缓存的读写请求都是经过代理层完成的。代理层是无状态的，主要负责读写请求的路由功能，并且在其中内置了一些高可用的逻辑，不同的开源中间代理层方案中使用的高可用策略各有不同。比如在 Twemproxy 中，Proxy 保证在某一个 Redis 节点挂掉之后会把它从集群中移除，后续的请求将由其他节点来完成；而 Codis 的实现略复杂，它提供了一个叫 Codis Ha 的工具来实现自动从节点提主节点，在 3.2 版本之后换做了 Redis Sentinel 方式，从而实现 Redis 节点的高可用。 服务端方案Redis 在 2.4 版本中提出了 Redis Sentinel 模式来解决主从 Redis 部署时的高可用问题，它可以在主节点挂了以后自动将从节点提升为主节点，保证整体集群的可用性，整体的架构如下图所示： Redis Sentinel 也是集群部署的，这样可以避免 Sentinel 节点挂掉造成无法自动故障恢复的问题，每一个 Sentinel 节点都是无状态的。在 Sentinel 中会配置 Master 的地址，Sentinel 会时刻监控 Master 的状态，当发现 Master 在配置的时间间隔内无响应，就认为 Master 已经挂了，Sentinel 会从从节点中选取一个提升为主节点，并且把所有其他的从节点作为新主的从节点。Sentinel 集群内部在仲裁的时候，会根据配置的值来决定当有几个 Sentinel 节点认为主挂掉可以做主从切换的操作，也就是集群内部需要对缓存节点的状态达成一致才行。 Redis Sentinel 不属于代理层模式，因为对于缓存的写入和读取请求不会经过 Sentinel 节点。Sentinel 节点在架构上和主从是平级的，是作为管理者存在的，所以可以认为是在服务端提供的一种高可用方案。","link":"/2019/12/04/2019-12-04-分布式缓存高可用/"},{"title":"[片段] Mybatis ParameterHandler 实践","text":"用来批量加密用@Decrypted注解的String字段，可能还有一些坑。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228import com.google.common.cache.CacheBuilder;import com.google.common.cache.CacheLoader;import com.google.common.cache.LoadingCache;import com.ke.zhaopin.manage.server.config.mybatis.interceptor.anno.Decrypted;import com.lianjia.ctt.kinko.spi.CipherSpi;import com.sun.istack.internal.NotNull;import lombok.Getter;import lombok.extern.slf4j.Slf4j;import org.apache.commons.lang3.StringUtils;import org.apache.ibatis.binding.MapperMethod;import org.apache.ibatis.executor.parameter.ParameterHandler;import org.apache.ibatis.plugin.*;import org.apache.ibatis.reflection.MetaObject;import org.apache.ibatis.scripting.defaults.DefaultParameterHandler;import org.apache.ibatis.session.Configuration;import org.apache.ibatis.session.defaults.DefaultSqlSession;import org.joor.Reflect;import reactor.core.publisher.Flux;import java.lang.reflect.Array;import java.lang.reflect.Field;import java.sql.PreparedStatement;import java.util.*;import java.util.concurrent.ExecutionException;import java.util.concurrent.TimeUnit;import java.util.function.Function;import java.util.stream.Collectors;@Intercepts({ @Signature(type = ParameterHandler.class, method = \"setParameters\", args = {PreparedStatement.class}),})@Slf4jpublic class EncryptInterceptor implements Interceptor { private static final String COLLECTION_KEY = \"collection\"; private static final String ARRAY_KEY = \"array\"; private final LoadingCache&lt;Class, List&lt;String&gt;&gt; decryptFieldCaches = CacheBuilder.newBuilder() .maximumSize(200) .expireAfterAccess(10L, TimeUnit.MINUTES) .build(new CacheLoader&lt;Class, List&lt;String&gt;&gt;() { @Override public List&lt;String&gt; load(Class key) { return Arrays.stream(key.getDeclaredFields()) .filter(f -&gt; f.getAnnotation(Decrypted.class) != null) .filter(f -&gt; { boolean isString = f.getType() == String.class; if (!isString) { log.warn(f.getName() + \"is not String, actual type is \" + f.getType().getSimpleName() + \" ignored\"); } return isString; }) .map(Field::getName) .collect(Collectors.toList()); } } ); private CipherSpi cipherSpi; public EncryptInterceptor(CipherSpi cipherSpi) { this.cipherSpi = cipherSpi; } @Override public Object intercept(Invocation invocation) throws Throwable { Flux&lt;CryptContext&gt; contextFlux = Flux.empty(); do { if (!(invocation.getTarget() instanceof DefaultParameterHandler)) break; final Reflect parameterHandler = Reflect.on(invocation.getTarget()); final Object parameterObject = parameterHandler.get(\"parameterObject\"); final Configuration configuration = parameterHandler.get(\"configuration\"); if (parameterObject instanceof DefaultSqlSession.StrictMap) { // 单个Collection/Map/Array参数 DefaultSqlSession.StrictMap&lt;?&gt; paramMap = (DefaultSqlSession.StrictMap&lt;?&gt;) parameterObject; Collection&lt;?&gt; collection = null; Class&lt;?&gt; componentType = null; if (paramMap.containsKey(COLLECTION_KEY)) { collection = (Collection&lt;?&gt;) paramMap.get(COLLECTION_KEY); componentType = collection.iterator().next().getClass(); } else if (paramMap.containsKey(ARRAY_KEY)) { Object[] array = (Object[]) paramMap.get(ARRAY_KEY); componentType = array.getClass().getComponentType(); collection = Arrays.asList(array); } if (!isUserDefinedClass(componentType)) break; contextFlux = collection(configuration, collection, componentType); } else if (parameterObject instanceof MapperMethod.ParamMap) { // 多个参数 MapperMethod.ParamMap&lt;?&gt; paramMap = (MapperMethod.ParamMap&lt;?&gt;) parameterObject; final List&lt;?&gt; params = paramMap.values().stream().filter(Objects::nonNull).distinct().collect(Collectors.toList()); for (Object parameter : params) { if (parameter instanceof Collection) { Collection&lt;?&gt; collection = (Collection&lt;?&gt;) parameter; if (collection.isEmpty()) { continue; } Class&lt;?&gt; componentType = collection.iterator().next().getClass(); if (!isUserDefinedClass(componentType)) { continue; } final Flux&lt;CryptContext&gt; collectionFlux = collection(configuration, collection, componentType); contextFlux = contextFlux.concatWith(collectionFlux); } else if (parameter.getClass().isArray()) { if (Array.getLength(parameter) == 0) continue; final Class&lt;?&gt; componentType = parameter.getClass().getComponentType(); if (!isUserDefinedClass(componentType)) { continue; } Collection&lt;?&gt; collection = Arrays.asList((Object[]) parameter); final Flux&lt;CryptContext&gt; collectionFlux = collection(configuration, collection, componentType); contextFlux = contextFlux.concatWith(collectionFlux); } else if (isUserDefinedClass(parameter.getClass())) { final Flux&lt;CryptContext&gt; singleFlux = collection(configuration, Collections.singletonList(parameter), parameter.getClass()); contextFlux = contextFlux.concatWith(singleFlux); } } } else if (isUserDefinedClass(parameterObject.getClass())) { // 单个非Collection/Map/Array参数 contextFlux = collection(configuration, Collections.singletonList(parameterObject), parameterObject.getClass()); } else { // 不是用interface的情况 } } while (false); final List&lt;CryptContext&gt; cryptContexts = encrypt(contextFlux); invocation.proceed(); restore(cryptContexts); return null; } private void restore(List&lt;CryptContext&gt; cryptContexts) { for (CryptContext cryptContext : cryptContexts) { cryptContext.metaObject.setValue(cryptContext.fieldName, cryptContext.value); } } private Flux&lt;CryptContext&gt; collection(Configuration configuration, Collection&lt;?&gt; collection, Class&lt;?&gt; componentType) throws ExecutionException { final List&lt;String&gt; fieldNames = this.getDecryptFields(componentType); return Flux.fromIterable(collection) .map(configuration::newMetaObject) .flatMapIterable(metaObject -&gt; fieldNames.stream().map(fieldName -&gt; new CryptContext(metaObject, fieldName)).collect(Collectors.toList())); } private List&lt;CryptContext&gt; encrypt(Flux&lt;CryptContext&gt; contextFlux) { return contextFlux .filter(context -&gt; StringUtils.isNotBlank(context.value)) .buffer(1000) .doOnNext(contexts -&gt; { Map&lt;String, String&gt; secretMap = Collections.emptyMap(); try { secretMap = cipherSpi.batchEncrypt(contexts.stream().map(CryptContext::getValue).distinct().collect(Collectors.toList())); } catch (Exception e) { } for (CryptContext context : contexts) { context.secret = secretMap.get(context.value); } }) .flatMapIterable(Function.identity()) .doOnNext(context -&gt; context.metaObject.setValue(context.fieldName, context.secret)) .collectList() .block(); } @NotNull private List&lt;String&gt; getDecryptFields(Class&lt;?&gt; modelClazz) throws ExecutionException { return this.decryptFieldCaches.get(modelClazz); } private boolean isUserDefinedClass(Class&lt;?&gt; clazz) { return !clazz.isPrimitive() &amp;&amp; !clazz.getPackage().getName().startsWith(\"java\"); } @Override public Object plugin(Object target) { return Plugin.wrap(target, this); } @Override public void setProperties(Properties properties) { }}@Getterclass CryptContext { CryptContext(MetaObject metaObject, String fieldName) { this.metaObject = metaObject; this.fieldName = fieldName; this.value = (String) metaObject.getValue(fieldName); if (StringUtils.isBlank(value)) { this.secret = StringUtils.EMPTY; } } final MetaObject metaObject; final String fieldName; final String value; String secret;}","link":"/2019/12/04/2019-12-04-Mybatis ParameterInterceptor/"},{"title":"缓存专题(五) 如何解决缓存穿透","text":"什么是缓存穿透缓存穿透其实是指从缓存中没有查到数据，而不得不从后端系统（比如数据库）中查询的情况。你可以把数据库比喻为手机，它是经受不了太多的划痕和磕碰的，所以你需要给它贴个膜再套个保护壳，就能对手机起到一定的保护作用了。 不过，少量的缓存穿透不可避免，对系统也是没有损害的，主要有几点原因： 一方面，互联网系统通常会面临极大数据量的考验，而缓存系统在容量上是有限的，不可能存储系统所有的数据，那么在查询未缓存数据的时候就会发生缓存穿透。 另一方面，互联网系统的数据访问模型一般会遵从“80/20 原则”。“80/20 原则”又称为帕累托法则，是意大利经济学家帕累托提出的一个经济学的理论。它是指在一组事物中，最重要的事物通常只占 20%，而剩余的 80% 的事物确实不重要的。把它应用到数据访问的领域，就是我们会经常访问 20% 的热点数据，而另外的 80% 的数据则不会被经常访问。比如你买了很多衣服，很多书，但是其实经常穿的，经常看的，可能也就是其中很小的一部分。 既然缓存的容量有限，并且大部分的访问只会请求 20% 的热点数据，那么理论上说，我们只需要在有限的缓存空间里存储 20% 的热点数据就可以有效地保护脆弱的后端系统了，也就可以放弃缓存另外 80% 的非热点数据了。所以，这种少量的缓存穿透是不可避免的，但是对系统是没有损害的。 那么什么样的缓存穿透对系统有害呢？答案是大量的穿透请求超过了后端系统的承受范围，造成了后端系统的崩溃。如果把少量的请求比作毛毛细雨，那么一旦变成倾盆大雨，引发洪水，冲倒房屋，肯定就不行了。 产生这种大量穿透请求的场景有很多，接下来，我就带你解析这几种场景以及相应的解决方案。 缓存穿透的解决方案先来考虑这样一种场景：在你的电商系统的用户表中，我们需要通过用户 ID 查询用户的信息，缓存的读写策略采用 Cache Aside 策略。 那么，如果要读取一个用户表中未注册的用户，会发生什么情况呢？按照这个策略，我们会先读缓存，再穿透读数据库。由于用户并不存在，所以缓存和数据库中都没有查询到数据，因此也就不会向缓存中回种数据（也就是向缓存中设置值的意思），这样当再次请求这个用户数据的时候还是会再次穿透到数据库。在这种场景下，缓存并不能有效地阻挡请求穿透到数据库上，它的作用就微乎其微了。 那如何解决缓存穿透呢？一般来说我们会有两种解决方案：回种空值以及使用布隆过滤器。 我们先来看看第一种方案。 回种空值回顾上面提到的场景，你会发现最大的问题在于数据库中并不存在用户的数据，这就造成无论查询多少次，数据库中永远都不会存在这个用户的数据，穿透永远都会发生。 类似的场景还有一些：比如由于代码的 bug 导致查询数据库的时候抛出了异常，这样可以认为从数据库查询出来的数据为空，同样不会回种缓存。 那么，当我们从数据库中查询到空值或者发生异常时，我们可以向缓存中回种一个空值。但是因为空值并不是准确的业务数据，并且会占用缓存的空间，所以我们会给这个空值加一个比较短的过期时间，让空值在短时间之内能够快速过期淘汰。下面是这个流程的伪代码： 1234567891011Object nullValue = new Object();try { Object valueFromDB = getFromDB(uid); // 从数据库中查询数据 if (valueFromDB == null) { cache.set(uid, nullValue, 10); // 如果从数据库中查询到空值，就把空值写入缓存，设置较短的超时时间 } else { cache.set(uid, valueFromDB, 1000); }} catch(Exception e) { cache.set(uid, nullValue, 10);} 回种空值虽然能够阻挡大量穿透的请求，但如果有大量获取未注册用户信息的请求，缓存内就会有有大量的空值缓存，也就会浪费缓存的存储空间，如果缓存空间被占满了，还会剔除掉一些已经被缓存的用户信息反而会造成缓存命中率的下降。 所以这个方案，我建议你在使用的时候应该评估一下缓存容量是否能够支撑。如果需要大量的缓存节点来支持，那么就无法通过通过回种空值的方式来解决，这时你可以考虑使用布隆过滤器。 使用布隆过滤器1970 年布隆提出了一种布隆过滤器的算法，用来判断一个元素是否在一个集合中。这种算法由一个二进制数组和一个 Hash 算法组成。它的基本思路如下： 我们把集合中的每一个值按照提供的 Hash 算法算出对应的 Hash 值，然后将 Hash 值对数组长度取模后得到需要计入数组的索引值，并且将数组这个位置的值从 0 改成 1。在判断一个元素是否存在于这个集合中时，你只需要将这个元素按照相同的算法计算出索引值，如果这个位置的值为 1 就认为这个元素在集合中，否则则认为不在集合中。 下图是布隆过滤器示意图，我来带你分析一下图内的信息。 A、B、C 等元素组成了一个集合，元素 D 计算出的 Hash 值所对应的的数组中值是 1，所以可以认为 D 也在集合中。而 F 在数组中的值是 0，所以 F 不在数组中。 那么我们如何使用布隆过滤器来解决缓存穿透的问题呢？ 还是以存储用户信息的表为例进行讲解。首先，我们初始化一个很大的数组，比方说长度为 20 亿的数组，接下来我们选择一个 Hash 算法，然后我们将目前现有的所有用户的 ID 计算出 Hash 值并且映射到这个大数组中，映射位置的值设置为 1，其它值设置为 0。 新注册的用户除了需要写入到数据库中之外，它也需要依照同样的算法更新布隆过滤器的数组中，相应位置的值。那么当我们需要查询某一个用户的信息时，我们首先查询这个 ID 在布隆过滤器中是否存在，如果不存在就直接返回空值，而不需要继续查询数据库和缓存，这样就可以极大地减少异常查询带来的缓存穿透。 布隆过滤器拥有极高的性能，无论是写入操作还是读取操作，时间复杂度都是 O(1)，是常量值。在空间上，相对于其他数据结构它也有很大的优势，比如，20 亿的数组需要 2000000000/8/1024/1024 = 238M 的空间，而如果使用数组来存储，假设每个用户 ID 占用 4 个字节的空间，那么存储 20 亿用户需要 2000000000 * 4 / 1024 / 1024 = 7600M 的空间，是布隆过滤器的 32 倍。 不过，任何事物都有两面性，布隆过滤器也不例外，它主要有两个缺陷： 它在判断元素是否在集合中时是有一定错误几率的，比如它会把不是集合中的元素判断为处在集合中； 不支持删除元素。 关于第一个缺陷，主要是 Hash 算法的问题。因为布隆过滤器是由一个二进制数组和一个 Hash 算法组成的，Hash 算法存在着一定的碰撞几率。Hash 碰撞的含义是不同的输入值经过 Hash 运算后得到了相同的 Hash 结果。 本来，Hash 的含义是不同的输入，依据不同的算法映射成独一无二的固定长度的值，也就是我输入字符串“1”，根据 CRC32 算法，值是 2212294583。但是现实中 Hash 算法的输入值是无限的，输出值的值空间却是固定的，比如 16 位的 Hash 值的值空间是 65535，那么它的碰撞几率就是 1/65535，即如果输入值的个数超过 65535 就一定会发生碰撞。 那么你可能会问为什么不映射成更长的 Hash 值呢？ 因为更长的 Hash 值会带来更高的存储成本和计算成本。即使使用 32 位的 Hash 算法，它的值空间长度是 2 的 32 次幂减一，约等于 42 亿，用来映射 20 亿的用户数据，碰撞几率依然有接近 50%。 Hash 的碰撞就造成了两个用户 ID ，A 和 B 会计算出相同的 Hash 值，那么如果 A 是注册的用户，它的 Hash 值对应的数组中的值是 1，那么 B 即使不是注册用户，它在数组中的位置和 A 是相同的，对应的值也是 1，这就产生了误判。 布隆过滤器的误判有一个特点，就是它只会出现“false positive”的情况。这是什么意思呢？当布隆过滤器判断元素在集合中时，这个元素可能不在集合中。但是一旦布隆过滤器判断这个元素不在集合中时，它一定不在集合中。这一点非常适合解决缓存穿透的问题。为什么呢？ 你想，如果布隆过滤器会将集合中的元素判定为不在集合中，那么我们就不确定，被布隆过滤器判定为不在集合中的元素，是不是在集合中。假设在刚才的场景中，如果有大量查询未注册的用户信息的请求存在，那么这些请求到达布隆过滤器之后，即使布隆过滤器判断为不是注册用户，那么我们也不确定它是不是真的不是注册用户，那么就还是需要去数据库和缓存中查询，这就使布隆过滤器失去了价值。 所以你看，布隆过滤器虽然存在误判的情况，但是还是会减少缓存穿透的情况发生，只是我们需要尽量减少误判的几率，这样布隆过滤器的判断正确的几率更高，对缓存的穿透也更少。一个解决方案是： 使用多个 Hash 算法为元素计算出多个 Hash 值，只有所有 Hash 值对应的数组中的值都为 1 时，才会认为这个元素在集合中。 布隆过滤器不支持删除元素的缺陷也和 Hash 碰撞有关。给你举一个例子，假如两个元素 A 和 B 都是集合中的元素，它们有相同的 Hash 值，它们就会映射到数组的同一个位置。这时我们删除了 A，数组中对应位置的值也从 1 变成 0，那么在判断 B 的时候发现值是 0，也会判断 B 是不在集合中的元素，就会得到错误的结论。 那么我是怎么解决这个问题的呢？我会让数组中不再只有 0 和 1 两个值，而是存储一个计数。比如如果 A 和 B 同时命中了一个数组的索引，那么这个位置的值就是 2，如果 A 被删除了就把这个值从 2 改为 1。这个方案中的数组不再存储 bit 位，而是存储数值，也就会增加空间的消耗。所以，你要依据业务场景来选择是否能够使用布隆过滤器，比如像是注册用户的场景下，因为用户删除的情况基本不存在，所以还是可以使用布隆过滤器来解决缓存穿透的问题的。 讲了这么多，关于布隆过滤器的使用上，我也给你几个建议： 选择多个 Hash 函数计算多个 Hash 值，这样可以减少误判的几率； 布隆过滤器会消耗一定的内存空间，所以在使用时需要评估你的业务场景下需要多大的内存，存储的成本是否可以接受。 总的来说，回种空值和布隆过滤器是解决缓存穿透问题的两种最主要的解决方案，但是它们也有各自的适用场景，并不能解决所有问题。比方说当有一个极热点的缓存项，它一旦失效会有大量请求穿透到数据库，这会对数据库造成瞬时极大的压力，我们把这个场景叫做“dog-pile effect”（狗桩效应）， 这是典型的缓存并发穿透的问题，那么，我们如何来解决这个问题呢？解决狗桩效应的思路是尽量地减少缓存穿透后的并发，方案也比较简单： 在代码中，控制在某一个热点缓存项失效之后启动一个后台线程，穿透到数据库，将数据加载到缓存中，在缓存未加载之前，所有访问这个缓存的请求都不再穿透而直接返回。 通过在 Memcached 或者 Redis 中设置分布式锁，只有获取到锁的请求才能够穿透到数据库。 分布式锁的方式也比较简单，比方说 ID 为 1 的用户是一个热点用户，当他的用户信息缓存失效后，我们需要从数据库中重新加载数据时，先向 Memcached 中写入一个 Key 为”lock.1”的缓存项，然后去数据库里面加载数据，当数据加载完成后再把这个 Key 删掉。这时，如果另外一个线程也要请求这个用户的数据，它发现缓存中有 Key 为“lock.1”的缓存，就认为目前已经有线程在加载数据库中的值到缓存中了，它就可以重新去缓存中查询数据，不再穿透数据库了。","link":"/2019/12/04/2019-12-04-缓存穿透解决方案/"},{"title":"高性能MySql 4至6章读书笔记","text":"第四章 Schema设计选择优化的数据类型更小的通常更好，但是要确保没有低估需要存储的值的范围，因为在schema中的多个地方增加数据类型范围是一个十分耗时的操作。 简单就好简单的数据类型的操作通常需要更少的cpu时间。 尽量避免NULL可为NULL的列使得索引、索引比统计和值比较都更为复杂。 当然也有例外，InnoDB使用单独的位存储NULL值， 所以对于稀疏数据有很好的空间效率。 选择标识符一旦选定一种类型，要确保在所有的关联表中都使用同样的类型。类型之间需要精确匹配，包括像UNSIGNED这样的属性。尽量只用整型定义标识符。 注意可变长字符串其在临时表和排序时可能导致悲观的按最大长度分配内存 范式与反范式范式是好的，但是反范式有时也是必须的，并且能带来好处。 第五章 创建高性能索引B-Tree 索引的查询类型 全值匹配： 指的是和索引的所有列进行匹配 匹配最左前缀： 查找索引前几列进行匹配 匹配列前缀: 只匹配某一列的值的开头部分 匹配范围值： 查找索引某一范围的值 精确匹配某一列并范围匹配另外一列 只访问索引的查询：覆盖索引 B-Tree 索引的限制 如果不是按照索引的最左列开始查找，则无法使用索引 不能跳过索引中的列 如果查询中有某个列的范围查询，则其右边的所有列都无法使用索引： 例如查询 索引为key(last_name, fisrt_name, dob) 1where last_name = &apos;a&apos; and first_name like &apos;J%&apos; and dob = &apos;1877-12-23&apos; 索引的优点 大大减少服务器需要扫描的数据量 帮助服务器避免排序和临时表 将随机I/O变为顺序I/O 高性能索引-独立的列如果查询中列不是独立的，则mysql不会使用索引 1select actor_id from sakila.actor where actor_id + 1 = 5; 高性能索引-前缀索引和索引选择性有时候需要索引很长的字符列，通常可以索引开始部分的字符，同时也会降低索引的选择性。 索引的选择性指的是，不重复的索引值和数据表的记录总数的比值。索引的选择性越高表示查询效率越高，因为选择性高的索引可以过滤掉更多的行。 前缀索引是一种能使索引更小，更快的有效方法，但是也有其缺点：前缀索引无法做order by 和 group by，也无法使用前缀索引做覆盖索引。 高性能索引-多列索引最容易遇到的困惑是多列索引的顺序，正确的顺序依赖于使用索引的查询，同时需要考虑如何更好的满足排序和分组需要。对于如何选择索引的列顺序有一个经验法则：将选择性最高的列放在索引的最前列。只有不需要考虑排序和分组时，将选择性跟高的列放在最前面通常是最好的，但是考虑问题需要更全面，避免随机I/O和排序更加重要。 高性能索引-覆盖索引如果一个索引包含所需要查询的字段的值，我们就可以称之为“覆盖索引” 覆盖索引的好处： 索引条目通常远小于数据行大小，所以如果只需要读取索引，那mysql就会极大的减少数据访问量。 因为索引是按照列值顺序存储的，所以对于I/O密集型范围查询回避随机从磁盘读取每一行数据的I/O要小的多 由于Innodb的聚簇索引，覆盖索引对Innodb表特别有用，可以避免对主键索引的二次查询。 覆盖索引的陷阱： 1select * from products where actor = &apos;SEAN CARREY&apos; and title like &apos;%APOLLO%&apos;; 没有索引能够覆盖这个查询，因为查询从表中选择了所有的列 mysql不能再索引中执行like操作，只能做最左前缀匹配 高性能索引-使用索引扫描来做排序mysql有两种方式可以生成有序的结果：通过排序操作；或者使用索引顺序扫描。mysql可以使用同一个索引既满足排序，有用于查找行。 只有当索引的列顺序和ORDER BY子句的顺序完全一致，并且所有列的排序方向都一样时，mysql才能使用索引来对结果做排序。如果查询需要关联多张表，则只有当ORDER BY子句引用的字段全部为第一个表时，才能用索引做排序。ORDER BY子句和查询的限制是一样的：需要满足索引的最左前缀的要求。 有一种情况下ORDER BY子句可以不满足索引的最左前缀要求： 12select rental_id, staff_id from sakila.rental where rental_date = &apos;2005-05-25&apos;order by inventory_id, customer_id; 索引为key(rental_date, inventory_id, customer_id)，前导列为常量的时候，如果where子句或者join子句中对这些列指定了常量，就可以弥补ORDER BY的不足。 12where rental_date &gt; &apos;2005-12-25&apos; order by inventory_id, customer_id;where rental_date = &apos;2005-12-25&apos; and inventory_id in (1, 2) order by cusomter_id; 对于索引上是范围查询，mysql无法使用之后的索引列 高性能索引-使用索引扫描减少锁索引可以让查询锁定更少的行，如果你的查询从不访问那些不需要的行，那么就会锁定更少的行。但这只有当innoDB在存储引擎层能够过滤掉所有不需要的行是才有效。如果索引无法过滤掉无效的行，那么innoDB检索到数据并返回给服务器层后，innoDB已经锁定这些行了（mysql 5.6后没有这个问题）。 高性能索引-避免多个范围条件下面的查询： 1where last_online &gt; date_sub(now(), interval 7 day) and age bwtween 18 and 25 这个查询有一个问题：它有两个范围条件,last_online和age列，mysql可以使用last_online的索引或者是age列的索引，但是无法同时使用它们。 高性能索引-延迟关联优化分页如果一个查询匹配结果有上百万行的话会怎样？ 1select * from profiles where sex = &apos;m&apos; order by rating limit 10; 即使有索引，如果用户界面需要翻页，并且翻页到比较靠后的地方也会非常慢，如： 1select * from profiles where sex = &apos;m&apos; order by rating limit 1000000, 10; 无论如何创建索引，这种查询都是个严重的问题，mysql需要花费大量时间来扫描需要丢弃的数据。其中一个解决的办法是限制能够翻页的数量。 优化这类索引的另一个比较好的策略是使用延迟关联，通过使用覆盖索引返回需要的主建，再根据这些主建回主表获得需要的行，这样可以减少mysql扫描需要丢弃的行数。 12345select * from profiles innner join ( select id fomr profiles p where p.sex = &apos;m&apos; order by rating limit 1000000, 10) as t using (id); 第六章 慢查询优化优化数据访问查询性能低下最基本的原因是访问数据太多。某些查询可能不可避免的需要筛选大量数据，单这并不常见。大部分性能低下的查询都可以通过减少访问的数据量的方式进行优化。对于低效的查询，我们发现通过下面几个步骤来分析总是很有效： 确认应用程序时候检索大量超过需要的数据。 确认mysql服务层是否在分许大量超过需要的数据行。 第一种情况可以使用limit和选择需要的列来解决。在确定查询只返回需要的数据之后，接下来应该看看查询为了返回结果是否扫描了过多的数据，对于mysql有三个衡量查询开销的指标： 响应时间 扫描的行数 返回的行数 没有那个指标能完美地衡量查询的开销，但它们大致反映了mysql内部查询时需要访问多少数据，并可以大概推算出查询运行的时间。 在评估查询开销的时候，需要考虑一下从表中找到某一行数据的成本。mysql有好几种访问方式可以查找并返回一行结果。有些访问方式可能需要扫描很多行才能返回一行结果，也有些访问方式可能无需扫描就能返回结果。 在explain语句中的type列反应了访问类型。访问类型有很多种，从全表扫描到索引扫描、范围扫描、唯一索引查询、常数引用等。这里列的这些，速度是从慢到快，扫描行数也是从大到小。如果查询没有办法找到合适是访问类型，那么解决的最好办法通常是增加一个合适的索引。 一般mysql能够使用如下三种方式应用where条件，从好到坏依次为： 从索引中使用where条件吗来过滤不匹配的记录，这是在存储引擎层完成的。 使用索引覆盖扫描来返回记录（extra出现using index），直接从索引中过滤不需要的数据并返回命中结果。这是在mysql服务层完成的，但无需再回表查询记录。 从数据表中返回数据，然后过滤不满足条件的记录（extra出现using where）。这是在mysql服务器层完成，mysql需要从数据表中读出记录然后过滤。 如果发现查询需要扫描大量的数据但只返回少数的行，那么可以使用下面的技巧去优化它： 使用覆盖索引，把需要用的列都放到索引中 改变库表结构。例如使用单独的汇总表 重写复杂的查询， 让mysql优化器能够以更加高效的方式执行这个查询 重构查询方式-切分查询有时候对于一个大查询我们需要分而治之，将大查询分成小查询。删除旧数据是一个很好的例子，如果用一个大的语句一次性完成，则可能一次需要锁住很多数据、占满整个事务日志、耗尽系统资源、阻塞恨到重要的查询。 重构查询方式-分解关联查询例如下面这个查询： 1234select * from tagjoin tag_post on tag_post.tag_id = tag.idjoin post on tag_post.post_id = post.idwhere tag.tag = &apos;mysql&apos;; 可以分解成下面这个查询来代替： 12345select * from tag where tag = &apos;mysql&apos;;select * from tag_post where tag_id = 1234;select * from post where post.id in (123,456,567); 用分解关联查询的方式重构查询有如下的优势： 让缓存效率更高。许多应用程序可以方便地缓存单表查询对应的结果对象。 将查询分解后，执行单个查询可以减少锁的竞争。 在应用层做关联，可以更容易对数据库进行拆分，更容易做到高性能和高扩展。 查询本身效率可能提升，使用in代替关联查询，可能比随机关联更高效。 减少冗余记录的查询 相当于在应用层实现了哈希关联 重构查询方式-优化关联查询 确保on或者using子句上的列上有索引。 确保任何group by和 order by 的表达式中只涉及到一个表中的列，这样mysql才有可能使用索引来优化这个过程","link":"/2020/01/28/2020-01-28-高性能MySql 4~6章/"},{"title":"AbstractQueuedSynchronizer解析","text":"AbstractQueuedSynchronizer 数据结构123456789101112131415161718 /** * Head of the wait queue , lazily initialized . Except for * initialization , it is modified only via method setHead . Note : * If head exists , its waitStatus is guaranteed not to be * CANCELLED . */ private transient volatile Node head; /** * Tail of the wait queue , lazily initialized . Modified only via * method enq to add new wait node . */ private transient volatile Node tail; /** * The synchronization state . */ private volatile int state; 稍微注意下在线程争用锁是才会初始化链表 AbstractQueuedSynchronizer.Node 数据结构123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081/** * Status field , taking on only the values : * SIGNAL : The successor of this node is ( or will soon be ) * blocked ( via park ), so the current node must * unpark its successor when it releases or * cancels . To avoid races , acquire methods must * first indicate they need a signal , * then retry the atomic acquire , and then , * on failure , block . * CANCELLED : This node is cancelled due to timeout or interrupt . * Nodes never leave this state . In particular , * a thread with cancelled node never again blocks . * CONDITION : This node is currently on a condition queue . * It will not be used as a sync queue node * until transferred , at which time the status * will be set to 0. ( Use of this value here has * nothing to do with the other uses of the * field , but simplifies mechanics .) * PROPAGATE : A releaseShared should be propagated to other * nodes . This is set ( for head node only ) in * doReleaseShared to ensure propagation * continues , even if other operations have * since intervened . * 0: None of the above * * The values are arranged numerically to simplify use . * Non - negative values mean that a node doesn ' t need to * signal . So , most code doesn ' t need to check for particular * values , just for sign . * * The field is initialized to 0 for normal sync nodes , and * CONDITION for condition nodes . It is modified using CAS * ( or when possible , unconditional volatile writes ). */ volatile int waitStatus; /** * Link to predecessor node that current node / thread relies on * for checking waitStatus . Assigned during enqueuing , and nulled * out ( for sake of GC ) only upon dequeuing . Also , upon * cancellation of a predecessor , we short - circuit while * finding a non - cancelled one , which will always exist * because the head node is never cancelled : A node becomes * head only as a result of successful acquire . A * cancelled thread never succeeds in acquiring , and a thread only * cancels itself , not any other node . */ volatile Node prev; /** * Link to the successor node that the current node / thread * unparks upon release . Assigned during enqueuing , adjusted * when bypassing cancelled predecessors , and nulled out ( for * sake of GC ) when dequeued . The enq operation does not * assign next field of a predecessor until after attachment , * so seeing a null next field does not necessarily mean that * node is at end of queue . However , if a next field appears * to be null , we can scan prev ' s from the tail to * double - check . The next field of cancelled nodes is set to * point to the node itself instead of null , to make life * easier for isOnSyncQueue . */ volatile Node next; /** * The thread that enqueued this node . Initialized on * construction and nulled out after use . */ volatile Thread thread; /** * Link to next node waiting on condition , or the special * value SHARED . Because condition queues are accessed only * when holding in exclusive mode , we just need a simple * linked queue to hold nodes while they are waiting on * conditions . They are then transferred to the queue to * re - acquire . And because conditions can only be exclusive , * we save a field by using special value to indicate shared * mode . */ Node nextWaiter; AbstractQueuedSynchronizer** 的数据结构（盗用的图） AbstractQueuedSynchronizer 做了什么 ?内部维护state和CLH队列，负责在资源争用时线程入队，资源释放时唤醒队列中线程。 而实现类只需要实现 什么条件获取资源成功 和 什么条件释放资源 成功就可以了 所以，最简单的CountDownLatch使用AbstractQueuedSynchronizer实现非常简单： 申明AbstractQueuedSynchronizer的state数量（比如十个） await方法尝试获取资源，如果state&gt;0表示获取失败（ 什么条件获取资源成功 ，CountDownLatch实现），获取失败线程休眠（AbstractQueuedSynchronizer负责） countDown方法state-1，如果state==0表示资源释放成功( 什么条件释放资源成功 ，CountDownLatch实现)，唤醒队列中所有线程（AbstractQueuedSynchronizer负责） AbstractQueuedSynchronizer 怎么做的?顺着ReentrantLock lock、unlock看一遍我们就大致总结出AbstractQueuedSynchronizer工作原理了 先简单介绍下ReentrantLock特性：可重入，中断，有超时机制。 ReentrantLock lock() 流程 ( 再盗图 ) 黄色表示ReentrantLock实现，绿色表示AbstractQueuedSynchronizer内部实现 lock方法入口 直接调用 AbstractQueuedSynchronizer.acquire方法 tryAcquire addWaiter acquireQueued AbstractQueuedSynchronizer.acquire12345**public** final void acquire ( int arg) { **if** (! tryAcquire (arg) &amp;&amp; acquireQueued ( addWaiter ( Node . EXCLUSIVE ), arg)) selfInterrupt (); } 获取的锁的逻辑：直接获取成功则返回，如果没有获取成功入队休眠（对就是这么简单） 下面我们仔细一个一个方法看 ReentrantLock.tryAcquire我这里贴的时非公平的所获取，公平和不公平的区别在于公平锁老老实实的会进入队列排队，非公平锁会先检查资源是否可用，如果可用不管队列中的情况直接尝试获取锁。 123456789101112131415161718final boolean nonfairTryAcquire ( int acquires) { final Thread current = Thread . currentThread (); int c = getState (); if (c == 0 ) { if ( compareAndSetState ( 0 , acquires)) { setExclusiveOwnerThread (current); return true ; } } else if (current == getExclusiveOwnerThread ()) { int nextc = c + acquires; if (nextc &lt; 0 ) // overflow throw new Error ( \"Maximum lock count exceeded\" ); setState (nextc); return true ; } return false ; } ReentrantLock.tryAcquire读取到state==0时尝试占用锁，并保证同一线程可以重复占用。其他情况下获取资源失败。如果获取成功就没啥事了，不过关键不就是锁争用的时候是如何处理的吗？ AbstractQueuedSynchronizer.addWaiter(Node.EXCLUSIVE)12345678910111213141516private Node addWaiter ( Node mode) { Node node = new Node (mode); for (;;) { Node oldTail = tail; if (oldTail != null ) { node. setPrevRelaxed (oldTail); if ( compareAndSetTail (oldTail, node)) { oldTail. next = node; return node; } } else { initializeSyncQueue (); } } } 一旦锁争用，一定会初始化队列（因为排队的线程需要前驱节点唤醒，所以要初始化一个前驱节点），之后自旋成为队列尾节点。 简单来说就是获取不到锁就放进队列里维护起来，等锁释放的时候再用。 这里还有一个 很具有参考性的小细节 ：先设置新节点的前驱结点，自旋成为尾节点后设置前驱的后驱 AbstractQueuedSynchronizer.acquireQueued1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253final boolean acquireQueued ( final Node node, int arg) { boolean interrupted = false ; try { for (;;) { final Node p = node. predecessor (); if (p == head &amp;&amp; tryAcquire (arg)) { setHead (node); p. next = null ; // help GC return interrupted; } if ( shouldParkAfterFailedAcquire (p, node)) interrupted |= parkAndCheckInterrupt (); } } catch ( Throwable t) { cancelAcquire (node); if (interrupted) selfInterrupt (); throw t; } } private static boolean shouldParkAfterFailedAcquire ( Node pred, Node node) { int ws = pred. waitStatus ; if (ws == Node . SIGNAL ) /* * This node has already set status asking a release * to signal it, so it can safely park. */ return true ; if (ws &gt; 0 ) { /* * Predecessor was cancelled. Skip over predecessors and * indicate retry. */ do { node. prev = pred = pred. prev ; } while (pred. waitStatus &gt; 0 ); pred. next = node; } else { /* * waitStatus must be 0 or PROPAGATE. Indicate that we * need a signal, but don't park yet. Caller will need to * retry to make sure it cannot acquire before parking. */ pred. compareAndSetWaitStatus (ws, Node . SIGNAL ); } return false ; } private final boolean parkAndCheckInterrupt () { LockSupport . park ( this ); return Thread . interrupted (); } 前面只是维护下链表数据结构，这里负责找到合适的唤醒前驱，然后让线程休眠。 这里主要是一个循环过程： 检查是否能获取到锁，获取到则返回 失败则寻找前面最近的未放弃争用的前驱，把前驱的waitStatus设置为-1，并把放弃争用的节点抛弃 检查是否能休眠 使用Usafe.park休眠（不是wait） ReentrantLock lock 总结 ReentrantLock unlock()12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849public final boolean release ( int arg) { if ( tryRelease (arg)) { Node h = head; if (h != null &amp;&amp; h. waitStatus != 0 ) unparkSuccessor (h); return true ; } return false ; } protected final boolean tryRelease ( int releases) { int c = getState () - releases; if ( Thread . currentThread () != getExclusiveOwnerThread ()) throw new IllegalMonitorStateException (); boolean free = false ; if (c == 0 ) { free = true ; setExclusiveOwnerThread ( null ); } setState (c); return free; } private void unparkSuccessor ( Node node) { /* * If status is negative (i.e., possibly needing signal) try * to clear in anticipation of signalling. It is OK if this * fails or if status is changed by waiting thread. */ int ws = node. waitStatus ; if (ws &lt; 0 ) node. compareAndSetWaitStatus (ws, 0 ); /* * Thread to unpark is held in successor, which is normally * just the next node. But if cancelled or apparently null, * traverse backwards from tail to find the actual * non-cancelled successor. */ Node s = node. next ; if (s == null || s. waitStatus &gt; 0 ) { s = null ; for ( Node p = tail; p != node &amp;&amp; p != null ; p = p. prev ) if (p. waitStatus &lt;= 0 ) s = p; } if (s != null ) LockSupport . unpark (s. thread ); } unlock的代码特别简单： 每unlock一次state-1 state == 0 时资源成功释放 如果释放成功，唤醒第二个节点 如果第二个节点没引用或者放弃争用，从队尾开始寻找可以唤醒的线程","link":"/2018/07/02/AbstractQueuedSynchronizer解析/"},{"title":"[片段] 使用redis创建简易搜索引擎（核心篇）","text":"支持and查询、多选、多字段排序分页，缺少的功能：or 条件 核心类，有一些测试代码，将就一下。另外需要spring-data-redis 2.0版本以上 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321322323324325326327328329330331332333334335336337338339340341342343344345346347348349350351352353354355356357358359360361362363364365366367368369370371372373374375376377378379380381382package app.pooi.redissearch.search;import app.pooi.redissearch.search.anno.CreateIndex;import app.pooi.redissearch.search.anno.Field;import com.google.common.collect.Lists;import com.google.common.collect.Sets;import lombok.Data;import org.apache.commons.lang3.ArrayUtils;import org.springframework.dao.DataAccessException;import org.springframework.data.redis.connection.RedisZSetCommands;import org.springframework.data.redis.core.*;import org.springframework.data.redis.hash.Jackson2HashMapper;import org.springframework.stereotype.Service;import org.springframework.web.bind.annotation.*;import reactor.util.function.Tuple2;import reactor.util.function.Tuples;import java.util.*;import java.util.concurrent.TimeUnit;import java.util.function.Consumer;import java.util.function.Function;import java.util.regex.Matcher;import java.util.regex.Pattern;import java.util.stream.Collectors;import java.util.stream.Stream;import static app.pooi.redissearch.search.SearchCore.Util.*;@RestController@Servicepublic class SearchCore { private StringRedisTemplate redisTemplate; private Jackson2HashMapper hashMapper = new Jackson2HashMapper(true); @Data private static class Person { private Long id; private String name; private Integer age; private Long ctime; } @PostMapping(\"/person\") @CreateIndex( index = \"person\", documentId = \"#p0.id\", fields = { @Field(propertyName = \"name\", value = \"#p0.name\"), @Field(propertyName = \"age\", value = \"#p0.age\", sort = true), @Field(propertyName = \"ctime\", value = \"#p0.ctime\", sort = true) }) Person addPerson(Person person) { return person; } public SearchCore(StringRedisTemplate redisTemplate) { this.redisTemplate = redisTemplate; } public void indexMeta(String index, Map&lt;String, FieldMeta&gt; fieldMeta) { this.redisTemplate.opsForHash().putAll(genIdxMetaName(index), hashMapper.toHash(fieldMeta)); } @PostMapping(\"/index\") public int indexDocument( final String index, final String field, final String documentId, final String document) { return this.indexDocument(index, field, documentId, document, doc -&gt; Lists.newArrayList(doc.split(\"\"))); } public int indexDocument( final String index, final String field, final String documentId, final String document, final Function&lt;String, List&lt;String&gt;&gt; tokenizer) { final List&lt;String&gt; tokens = tokenizer != null ? tokenizer.apply(document) : Collections.singletonList(document); final String docKey = genDocIdxName(index, documentId); final List&lt;Object&gt; results = redisTemplate.executePipelined(new SessionCallback&lt;Integer&gt;() { @Override public Integer execute(RedisOperations operations) throws DataAccessException { final StringRedisTemplate template = (StringRedisTemplate) operations; final String[] idxs = tokens.stream() .map(word -&gt; genIdxName(index, field, word)) .peek(idx -&gt; ((StringRedisTemplate) operations).opsForSet().add(idx, documentId)) .toArray(String[]::new); template.opsForSet().add(docKey, idxs); return null; } }); return results.size(); } public int indexSortField( final String index, final String field, final String documentId, final Double document) { final String docKey = genDocIdxName(index, documentId); final List&lt;Object&gt; results = redisTemplate.executePipelined(new SessionCallback&lt;Integer&gt;() { @Override public Integer execute(RedisOperations operations) throws DataAccessException { final StringRedisTemplate template = (StringRedisTemplate) operations; final String idxName = genSortIdxName(index, field); template.opsForZSet().add(idxName, documentId, document); template.opsForSet().add(docKey, idxName); return null; } }); return results.size(); } @DeleteMapping(\"/index\") public int deleteDocumentIndex(final String index, final String documentId) { final String docKey = genDocIdxName(index, documentId); final Boolean hasKey = redisTemplate.hasKey(docKey); if (!hasKey) { return 0; } final List&lt;Object&gt; results = redisTemplate.executePipelined(new SessionCallback&lt;Integer&gt;() { @Override public Integer execute(RedisOperations operations) throws DataAccessException { final Set&lt;String&gt; idx = redisTemplate.opsForSet().members(docKey); ((StringRedisTemplate) operations).delete(idx); ((StringRedisTemplate) operations).delete(docKey); return null; } }); return results.size(); } @PatchMapping(\"/index\") public int updateDocumentIndex(final String index, final String field, final String documentId, final String document) { this.deleteDocumentIndex(index, documentId); return this.indexDocument(index, field, documentId, document); } public int updateSortField(final String index, final String field, final String documentId, final Double document) { this.deleteDocumentIndex(index, documentId); return this.indexSortField(index, field, documentId, document); } private Consumer&lt;SetOperations&lt;String, String&gt;&gt; operateAndStore(String method, String key, Collection&lt;String&gt; keys, String destKey) { switch (method) { case \"intersectAndStore\": return (so) -&gt; so.intersectAndStore(key, keys, destKey); case \"unionAndStore\": return (so) -&gt; so.unionAndStore(key, keys, destKey); case \"differenceAndStore\": return (so) -&gt; so.differenceAndStore(key, keys, destKey); default: return so -&gt; { }; } } private Consumer&lt;ZSetOperations&lt;String, String&gt;&gt; zOperateAndStore(String method, String key, Collection&lt;String&gt; keys, String destKey, final RedisZSetCommands.Weights weights) { switch (method) { case \"intersectAndStore\": return (so) -&gt; so.intersectAndStore(key, keys, destKey, RedisZSetCommands.Aggregate.SUM, weights); case \"unionAndStore\": return (so) -&gt; so.unionAndStore(key, keys, destKey, RedisZSetCommands.Aggregate.SUM, weights); default: return so -&gt; { }; } } private String common(String index, String method, List&lt;String&gt; keys, long ttl) { final String destKey = Util.genQueryIdxName(index); redisTemplate.executePipelined(new SessionCallback&lt;String&gt;() { @Override public &lt;K, V&gt; String execute(RedisOperations&lt;K, V&gt; operations) throws DataAccessException { operateAndStore(method, keys.stream().limit(1L).findFirst().get(), keys.stream().skip(1L).collect(Collectors.toList()), destKey) .accept(((StringRedisTemplate) operations).opsForSet()); ((StringRedisTemplate) operations).expire(destKey, ttl, TimeUnit.SECONDS); return null; } }); return destKey; } public String intersect(String index, List&lt;String&gt; keys, long ttl) { return common(index, \"intersectAndStore\", keys, ttl); } public String union(String index, List&lt;String&gt; keys, long ttl) { return common(index, \"unionAndStore\", keys, ttl); } public String diff(String index, List&lt;String&gt; keys, long ttl) { return common(index, \"differenceAndStore\", keys, ttl); } private static Tuple2&lt;Set&lt;Tuple2&lt;String, String&gt;&gt;, Set&lt;Tuple2&lt;String, String&gt;&gt;&gt; parse(String query) { final Pattern pattern = Pattern.compile(\"[+-]?([\\\\w\\\\d]+):(\\\\S+)\"); final Matcher matcher = pattern.matcher(query); Set&lt;Tuple2&lt;String, String&gt;&gt; unwant = Sets.newHashSet(); Set&lt;Tuple2&lt;String, String&gt;&gt; want = Sets.newHashSet(); while (matcher.find()) { String word = matcher.group(); String prefix = null; if (word.length() &gt; 1) { prefix = word.substring(0, 1); } final Tuple2&lt;String, String&gt; t = Tuples.of(matcher.group(1), matcher.group(2)); if (\"-\".equals(prefix)) { unwant.add(t); } else { want.add(t); } } return Tuples.of(want, unwant); } public String query( String index, String query) { final Tuple2&lt;Set&lt;Tuple2&lt;String, String&gt;&gt;, Set&lt;Tuple2&lt;String, String&gt;&gt;&gt; parseResult = parse(query); final Set&lt;Tuple2&lt;String, String&gt;&gt; want = parseResult.getT1(); final Set&lt;Tuple2&lt;String, String&gt;&gt; unwant = parseResult.getT2(); if (want.isEmpty()) { return \"\"; } final Map&lt;String, FieldMeta&gt; entries = (Map&lt;String, FieldMeta&gt;) hashMapper.fromHash(redisTemplate.&lt;String, Object&gt;opsForHash().entries(genIdxMetaName(index))); // union final List&lt;Tuple2&lt;String, String&gt;&gt; unionFields = want.stream() .filter(w -&gt; w.getT2().contains(\",\")) .filter(w -&gt; \"true\".equals(entries.get(w.getT1()).getSort())) .collect(Collectors.toList()); final List&lt;String&gt; unionIdx = unionFields.stream() .flatMap(w -&gt; Arrays.stream(w.getT2().split(\",\")).map(value -&gt; Tuples.of(w.getT1(), value))) .map(w -&gt; genIdxName(index, w.getT1(), w.getT2())) .collect(Collectors.toList()); final String unionResultId = unionIdx.isEmpty() ? \"\" : this.union(index, unionIdx, 30L); want.removeAll(unionFields); // intersect final List&lt;String&gt; intersectIdx = want.stream() .flatMap(t -&gt; { if (\"true\".equals(entries.get(t.getT1()).getSort())) return Stream.of(t); return Arrays.stream(t.getT2().split(\"\")).map(value -&gt; Tuples.of(t.getT1(), value)); }) .map(w -&gt; genIdxName(index, w.getT1(), w.getT2())) .collect(Collectors.toList()); if (!unionResultId.isEmpty()) intersectIdx.add(unionResultId); String intersectResult = this.intersect(index, intersectIdx, 30L); // diff return unwant.isEmpty() ? intersectResult : this.diff(index, Stream.concat(Stream.of(intersectResult), unwant.stream().map(w -&gt; genIdxName(index, w.getT1(), w.getT2()))).collect(Collectors.toList()), 30L); } @GetMapping(\"/query/{index}\") public Set&lt;String&gt; queryAndSort( @PathVariable(\"index\") String index, @RequestParam(\"param\") String query, @RequestParam(\"sort\") String sort, Integer start, Integer stop ) { final String[] sorts = sort.split(\" \"); final Map&lt;String, Integer&gt; map = Arrays.stream(sorts).collect( Collectors.toMap(f -&gt; { if (f.startsWith(\"+\") || f.startsWith(\"-\")) { f = f.substring(1); } return genSortIdxName(\"person\", f); }, field -&gt; field.startsWith(\"-\") ? -1 : 1) ); final int[] weights = map.values() .stream() .mapToInt(Integer::intValue) .toArray();// if (!sort.startsWith(\"+\") &amp;&amp; !sort.startsWith(\"-\")) {// sort = \"+\" + sort;// }// boolean desc = sort.startsWith(\"-\");// sort = sort.substring(1); String queryId = this.query(index, query); Long size; if (queryId.length() == 0 || (size = redisTemplate.opsForSet().size(queryId)) == null || size == 0) { return Collections.emptySet(); } final String resultId = genQueryIdxName(index);// String sortField = sort; redisTemplate.executePipelined(new SessionCallback&lt;Object&gt;() { @Override public &lt;K, V&gt; Object execute(RedisOperations&lt;K, V&gt; operations) throws DataAccessException { final StringRedisTemplate template = (StringRedisTemplate) operations;// template.opsForZSet().intersectAndStore(genSortIdxName(index, sortField), queryId, resultId); SearchCore.this.zOperateAndStore(\"intersectAndStore\", map.keySet().stream().limit(1L).findFirst().get(), Stream.concat(map.keySet().stream().skip(1L), Stream.of(queryId)).collect(Collectors.toList()), resultId, RedisZSetCommands.Weights.of(ArrayUtils.add(weights, 0))).accept(template.opsForZSet());// template.opsForZSet().size(resultId); template.expire(resultId, 30L, TimeUnit.SECONDS); return null; } }); // sort return redisTemplate.opsForZSet().range(resultId, start, stop); } static class Util { private Util() { } static String genIdxMetaName(String index) { return String.format(\"meta:idx:%s\", index); } static String genIdxName(String index, String field, String value) { return String.format(\"idx:%s:%s:%s\", index, field, value); } static String genSortIdxName(String index, String field) { return String.format(\"idx:%s:%s\", index, field); } static String genQueryIdxName(String index) { return String.format(\"idx:%s:q:%s\", index, UUID.randomUUID().toString()); } static String genDocIdxName(String index, String documentId) { return String.format(\"doc:%s:%s\", index, documentId); } }} 辅助类 123456789101112131415161718import lombok.Data;@Datapublic class FieldMeta { private String sort = \"false\"; private String splitFun = \"\"; public FieldMeta() { } public FieldMeta(boolean sort) { this.sort = Boolean.toString(sort); }} 做一个轻量级的搜索还是可以的。","link":"/2019/03/11/[片段]使用redis创建简易搜索引擎（核心篇）/"},{"title":"LeetCode二叉树基础算法","text":"树的高度104. Maximum Depth of Binary Tree (Easy) 递归计算二叉树左右两边深度，取最大值。 12345678910111213141516171819/** * Definition for a binary tree node. * public class TreeNode { * int val; * TreeNode left; * TreeNode right; * TreeNode(int x) { val = x; } * } */class Solution { public int maxDepth(TreeNode root) { if (root == null) return 0; int left = maxDepth(root.left); int right = maxDepth(root.right); return Math.max(left, right) + 1; }} 平衡树110. Balanced Binary Tree (Easy) 递归遍历二叉树左右子树深度 1234567891011121314151617181920212223242526/** * Definition for a binary tree node. * public class TreeNode { * int val; * TreeNode left; * TreeNode right; * TreeNode(int x) { val = x; } * } */class Solution { private boolean balance = true; public boolean isBalanced(TreeNode root) { visitTree(root); return balance; } private int visitTree(TreeNode root) { if (root == null) return 0; int left = visitTree(root.left); int right = visitTree(root.right); if (Math.abs(left - right) &gt; 1 ) this.balance = false; return Math.max(left, right) + 1; }} 两节点的最长路径543. Diameter of Binary Tree (Easy) 递归遍历二叉树左右子树深度， 路径就是两边子树深度之和 1234567891011121314151617181920212223242526/** * Definition for a binary tree node. * public class TreeNode { * int val; * TreeNode left; * TreeNode right; * TreeNode(int x) { val = x; } * } */class Solution { private int max; public int diameterOfBinaryTree(TreeNode root) { deep(root); return max; } private int deep(TreeNode root) { if (root == null) return 0; int left = deep(root.left); int right = deep(root.right); max = Math.max(max,left+right); return Math.max(left, right) + 1; }} 翻转树226. Invert Binary Tree (Easy) 递归交换左右子树的引用 123456789101112131415161718/** * Definition for a binary tree node. * public class TreeNode { * int val; * TreeNode left; * TreeNode right; * TreeNode(int x) { val = x; } * } */class Solution { public TreeNode invertTree(TreeNode root) { if (root == null) return root; TreeNode right =root.right; root.right = invertTree(root.left); root.left = invertTree(right); return root; }} 归并两棵树617. Merge Two Binary Trees (Easy) 递归时如果其中一个节点是空，可以直接复用该节点。如果新建节点，需要拷贝节点的左右子树引用，递归时会用到。 1234567891011121314151617181920/** * Definition for a binary tree node. * public class TreeNode { * int val; * TreeNode left; * TreeNode right; * TreeNode(int x) { val = x; } * } */class Solution { public TreeNode mergeTrees(TreeNode t1, TreeNode t2) { if (t1 == null &amp;&amp; t2 == null ) return null; if (t1 == null) return t2; if (t2 == null) return t1; TreeNode root = new TreeNode(t1.val + t2.val); root.left = mergeTrees(t1.left, t2.left); root.right = mergeTrees(t1.right, t2.right); return root; }} 判断路径和是否等于一个数Leetcode : 112. Path Sum (Easy) 递归查询子树和是否等于目标和 12345678910111213141516/** * Definition for a binary tree node. * public class TreeNode { * int val; * TreeNode left; * TreeNode right; * TreeNode(int x) { val = x; } * } */class Solution { public boolean hasPathSum(TreeNode root, int sum) { if (root == null) return false; if (root.val == sum &amp;&amp; root.left == null &amp;&amp; root.right == null) return true; return hasPathSum(root.left, sum - root.val) || hasPathSum(root.right, sum - root.val); }} 统计路径和等于一个数的路径数量437. Path Sum III (Easy) 双层递归 以当前节点为起点统计路径和 当前节点以下节点为起点统计路径和 以root为根节点的路径数量= 以root为起点统计路径和+root左节点为起点统计路径和+root右节点为起点统计路径和 123456789101112131415161718192021222324/** * Definition for a binary tree node. * public class TreeNode { * int val; * TreeNode left; * TreeNode right; * TreeNode(int x) { val = x; } * } */class Solution { public int pathSum(TreeNode root, int sum) { if (root == null) return 0; //结果数 等于 以当前root为父节点和 root以下为父节点结果数之和 return sum(root, sum) + pathSum(root.left, sum) + pathSum(root.right, sum); } // 计算以当前node为父节点能都多少路径数 private int sum(TreeNode node, int sum) { if (node == null) return 0; int count = 0; if (node.val == sum) count++; count += sum(node.left, sum - node.val) + sum(node.right, sum - node.val); return count; }} 子树572. Subtree of Another Tree (Easy) 12345678910111213141516171819202122/** * Definition for a binary tree node. * public class TreeNode { * int val; * TreeNode left; * TreeNode right; * TreeNode(int x) { val = x; } * } */class Solution { public boolean isSubtree(TreeNode s, TreeNode t) { if (s == null) return false; return isSubRoot(s, t) || isSubtree(s.left, t) || isSubtree(s.right, t); } public boolean isSubRoot(TreeNode node, TreeNode t) { if (node == null &amp;&amp; t == null) return true; if (node == null || t == null) return false; if (node.val != t.val) return false; return isSubRoot(node.left, t.left) &amp;&amp; isSubRoot(node.right, t.right); }} 树的对称101. Symmetric Tree (Easy) 12345678910111213141516171819202122/** * Definition for a binary tree node. * public class TreeNode { * int val; * TreeNode left; * TreeNode right; * TreeNode(int x) { val = x; } * } */class Solution { public boolean isSymmetric(TreeNode root) { if (root == null) return true; return isSymmetric(root.left, root.right); } public boolean isSymmetric(TreeNode left, TreeNode right) { if (left == null &amp;&amp; right == null) return true; if (left == null || right == null) return false; if (left.val != right.val) return false; return isSymmetric(left.left, right.right) &amp;&amp; isSymmetric(left.right, right.left); }} 最小路径111. Minimum Depth of Binary Tree (Easy) 和最大路径类似 123456789101112131415161718/** * Definition for a binary tree node. * public class TreeNode { * int val; * TreeNode left; * TreeNode right; * TreeNode(int x) { val = x; } * } */class Solution { public int minDepth(TreeNode root) { if (root == null) return 0; int left = minDepth(root.left); int right = minDepth(root.right); if (left == 0 || right == 0) return left + right + 1; return Math.min(left, right) + 1; }} 统计左叶子节点的和404. Sum of Left Leaves (Easy) 123456789101112131415161718/** * Definition for a binary tree node. * public class TreeNode { * int val; * TreeNode left; * TreeNode right; * TreeNode(int x) { val = x; } * } */class Solution { public int sumOfLeftLeaves(TreeNode root) { if (root == null) return 0; if (root.left != null &amp;&amp; root.left.left == null &amp;&amp; root.left.right == null) return root.left.val + sumOfLeftLeaves(root.right); return sumOfLeftLeaves(root.left) + sumOfLeftLeaves(root.right); }}` 相同节点值的最大路径长度687. Longest Univalue Path (Easy) 递归查找左右子树相同节点值最大路径，最大路径的计算：如果相等路径+1，如果不相等置为0。 12345678910111213141516171819202122232425262728/** * Definition for a binary tree node. * public class TreeNode { * int val; * TreeNode left; * TreeNode right; * TreeNode(int x) { val = x; } * } */class Solution { private int path = 0; public int longestUnivaluePath(TreeNode root) { visit(root); return path; } private int visit(TreeNode root) { if (root == null) return 0; int left = visit(root.left); int right = visit(root.right); left = (root.left != null &amp;&amp; root.val == root.left.val) ? left + 1 : 0; right = (root.right != null &amp;&amp; root.val == root.right.val)? right + 1 : 0; path = Math.max(path, left+right); return Math.max(left, right ); }} 间隔遍历337. House Robber III (Medium) 递归查询两种情况 如果从当前节点开始 从当前节点的子节点开始 1234567891011121314151617181920/** * Definition for a binary tree node. * public class TreeNode { * int val; * TreeNode left; * TreeNode right; * TreeNode(int x) { val = x; } * } */class Solution { public int rob(TreeNode root) { if (root == null) return 0; int val1 = root.val, val2 = 0; if (root.left != null) val1+= rob(root.left.left) + rob(root.left.right); if (root.right != null) val1+= rob(root.right.left) + rob(root.right.right); val2 = rob(root.left) + rob(root.right); return Math.max(val1, val2); }} 找出二叉树中第二小的节点Second Minimum Node In a Binary Tree (Easy) 第二小节点在子树节点上，如果子树值与根节点相等，继续向下查找 123456789101112131415161718192021/** * Definition for a binary tree node. * public class TreeNode { * int val; * TreeNode left; * TreeNode right; * TreeNode(int x) { val = x; } * } */class Solution { public int findSecondMinimumValue(TreeNode root) { if (root == null) return -1; if (root.left == null) return -1; int left = root.left.val, right = root.right.val; if (root.val == root.left.val) left = findSecondMinimumValue(root.left); if (root.val == root.right.val) right = findSecondMinimumValue(root.right); if (left != -1 &amp;&amp; right != -1) return Math.min(left, right); if (left &gt; -1) return left; return right; }} 二叉树的层平均值637. Average of Levels in Binary Tree (Easy) BFS 123456789101112131415161718192021222324252627282930/** * Definition for a binary tree node. * public class TreeNode { * int val; * TreeNode left; * TreeNode right; * TreeNode(int x) { val = x; } * } */class Solution { public List&lt;Double&gt; averageOfLevels(TreeNode root) { List&lt;Double&gt; ret = new ArrayList&lt;&gt;(); if (root == null) return ret; Queue&lt;TreeNode&gt; queue = new LinkedList&lt;&gt;(); queue.add(root); while(!queue.isEmpty()) { int count = queue.size(); double sum = 0d; for(int i = 0; i &lt; count; i++) { TreeNode node = queue.poll(); sum+= node.val; if (node.left != null) queue.add(node.left); if (node.right != null) queue.add(node.right); } ret.add(sum/count); } return ret; }} 找树左下角的值513. Find Bottom Left Tree Value (Easy) DFS 123456789101112131415161718192021222324252627282930/** * Definition for a binary tree node. * public class TreeNode { * int val; * TreeNode left; * TreeNode right; * TreeNode(int x) { val = x; } * } */class Solution { private int val = 0; public int findBottomLeftValue(TreeNode root) { if (root == null) return val; Queue&lt;TreeNode&gt; queue = new LinkedList&lt;&gt;(); queue.add(root); while(!queue.isEmpty()) { // 这一行的数量 int count = queue.size(); for (int i = 0; i &lt; count; i++) { TreeNode node = queue.poll(); if(i == 0) val = node.val; if (node.left != null) queue.add(node.left); if (node.right != null) queue.add(node.right); } } return val; }} 非递归实现二叉树的后序遍历入栈条件： 未访问过该节点出栈条件： 访问过该节点 12345678910111213141516171819202122232425262728293031323334353637383940414243/** * Definition for a binary tree node. * public class TreeNode { * int val; * TreeNode left; * TreeNode right; * TreeNode(int x) { val = x; } * } */class Solution { private List&lt;Integer&gt; res = new LinkedList&lt;&gt;(); private Stack&lt;TreeNode&gt; stack = new Stack&lt;&gt;(); private Set&lt;TreeNode&gt; visited = new HashSet&lt;&gt;(); public List&lt;Integer&gt; postorderTraversal(TreeNode root) { if (root == null) return res; stack.push(root); while (!stack.isEmpty()) { TreeNode node = stack.peek(); if ((node.left == null &amp;&amp; node.right == null) || visited.contains(node)) { TreeNode i = stack.pop(); res.add(i.val); } else { visited.add(node); if (node.right != null) stack.push(node.right); if (node.left != null) stack.push(node.left); } } return res; } private void visit(TreeNode root) { if(root == null) return; visit(root.left); visit(root.right); res.add(root.val); }} 非递归实现二叉树的前序遍历入栈条件： 无出栈条件： 直接出栈 1234567891011121314151617181920212223242526/** * Definition for a binary tree node. * public class TreeNode { * int val; * TreeNode left; * TreeNode right; * TreeNode(int x) { val = x; } * } */class Solution { private List&lt;Integer&gt; res = new LinkedList&lt;&gt;(); private Stack&lt;TreeNode&gt; stack = new Stack&lt;&gt;(); public List&lt;Integer&gt; preorderTraversal(TreeNode root) { if (root == null) return res; stack.push(root); while (!stack.isEmpty()) { TreeNode node = stack.pop(); res.add(node.val); if (node.right != null) stack.push(node.right); if (node.left != null) stack.push(node.left); } return res; }} 非递归实现二叉树的中序遍历入栈条件： 未访问过该节点出栈条件： 访问过该节点入栈顺序: right -&gt; middle -&gt; left 12345678910111213141516171819202122232425262728293031323334353637/** * Definition for a binary tree node. * public class TreeNode { * int val; * TreeNode left; * TreeNode right; * TreeNode(int x) { val = x; } * } */class Solution { private List&lt;Integer&gt; res = new LinkedList&lt;&gt;(); private Stack&lt;TreeNode&gt; stack = new Stack&lt;&gt;(); private Set&lt;TreeNode&gt; visited = new HashSet&lt;&gt;(); public List&lt;Integer&gt; inorderTraversal(TreeNode root) { if (root == null) return res; push(root); while (!stack.isEmpty()) { TreeNode node = stack.pop(); if ((node.left == null &amp;&amp; node.right == null) || visited.contains(node)) { res.add(node.val); } else { push(node); } } return res; } private void push(TreeNode root) { if (root == null) return; visited.add(root); if (root.right != null) stack.push(root.right); stack.push(root); if (root.left != null) stack.push(root.left); }}","link":"/2019/06/15/2019-06-15树算法/"},{"title":"为什么引入间隙锁","text":"为了便于说明问题，我们先使用一个小一点儿的表，建表和初始化语句如下： 123456789101112131415CREATE TABLE `t` ( `id` int(11) NOT NULL, `c` int(11) DEFAULT NULL, `d` int(11) DEFAULT NULL, PRIMARY KEY (`id`), KEY `c` (`c`)) ENGINE=InnoDB; insert into t values(0,0,0),(5,5,5),(10,10,10),(15,15,15),(20,20,20),(25,25,25); 这个表除了主键 id 外，还有一个索引 c，初始化语句在表中插入了 6 行数据。 下面的语句序列，是怎么加锁的，加的锁又是什么时候释放的呢？ 1select * from t where d = 5 for update; 比较好理解的是，这个语句会命中 d = 5 的这一行，对应的主键 id = 5，因此在 select 语句执行完成后，会在id = 5 这一行主键上加一个写锁，而且由于两阶段锁协议，这个写锁会在执行 commit 语句的时候释放。 由于字段 d 上没有索引，因此这条查询语句会做全表扫描。那么，其他被扫描到的，但是不满足条件的 5 行记录上，会不会被加锁呢？ 我们知道，InnoDB 的默认事务隔离级别是可重复读，所以本文接下来没有特殊说明的部分，都是设定在可重复读隔离级别下。 幻读是什么？现在，我们就来分析一下，假设只在 id = 5这一行加锁，而其他行的不加锁的话，会怎么样。 下面先来看一下这个场景（这个结果是建立在前面假设之上，实际上是错误的）： 假设只在 id = 5 这一行加行锁，可以看到，session A 里执行了三次查询，分别是 Q1、Q2 和 Q3。它们的 SQL 语句相同，都是select * fom t where d=5 for update。我们来看一下这三条 SQL 语句，分别会返回什么结果。 Q1 只返回 id = 5 这一行； 在 T2 时刻，session B 把 id = 0 这一行的 d 值改成了 5，因此 T3 时刻 Q2 查出来的是id = 0 和id = 5 这两行； 在 T4 时刻，session C 又插入一行（1,1,5），因此 T5 时刻 Q3 查出来的是id = 0、id = 1 和 id = 5 的这三行。 其中，Q3 读到id = 1 这一行的现象，被称为“幻读”。也就是说，幻读指的是一个事务在前后两次查询同一个范围的时候，后一次查询看到了前一次查询没有看到的行。 这里，我需要对“幻读”做一个说明： 在可重复读隔离级别下，普通的查询是快照读，是不会看到别的事务插入的数据的。因此，幻读在当前读下才会出现。 上面 session B 的修改结果，被 session A 之后的 select 语句用当前读看到，不能称为幻读。幻读仅专指新插入的行。 如果只从我们学到的事务可见性规则来分析的话，上面这三条 SQL 语句的返回结果都没有问题。 因为这三个查询都是加了 for update，都是当前读。而当前读的规则，就是要能读到所有已经提交的记录的最新值。并且，session B 和 sessionC 的两条语句，执行后就会提交，所以 Q2 和 Q3 就是应该看到这两个事务的操作效果，而且也看到了，这跟事务的可见性规则并不矛盾。 幻读有什么问题？首先是语义上的。session A 在 T1 时刻就声明了，“我要把所有 d=5 的行锁住，不准别的事务进行读写操作”。所以我们假设只锁了id = 5这一行的语义与select * from t where d = 5 for update 不同。 其次，是数据一致性的问题。 这个数据不一致到底是怎么引入的？肯定是前面的假设有问题。 我们把扫描过程中碰到的行，也都加上写锁，再来看看执行效果。 由于 session A 把所有的行都加了写锁，所以 session B 在执行第一个 update 语句的时候就被锁住了。需要等到 T6 时刻 session A 提交以后，session B 才能继续执行。 这样对于 id = 0 这一行，在数据库里的最终结果还是 (0,5,5)。在 binlog 里面，执行序列是这样的： 1234567insert into t values(1,1,5); /*(1,1,5)*/update t set c=5 where id=1; /*(1,5,5)*/ update t set d=100 where d=5;/* 所有 d=5 的行，d 改成 100*/ update t set d=5 where id=0; /*(0,0,5)*/update t set c=5 where id=0; /*(0,5,5)*/ 可以看到，按照日志顺序执行，id = 0 这一行的最终结果也是 (0,5,5)。所以，id = 0 这一行的问题解决了。 但同时你也可以看到，id = 1 这一行，在数据库里面的结果是 (1,5,5)，而根据 binlog 的执行结果是 (1,5,100)，也就是说幻读的问题还是没有解决。为什么我们已经这么“凶残”地，把所有的记录都上了锁，还是阻止不了 id = 1 这一行的插入和更新呢？ 原因很简单。在 T3 时刻，我们给所有行加锁的时候，id = 1 这一行还不存在，不存在也就加不上锁。 也就是说，即使把所有的记录都加上锁，还是阻止不了新插入的记录，这也是为什么“幻读”会被单独拿出来解决的原因。 如何解决幻读？现在你知道了，产生幻读的原因是，行锁只能锁住行，但是新插入记录这个动作，要更新的是记录之间的“间隙”。因此，为了解决幻读问题，InnoDB 只好引入新的锁，也就是间隙锁 (Gap Lock)。 顾名思义，间隙锁，锁的就是两个值之间的空隙。比如文章开头的表 t，初始化插入了 6 个记录，这就产生了 7 个间隙。 这样，当你执行 select * from t where d=5 for update 的时候，就不止是给数据库中已有的 6 个记录加上了行锁，还同时加了 7 个间隙锁。这样就确保了无法再插入新的记录。 也就是说这时候，在一行行扫描的过程中，不仅将给行加上了行锁，还给行两边的空隙，也加上了间隙锁。 现在你知道了，数据行是可以加上锁的实体，数据行之间的间隙，也是可以加上锁的实体。但是间隙锁跟我们之前碰到过的锁都不太一样。 比如行锁，分成读锁和写锁。下图就是这两种类型行锁的冲突关系。 也就是说，跟行锁有冲突关系的是“另外一个行锁”。 但是间隙锁不一样，跟间隙锁存在冲突关系的，是“往这个间隙中插入一个记录”这个操作。间隙锁之间都不存在冲突关系。 这句话不太好理解，我给你举个例子： 这里 session B 并不会被堵住。因为表 t 里并没有 c = 7 这个记录，因此 session A 加的是间隙锁 (5,10)。而 session B 也是在这个间隙加的间隙锁。它们有共同的目标，即：保护这个间隙，不允许插入值。但，它们之间是不冲突的。 间隙锁和行锁合称 next-key lock，每个 next-key lock 是前开后闭区间。也就是说，我们的表 t 初始化以后，如果用 select * from t for update 要把整个表所有记录锁起来，就形成了 7 个 next-key lock，分别是 (-∞,0]、(0,5]、(5,10]、(10,15]、(15,20]、(20, 25]、(25, +supremum]。 备注：这篇文章中，如果没有特别说明，我们把间隙锁记为开区间，把 next-key lock 记为前开后闭区间。 你可能会问说，这个 supremum 从哪儿来的呢？ 这是因为 +∞是开区间。实现上，InnoDB 给每个索引加了一个不存在的最大值 supremum，这样才符合我们前面说的“都是前开后闭区间”。 间隙锁和 next-key lock 的引入，帮我们解决了幻读的问题，但同时也带来了一些“困扰”。 对应到我们这个例子的表来说，业务逻辑这样的：任意锁住一行，如果这一行不存在的话就插入，如果存在这一行就更新它的数据，代码如下： 123456789begin;select * from t where id=N for update; /* 如果行不存在 */insert into t values(N,N,N);/* 如果行存在 */update t set d=N set id=N; commit; 这个逻辑一旦有并发，就会碰到死锁。你一定也觉得奇怪，这个逻辑每次操作前用 for update 锁起来，已经是最严格的模式了，怎么还会有死锁呢？ 这里，我用两个 session 来模拟并发，并假设 N=9。 图 8 间隙锁导致的死锁 你看到了，其实都不需要用到后面的 update 语句，就已经形成死锁了。我们按语句执行顺序来分析一下： session A 执行 select … for update 语句，由于 id = 9 这一行并不存在，因此会加上间隙锁 (5,10); session B 执行 select … for update 语句，同样会加上间隙锁 (5,10)，间隙锁之间不会冲突，因此这个语句可以执行成功； session B 试图插入一行 (9,9,9)，被 session A 的间隙锁挡住了，只好进入等待； session A 试图插入一行 (9,9,9)，被 session B 的间隙锁挡住了。 至此，两个 session 进入互相等待状态，形成死锁。当然，InnoDB 的死锁检测马上就发现了这对死锁关系，让 session A 的 insert 语句报错返回了。 你现在知道了，间隙锁的引入，可能会导致同样的语句锁住更大的范围，这其实是影响了并发度的。 你可能会说，为了解决幻读的问题，我们引入了这么一大串内容，有没有更简单一点的处理方法呢。 我在文章一开始就说过，如果没有特别说明，今天和你分析的问题都是在可重复读隔离级别下的，间隙锁是在可重复读隔离级别下才会生效的。所以，你如果把隔离级别设置为读提交的话，就没有间隙锁了。但同时，你要解决可能出现的数据和日志不一致问题，需要把 binlog 格式设置为 row。这，也是现在不少公司使用的配置组合。","link":"/2020/02/27/2020-02-27-为什么引入间隙锁/"}],"tags":[{"name":"工作","slug":"工作","link":"/tags/工作/"},{"name":"代码","slug":"代码","link":"/tags/代码/"},{"name":"算法","slug":"算法","link":"/tags/算法/"},{"name":"读书","slug":"读书","link":"/tags/读书/"},{"name":"java","slug":"java","link":"/tags/java/"},{"name":"感悟","slug":"感悟","link":"/tags/感悟/"},{"name":"mybatis","slug":"mybatis","link":"/tags/mybatis/"},{"name":"spring","slug":"spring","link":"/tags/spring/"},{"name":"aop","slug":"aop","link":"/tags/aop/"},{"name":"缓存","slug":"缓存","link":"/tags/缓存/"},{"name":"分布式","slug":"分布式","link":"/tags/分布式/"},{"name":"数据库","slug":"数据库","link":"/tags/数据库/"},{"name":"消息","slug":"消息","link":"/tags/消息/"},{"name":"linux","slug":"linux","link":"/tags/linux/"},{"name":"团队","slug":"团队","link":"/tags/团队/"},{"name":"搜索","slug":"搜索","link":"/tags/搜索/"},{"name":"redis","slug":"redis","link":"/tags/redis/"}],"categories":[{"name":"工作","slug":"工作","link":"/categories/工作/"},{"name":"算法","slug":"算法","link":"/categories/算法/"},{"name":"读书","slug":"读书","link":"/categories/读书/"},{"name":"挑战","slug":"工作/挑战","link":"/categories/工作/挑战/"},{"name":"Java基础","slug":"Java基础","link":"/categories/Java基础/"},{"name":"字符串","slug":"算法/字符串","link":"/categories/算法/字符串/"},{"name":"Java框架","slug":"Java框架","link":"/categories/Java框架/"},{"name":"总结","slug":"工作/总结","link":"/categories/工作/总结/"},{"name":"排序","slug":"算法/排序","link":"/categories/算法/排序/"},{"name":"架构","slug":"架构","link":"/categories/架构/"},{"name":"Mybatis","slug":"Java框架/Mybatis","link":"/categories/Java框架/Mybatis/"},{"name":"Spring","slug":"Java框架/Spring","link":"/categories/Java框架/Spring/"},{"name":"中间件","slug":"中间件","link":"/categories/中间件/"},{"name":"缓存","slug":"架构/缓存","link":"/categories/架构/缓存/"},{"name":"nosql","slug":"架构/nosql","link":"/categories/架构/nosql/"},{"name":"消息","slug":"架构/消息","link":"/categories/架构/消息/"},{"name":"mysql","slug":"架构/mysql","link":"/categories/架构/mysql/"},{"name":"resilience4j","slug":"中间件/resilience4j","link":"/categories/中间件/resilience4j/"},{"name":"二叉树","slug":"算法/二叉树","link":"/categories/算法/二叉树/"},{"name":"Java并发","slug":"Java并发","link":"/categories/Java并发/"},{"name":"搜索引擎","slug":"中间件/搜索引擎","link":"/categories/中间件/搜索引擎/"}]}