{"pages":[{"title":"categories","text":"","link":"/categories/index.html"},{"title":"tags","text":"","link":"/tags/index.html"}],"posts":[{"title":"根据权限查询时避免角色切换的一种思路","text":"1. 问题背景权限系统现状UC权限系统基于角色访问控制技术RBAC（Role Based Access Control） 。具体来说，就是赋予用户某个角色，角色给与角色对应的权限能访问及操作不同范围的资源。 什么是数据权限代表一个角色对应某个权限所能操作的数据范围，比如gitlab组管理员能看到组下的所有项目代码，我们可以这样配置： 创建组管理员 分配给组管理员查看项目代码的权限 查看项目代码权限设置约束条件，约束为自己组下的项目 实际产生遇到的问题对绝大多数简单的系统来说一个用户对应一个系统只会有一个角色，一个角色只有一个数据权限范围（即使有多个，也可以合并成一个）。但是随着产品的功能迭代，用户的变更和系统设计的原因，总有一些特殊且重要的用户在同一个系统中拥有多个角色。在多角色和数据权限的组合下，一个用户可以拥有复数的数据权限范围。 考虑到实现复杂性，大多数系统选择使用角色切换的手段简化系统实现，同时对用户暴露出了他们不熟悉的角色这一概念，造成这些用户在系统使用中的各种不便。 本文重点讨论在避免角色切换的前提下，进行多角色数据范围查询的一种思路。 具体需要解决的需求我们的数据报表后台，不同的角色拥有不同的数据查看范围（不同角色所能看到的员工数据字段也各不相同），例如： 薪酬管理员：查看非高职级员工数据 高级薪酬管理员： 查看高职级员工数据 长期激励管理员：查看有长期激励员工数据 等等 简单来说，拥有长期激励管理员和高级薪酬管理员的用户能否直接看到高职级员工数据和长期激励员工数据？至少在直觉上是可行的。 2.多角色数据范围查询直觉的做法单角色单数据范围可以使用一句sql查询出结果，那多角色多数据范围是不是使用多句sql查询出结果合并就可以了？ 深入思考 多角色数据范围对行的影响 查询条件合并还是结果合并？ —-结果合并 如何排序？ —–外部排序，或先内部排序,limit,再外部排序 有重复数据怎么办？ —-使用groupby去重 查询性能有影响吗？—-有 具体体现： 1234567select * from ((select id, sortvalue from table_1 where t_name = &apos;a&apos; order by sortvalue desc limit 20) -- 先内部排序,limitunion all -- 结果合并(select id, sortvalue from table_1 where t_name = &apos;b&apos; order by sortvalue desc limit 20) -- 先内部排序,limitorder by sortvalue desc -- 外部排序) a group by id -- 使用groupby去重limit 10, 10 深入思考 多角色数据范围对列的影响 薪酬管理员： 查看员工薪酬字段 长期激励管理员：查看员工长期激励字段 如何解决？方法有很多！ 综合思考，给出一种解决方案123graph LR A(查询行及角色信息) --&gt; B(根据角色查询对应列字段) B --&gt; C(结果) 步骤： 查询多角色数据范围下的数据，附带角色信息 1234567select id, GROUP_CONCAT(a.role) as roles from ((select id, &apos;role_a&apos; as role from table_1 where sortvalue &gt; 10 order by `sortvalue` desc limit 2)union all(select id, &apos;role_b&apos; as role from table_1 where sortvalue &gt; 20 order by `sortvalue` desc limit 2) order by `sortvalue` desc ) a group by idlimit 0, 2 结果： id roles 1 薪酬管理员 5 薪酬管理员，长期激励管理员 根据每一行不同的角色，查询出可见的字段，例如id=1的行只能查看ROLE_B对应字段，而id=5的行可以看到ROLE_A,ROLE_B对应的两个角色的字段 3.总结和延伸多角色数据范围写操作？遍历角色直到找到满足条件的权限即可。 收获自己不行动，等于等着被别人安排哈哈 还有疑问？自己想。还可以点这里","link":"/2018/11/06/2018-11-06-biz/"},{"title":"再谈最长公共子串","text":"序言这次遇到贝壳花名的需求，需要使用最长公共子串对花名做校验。这种算法在面试题中算是必会题，各种四层循环，三层循环，两层循环的代码在我脑中闪过，但是今天就是要带你实现不一样的最长公共子串！ 教科书式实现使用动态规划，两层循环，使用二维数组存储状态，时间复杂度O(n^2^)，空间复杂度O(n^2^)或O(2n) 一张图解释原理： 12345678910111213141516 先 横 向 处 理 +---------------------------&gt; e a b c b c f + +---+---+---+---+---+---+---+ | a | 0 | 1 | 0 | 0 | 0 | 0 | 0 |纵 | +---------------------------+向 | b | 0 | 0 | 2 | 0 | 1 | 0 | 0 |累 | +---------------------------+加 | c | 0 | 0 | 0 | 3 | 0 | 2 | 0 | | +---------------------------+ | d | 0 | 0 | 0 | 0 | 0 | 0 | 0 | | +---------------------------+ | e | 1 | 0 | 0 | 0 | 0 | 0 | 0 | v +---+---+---+---+---+---+---+ e a b c b c f 优化空间复杂度至O(1)从上图可以发现，在纵向累加时实际只需要左上方的计数器即可, O(n^2^)的空间白白被浪费了,最优的空间复杂度应该是O(1)。那么该如何处理呢？ 一张图解释原理： e a b c b c f +---+ +---+---+---+---+---+---+---+ a | 0 | | 1 | 0 | 0 | 0 | 0 | 0 | a +-------+ +-----------------------+ b | 0 | 0 | | 2 | 0 | 1 | 0 | 0 | b +-----------+ --------------------+ c | 0 | 0 | 0 | | 3 | 0 | 2 | 0 | c +---------------+ ----------------+ d | 0 | 0 | 0 | 0 | | 0 | 0 | 0 | d +-------------------+ +-----------+ e | 1 | 0 | 0 | 0 | 0 | | 0 | 0 | e +---+---+---+---+---+-------+ +---+---+ e a b c b c f答案就是沿着等长对着线处理。 有意思的代码抽象大家可以根据上面思路写一下，一般会把算法分成两部分：处理长方形的左下部分和处理长方形的右上部分，两部分都是双层循环，时间复杂度和空间负载度已经变为了O(n^2^) ，O(1)。 肯定有人已经发现自己的代码处理左下角的代码和处理右上角的代码不能复用，一个是从中间向左下角处理，一个是从中间向右上角处理，明明很类似，但是就是没发合并。 那么有没有方法把这两部分处理抽象成公共代码呢？不卖关子了，直接上图： 12345678910111213141516 + e | +--+ e a b c b c f a | 1|+---+---+---+---+---+---+---+ +-----+ | 1 | 0 | 0 | 0 | 0 | 0 | a b | 0| 2| +-----------------------+ +--------+ | 2 | 0 | 1 | 0 | 0 | b c | 0| 0| 3| --------------------+ 翻 折 +-----------+ | 3 | 0 | 2 | 0 | c +------------&gt; b | 0| 1| 0| 0| ----------------+ +--------------+ | 0 | 0 | 0 | d c | 0| 0| 2| 0| 0| +-----------+ +--------------+ | 0 | 0 | e f | 0| 0| 0| 0| 0| +---+---+ +--------------+ a b c d e 如果你想使用公共代码同时实现处理左下角和右上角是不可能的了。所以你需要把右上角的三角翻折，然后你就得到了两个三角： 12345678910111213141516 + e | +--+ a | 1| +---+ +-----+a | 0 | b | 0| 2| +-------+ +--------+b | 0 | 0 | c | 0| 0| 3| +-----------+ +-----------+c | 0 | 0 | 0 | b | 0| 1| 0| 0| +---------------+ +--------------+d | 0 | 0 | 0 | 0 | c | 0| 0| 2| 0| 0| +-------------------+ +--------------+e | 1 | 0 | 0 | 0 | 0 | f | 0| 0| 0| 0| 0| +---+---+---+---+---+------ +--------------+ e a b c b c f a b c d e 这样就变成了处理两遍左下角了，代码也可以完美复用！！！ 最终实现我的完整思考过程已经分析完毕，这样沿对着线处理还有一个小小的优点：提前结束搜索。这一点大家可以自行思考，这里不做过多解释。 直接干货上场： 1234567891011121314151617181920212223242526272829public class Solution { /** * @param A: A string * @param B: A string * @return: the length of the longest common substring. */ public int longestCommonSubstring(String A, String B) { // write your code here char[] achars = A.toCharArray(), bchars = B.toCharArray(); return getMaxLength(bchars, achars, getMaxLength(achars, bchars, 0, 0), 1); } private static int getMaxLength(char[] s1, char[] s2, int maxLength, int startIndex) { for (int start = startIndex; start &lt; s1.length; start++) { int upper = Math.min(s2.length, s1.length - start); // if (upper &lt;= maxLength) break; //提前结束搜索 for (int currentLineLength = 0, x = 0, y = start; x &lt; upper; x++, y++) { if (s1[y] == s2[x]) maxLength = Math.max(maxLength, ++currentLineLength); else { // if (upper - x - 1 &lt;= maxLength) break; //提前结束搜索 currentLineLength = 0; }; } } return maxLength; }} 结尾怎么样，经历这次优化过程是否感觉自己对最长公共子串的认识又更深了一步呢？虽然不能保证是首创（也可能是首创？），但是这次一步一步真切思考优化直到获得成果让我无比兴奋。 说了这么多，我就是要给我们贝壳招聘开发组打个广告&gt;_&gt;，期待更多爱思考优秀的同学加入！ ![](/Users/sage/Desktop/屏幕快照 2018-11-10 13.33.20.png)","link":"/2018/11/11/2018-11-11-lcs/"},{"title":"代码外的生存之道-读书笔记-职业篇","text":"职业发展的驱动力应该来自自身，工作属于公司，职业生涯属于自己。 第一要务 拥有正确的心态大多数人形成的错误的心态: 认为在为公司打工，没有把自己的职业生涯当作生意来看待。铭记在心，开始积极主动的管理自己的职业生涯吧。 像企业一样思考自己能提供什么:自己的能力就是创造软件自己需要做什么: 持续不断地改进和完善自己的产品 传达自己的价值，和千万同行的独特之处 一头扎进工作不可能非同凡响，你应该： 专注于你正在提供怎样的服务， 以及如何营销这项服务； 想方设 提升你的服务； 思考你可以专注为哪一 特定类型的客户或行业提供特定的服务； 集中精力成为一位专家，专门为某一特定类型的客户提供专业的整体服务（ 记住， 作为一个软件开发 人员， 你 只有真正专注 于一类客户，才能找到非常好的工作）。 更好的宣传自己的产品，更好的找到你的客户 第二要务 设定自己的目标无论因为何种原因你没有为自己的职业生涯设定目标， 现在都是时候设定目标了。 不是明天， 也不是下周， 就是现在。 没有明确的方向， 你走的每一步都是徒劳的。 如何设定目标？先在心中树立一个大目标，然后分解成多个小目标 追踪你的目标定期核对自己的目标，必要时还要调整。 人际交往能力","link":"/2018/12/27/2018-12-27-softablity/"},{"title":"《重构-改善既有代码设计》读书笔记","text":"1.1 一个简单的例子一个计算顾客租赁影片费用的程序，能容易写成面条式的代码（流水账）：顾客类调用影片类和租赁时长计算费用 对机器来言只要能运行正确没有好代码和坏代码之分，但是对（维护的）人来说很难找到修改点，容易引入bug 1.2重构前先写测试保证重构结果利用单元测试保证重构正确性 1.3以微小的步伐修改程序，保证问题快速发现解决不要为修改变量名感到羞耻，只有写出人能理解的代码才是好程序员 重构完可能 性能变差，但同时会带来更多的机会来优化 1.4干货－用多态代替switchswitch需要关心具体条件，多态具有switch不具备的优势：不需要关心具体类型 2.1重构的定义重构：不改变运行结果下 提高理解性 降低修改成本 2.2重构的原因 代码结构的流失是累积性的，越难看懂代码设计意图，越难保护其设计 消除重复代码，方便修改 我们编写代码时很容易忘记读者的感受，造成他人时间的浪费 重构时犯错可以加深对代码意图的理解，可以帮助发现bug 好的结构设计是加速开发的根本 2.3重构的时机 添加功能时重构，在修改过程中把结构理清，也可以更简单的添加功能 修复错误时重构 复审代码时重构 2.4 重构的价值程序有两面价值，今天可以为你做什么和明天可以为你做什么为了满足明天的需要，你会遇到： 难以阅读，逻辑重复 添加新功能许修改以前代码 复杂的条件逻辑等代码 而你希望看到的是： 容易阅读，所有逻辑在唯一指定地点， 新的修改不会危及现有行为 尽可能简单表达逻辑条件 重构就是把程序转变为这些特征的工具 2.5如何告诉（对付）经理很多经理都是进度驱动，所以更加需要重构带来的好处，所以不要告诉他们 他们不会理解的 2.5.1引入间接层与重构的关系间接层优点： 允许逻辑共享 增加解释意图和实现的机会－多了类名和函数名 隔离变化 多态封装条件逻辑 2.5.2何时不应该重构 软件根本不工作 最后期限已近 未完成的重构可以称之为债务，迟早要还 2.5.3重构与预先设计的关系重构可以节约不必要的时间精力花在预先设计上，让软件设计向简化发展 3代码的坏味道 重复代码 过长函数 过大类，过长参数 修改一处程序的原因过多/一个原因修改过多的程序 数据依赖过多 重复的字段和参数 总是放在一起的字段 switch语句 平行继承关系 不是所有分支下都需要的临时变量 过度耦合调用链 不必要的委托 失血数据类 频繁重写父类方法 过多注释 4~12具体如何做自己看书吧","link":"/2018/09/08/2018-9-8-reading/"},{"title":"2018下半年书单","text":"经济金融类： 《斯坦福极简经济学》分微观和宏观经济学 没有教科书式的介绍 比较好读，推荐 《随机漫步的傻瓜》 经验之谈 观点可以接受 《买晾衣杆的小贩为何不会倒》 贴近生活 过于简单 不推荐 《富爸爸 穷爸爸》小白入门首选 推荐 《小岛经济学》 深入浅出 最后映射中国和美国关系 后面有点看不懂 推荐 小说类： 《解忧杂货店》 个人感觉一般 后半部分剧情我都猜到了 《人间失格》 遭受到了社会的毒打 很现实 有点致郁 心里承受能力低的不推荐 《三体》 第一部可以的，后面一部不如一部 但是比起其他网上大众喜欢的爽文好多了 技术类： 《java编程的逻辑》 温故而知新，基础好的不用看了 小白推荐 《kafka权威指南》正在看 但是真的写的不错 推荐 《mybatis从入门到精通》 一般般，只有入门吧 《netty权威指南》 一般般 《redis设计与实现》 真设计与实现 推荐 《深入理解java虚拟机》多读几遍也不为过 推荐 《SpringBoot实战》 读的比较早没影响了 不推荐 《算法图解》 程序=数据结构+算法 小白推荐（我就是小白） 心理类： 《乌合之众》 人在小范围为私利行动 在民族范畴会抛开私利做出些过于崇高或粗鲁的行为 推荐 《原生家庭》 一个人受童年家庭的影响是最大的，这本书能看懂是一回事，能做好是另一回事","link":"/2018/09/04/2018booklist/"},{"title":"[片段]使用TypeToken在运行期保存泛型信息","text":"一般来说可以使用getGenericSuperclass 获取子类范型信息，但是泛型有嵌套的话想获取完整信息还是有点复杂的。例如：Message&lt;List&gt; 有两个泛型信息。 guava中有强大的TypeToken帮助你保存复杂泛型信息，可以参考： 123ParameterizedTypeReference&lt;Message&lt;T&gt;&gt; responseTypeRef = ParameterizedTypeReferenceBuilder.fromTypeToken( new TypeToken&lt;Message&lt;T&gt;&gt;() {}.where(new TypeParameter&lt;T&gt;() {}, new TypeToken&lt;List&lt;OrgSugVOV1&gt;&gt;() {})); 如果需要在spring框架中使用，需要一个适配器： 123456789101112131415161718192021222324252627282930313233343536public class ParameterizedTypeReferenceBuilder { public static &lt;T&gt; ParameterizedTypeReference&lt;T&gt; fromTypeToken(TypeToken&lt;T&gt; typeToken) { return new TypeTokenParameterizedTypeReference&lt;&gt;(typeToken); } private static class TypeTokenParameterizedTypeReference&lt;T&gt; extends ParameterizedTypeReference&lt;T&gt; { private final Type type; private TypeTokenParameterizedTypeReference(TypeToken&lt;T&gt; typeToken) { this.type = typeToken.getType(); } @Override public Type getType() { return type; } @Override public boolean equals(Object obj) { return (this == obj || (obj instanceof ParameterizedTypeReference &amp;&amp; this.type.equals(((ParameterizedTypeReference&lt;?&gt;) obj).getType()))); } @Override public int hashCode() { return this.type.hashCode(); } @Override public String toString() { return \"ParameterizedTypeReference&lt;\" + this.type + \"&gt;\"; } }} 关于java的泛型我就不多做吐槽了。","link":"/2019/02/26/2019-02-26-java-genic-type/"},{"title":"[项目感悟] 读《再谈敏捷开发与延期风控》","text":"本人本身不太喜欢方法论，感觉都是套路，生搬硬套不适合自己，敏捷开发就是其中让我保持谨慎态度的方法论之一。 敏捷开发与Scrum对于一个项目来说，能够即快又好地完成当然是非常棒的，但是众所周知，受限于项目管理三要素：时间、质量、成本，只能折衷选择。因此「敏捷」作为一种方法论（虽然Agile自称为Culture）被提出，其中Scrum(/skrʌm/，一种球类比赛)是比较知名的实现类之一。 在Scum中，它主要强调将瀑布式开发流程转为多阶段小迭代，可以理解为CPU的多级流水线(Instruction pipeline)设计，流水线设计将CPU的计算逻辑拆分，实现了复用计算模块，进而提高了时钟频率，同时也引入了寄存器/分支预测等管理模块增加了复杂度。 类似于CPU流水线机制，敏捷开发本质是在保持时间、质量不变的情况下，通过投入管理成本降低开发过程的空转成本，进而提高时钟周期的方法。 用白话来说，可以把软件开放比作流水车间，把PM，SE比作流水线工人。 我见过的的假敏捷然而到了现实，由于各种原因，却很容易成为假敏捷 将工位的隔栏拆开变成网吧“敏捷岛” 强行将Release计划拆成一个月一版，将Sprint拆成2周就看作快速迭代，照着人月神话反着搞 招聘一堆无责任心的开发让你去“敏捷”，永远无法实现“全功能部队” 客户难沟通，PO低估工作量，SE设计缺陷，编码质量低等原因，最终导致延期上述任何一个问题，都可能导致最终项目一锅粥，导致高层焦虑，中层跑路，底层混日子的结果。 敏捷能够提供强大高效的方法论，但是前提是需要本身基础过硬的团队，敏捷只能帮助存在进步瓶颈的团队。如果项目已经空心化，债务多，这不是敏捷方法论应该解决的问题。","link":"/2019/04/01/2019-04-01-[项目感悟] 读《再谈敏捷开发与延期风控》/"},{"title":"[项目] 根据权限查询时避免角色切换遇到的坑","text":"前情概要 1. 问题背景使用多个角色查询列表时，会遇到两个维度的不同点： 行维度：多个角色能够看到行的并集，sql需要多次查询取并集，之后还要去重分页排序 列维度：如果不同角色可见列不同，计算出当前行能看到列的并集 举一个例子： 假设存在一个登录员工拥有两个角色： 长期激励负责人：能看到拥有长期激励的人（行维度），能看到基本信息和长期激励信息（列维度） 薪酬负责人：能看到低职级的人（行维度），能看到基本信息和薪酬信息（列维度） 那么，在列表中他能看见： 基本信息 薪酬信息 长期激励信息 低职级/无长期激励 √ √ x 低职级/长期激励 √ √ √ 高职级/无长期激励 x x x 高职级/长期激励 √ x √ 2. 实际遇到的问题（困难重重）基本思路已经在前期概要里介绍，本人已经实践了一段时间，挖了两个深坑正在解决中。 性能问题（已解决）最开始的实现中数据是一条一条读取的，同时薪酬字段属于加密信息，使用了第三方微服务提供解密，读取字段多+解密字段多 导致了在百条分页的情况下接口在超时的边缘不断试探。。。 解决方案： 合并查询sql,批量查询数据 合并解密请求,批量调用解密微服务 因为之前为了方便我们解密使用了mybatis的TypeHandler做到字段隐式加解密，目前我们的做法是对于单条数据的加解密，还是保持原来的typeHandler做法，而对批量数据处理，重新写一套数据实体，同时使用mybatis的拦截器对查询的批量数据做批量解密的处理。具体做法可以参见我的另一片文章：【片段】 Mybatis ResultSetHandler 实践-续 批量查询带来的问题批量查询返回的列表中列字段都是一致的，而我们的需求是不同的行能看见不同的列字段，把批量查询出来的列表直接返回是有问题的，这个问题因为疏忽导致了线上的一次故障。 所以目前的思路是先做一次数据批量预取，之后在对列字段做处理，隐藏掉不能看见的字段。 3. 总结没有想到当时想解决权限查询时避免角色切换这个问题时会遇到这么多困难，想法是正确的，在实际执行时还是困难重重。值得欣慰的在最开始的时候思路和方向都是正确的，同时也把其中遇到的各种问题和心得记录了下来，经过层层积累，才到达现在的高度。 []: https://blog.yamato.moe/2018/11/06/2018-11-06-biz/ “根据权限查询时避免角色切换的一种思路”[]: https://blog.yamato.moe/2019/04/04/Mybatis%20ResultSetHandler_2019-04-04%20%E7%BB%AD/ “【片段】 Mybatis ResultSetHandler 实践-续”[]: https://blog.yamato.moe/2019/01/09/Mybatis%20ResultSetHandler_2019-01-09/ “【片段】 Mybatis ResultSetHandler 实践”","link":"/2019/05/17/2019-05-17-根据权限查询时避免角色切换遇到的坑/"},{"title":"季度总结 如何管理自我时间","text":"最近这个季度最近比较忙，算了下自己可以利用的空闲时间，忙时一天可能只有1到3个小时空闲时间，甚至一天没有空余时间。一周大约只有30小时时间是可以利用的，如果算上玩手机时间只会更短，如果再把这些时间再浪费掉，可能最近半年的成长就只有一些项目经验而已了。所幸平常关注的博客和《软技能》这本书里有提及一些关于自我时间管理相关内容，我简单实践了2个月，分享下这方面的心得感悟。 计划和休息《软技能》中作者是如何做计划的？ 季度计划 明确宏观目标，以及如何实现 月计划 估算当月能完成多少工作量 周计划 为每周安排必须完成的强制性任务 日计划 排除干扰，按需调整 休息 每隔一段时间休息，只做强制性任务 很惭愧，工作刚开始一两年我少有计划，单纯凭借好奇心学习到了不少东西。大约从去年写年度总结开始才刚刚做一些计划。目前我使用微软to-do跟踪自己的计划进度和deadline效果显著，治好了我的拖延症。 简单来说《软技能》中阐述的几个观点我感觉十分受益： 要有计划 完成计划时保持专注 使用番茄工作法可以保证保证一段时间的专注， 同时还可以确定自己的工作效率， 总结不足提高自身效率，从而帮助自己精确且高效的指定计划 只有你完成了计划的工作，接下来的休息时间才能安心 这边年读书情况这半年看的书比较少，但是刷题和博客总结写的多了些 技术类：《sql反模式》推荐， 应该叫数据库结构设计的最佳实践《软技能，代码之外的生存指南》 有很多事比代码更重要的多，推荐 心理类：《你的灯亮着吗》 解决问题前，先要搞清楚问题的本质。 一般般","link":"/2019/09/24/2019-06-24自我时间管理/"},{"title":"[项目] 多角色权限展示数据的一种实现","text":"多角色权限如果遇到不同角色能看到不同的列可以怎么做 逐行读取 最简单的解决方法，实现简单。但是在微服务中调用接口次数太多，性能很差。 批量读取 实现较复杂，但是性能好很多，下面主要介绍这种方法的思路 批量读取以分页读取数据为例： 读取第一页数据，包含需要展示数据的id和所属权限（多个） 为什么需要所属权限这个字段呢？ 因为决定能否看到这行是有你所拥有的所有权限决定的，而决定能否看到哪个列是由这行所拥有的权限决定的。 如何获取该行所拥有的权限呢，我的做法是分不同的权限查询结果通过union 组合起来 将第一页数据原始顺序保存， 然后按行拥有权限分组 记录原始顺序是因为后面分组后会打乱， 为什么要分组？分组后同样的查询才能聚合在一起，可以简化代码 根据权限分组多次查询所需要的字段，然后将查询结果合并 这里我使用的graphql来选择需要查询的字段 最后还原成原来的顺序 可以使用guava Ordering工具类方便生成Compartor []: https://blog.yamato.moe/2018/11/06/2018-11-06-biz/ “根据权限查询时避免角色切换的一种思路”[]: https://blog.yamato.moe/2019/04/04/Mybatis%20ResultSetHandler_2019-04-04%20%E7%BB%AD/ “【片段】 Mybatis ResultSetHandler 实践-续”[]: https://blog.yamato.moe/2019/01/09/Mybatis%20ResultSetHandler_2019-01-09/ “【片段】 Mybatis ResultSetHandler 实践”","link":"/2019/07/29/2019-09-27多角色权限展示数据的一种实现/"},{"title":"构建大型支付系统时分布式架构的关键点","text":"SLA在构建大型系统时，常常会遇到各种错误。在计划构建一个系统时，定义系统的“健康状态”十分重要。 “健康状态”必须是可度量的，一般做法是使用SLAs来度量系统的“健康状态”。最常见的SLA为 可达性 从时间维度衡量（99.999%可达性，每年下线50分钟） 准确性 对于数据的丢失或失真是否可以接受？可以达到多少百分比？对于支付系统来说，不接受任何数据的丢失和失真 容量 系统支持并发 延迟 响应延迟，一般衡量95%请求的响应时间和99%请求响应时间 确保新系统比被替代系统“更好”，可以使用上面四个SLA指标来衡量，可达性是最重要的需求。 水平和垂直伸缩随着新业务的增长，负载也会增加。最常见的伸缩策略是垂直和水平伸缩。 水平伸缩就是增加更多的机器或节点，对于分布式系统来说水平伸缩是最常有的方式。 垂直伸缩基本上就是买更大/好的机器。 一致性可达性对于任何系统都是很重要的，但是分布式系统一般都构建在低可达性的机器上（比如：服务的可达性要求99.999% 机器的可达性为99.9%）。简单的做法是维护一组机器组成集群，这样服务的可达性不依赖单独的机器。 一致性是在高可用系统中最需要关心的。一个一致性系统在所有的节点上看到和返回的数据在同一时间是相同的。如果使用一组机器来组成集群，它们还需要互相发送消息来保持同步，但是发送消息可能失败，这样一些节点就会因为不一致而不可达。 一致性有多个模型，在分布式系统最常用的是强一致性，弱一致性和最终一致性。一般来说，一致性要求越低，系统可以工作的更快，但是返回的数据不一定是最新的。 系统中的数据需要是一致的，但是到底是怎样的一致？对于一些系统，需要强一致性，比如一次支付必须是强一致的存储下来。对于没那么重要的部分，最终一致性是可以考虑的权衡。比如列出最近的交易。 数据持久性持久性表示一旦数据成功添加到数据存储，它就永远可以访问到。不同的分布式数据库拥有不同级别的数据持久性。一般使用副本来增加数据持久性。 对于支付系统来说，数据不允许丢失。我们构建的分布式数据存储需要支持集群级别的数据持久型。目前Cassandra, MongoDB, HDFS和Dynamodb 都支持多种级别的数据持久性。 消息保持与持久性分布式系统中的节点执行计算，存储数据，互相发送消息。发送消息的关键是消息的可靠到达。对于关键系统，经常需要消息零丢失。 对于分布式系统，发送消息一般石油分布式消息服务发送，如RabbitMQ，Kafka。这些消息服务支持不同级别的消息投递可靠性。 消息保持表示当节点处理消息失败时，在错误被解决前消息一直被保持着。消息的持久性一般在消息队列层被使用。如果在消息发送的时候队列或节点下线了，那在它们重新上线是还能接收到消息。 在支付系统中我们需要每一条消息投递一次，在构建系统中保证投递一次和投递至少一次在实现上是有区别的。最后我们使用了kafka来保证投递至少一次。 幂等性在分布式系统中，很多东西都可能出错，连接会丢包或超时，客户端经常会重试这些请求。一个幂等的系统保证无论多少特定的请求被执行，一个请求实际的操作只会执行一次。比如支付请求，如果客户端请求支付并且请求已经成功，但是客户端超时了，客户端是能够重试相同的请求的。对于一个幂等的系统，一个个人的支付是不能被收取两次的。 对幂等的设计，分布式系统需要某种分布式锁机制。假设我们想要使用乐观锁来实现幂等性，这时系统需要强一致性的锁来执行操作，我们可以使用同一个版本的乐观锁来检查是否有启动了额外的操作。 根据系统的一致性和操作的类型，有很多方式来实现幂等性。在设计分布式系统时，幂等性时最容易忽略的。在支付系统中，幂等操作时最重要的，它避免了双花和双收问题。消息系统已经保证了消息至少消费一次，我们只需要保证所有重复的消息保证幂等性即可。我们选择使用乐观锁，并使用强一致性存储作为乐观锁的数据源。 分片和法定人数分布式系统经常需要存储大量的数据，远超一台节点的容量。一般的解决方案时使用分片，数据使用某种hash算法被水平分区。尽管很多分布式数据库屏蔽了分片的实现，但是分片还是挺有意思的，特别是关于重新分片。 许多分布式系统在多个拥有数据和统计信息。为保证对数据操作的一致性，使用基于投票的方式是不行的，只有超过一定数量的节点操作成功，这个操作才是成功的，这个叫做法定人数。 Actor模型描述分布式系统最普遍的做法是使用Actor模型，还有一种方法是CSP。 Actor模型基于actor互相发送消息并作出回应。每一个actor只能做少量的动作，创建其他actors, 发送消息或者决定如何处理下个消息。通过这些简单的规则，复杂的分布式系统可以被准确描述，可以在actor崩溃后自我修复。 使用akka提供了标准的分布式模型，避免我们重复造轮子。 反应式架构当构建大型分布式系统时，目标常常是它们的弹性，伸缩性，和扩展性。反应式架构是在这个领域最流行和最通用的方案。","link":"/2018/11/19/Distributed architecture concepts I learned while building a large payments system/"},{"title":"IO Modle","text":"操作系统IO模型与Java IOJava IO模型和操作系统IO模型息息相关，之前阻塞/非阻塞，同步/非同步之间的关系一直分不清，所以很有必要了解下操作系统(linux)提供了哪些接口来进行IO。目前我们只需要了解即可，使用相关可以直接查看java io教程。 最基础的知识以使用IO读取数据为例，一般操作系统分为两个独立的阶段进行操作： 等待数据准备完成，可以是从磁盘拷贝到内核空间，或者是网卡接受到数据后拷贝到内核空间。 从内核空间将数据拷贝至请求数据的进程。如果是java可能还需从进程拷贝至jvm堆内存。 Blocking I/O Model这个是最常用的模型，望文生义就是阻塞IO，进行IO的两个阶段会都阻塞程序，直到读取到数据或者返回错误才会返回。 具体来说，通过调用系统recvfrom函数，而recvfrom函数会等到出错或者把数据拷贝到进程完成时才会返回。之后我们程序只需要处理错误或者处理数据就可以了。 阻塞模型对应java中绝大部分IO操作，比如网络请求api，io stream api，该模型优点在于简单直观，缺点在长时间阻塞很难支持大量并发IO请求。 Nonblocking I/O Model该模型在java中没有对应，所以这里只做简单介绍。 使用轮询方式调用系统recvfrom函数，recvfrom函数在第一阶段完成前一直返回错误，直到第一阶段完成后，阻塞至第二阶段完成。 这个模型稍显鸡肋，特点是在第一阶段是非阻塞的（进程不会被切换），代码相比阻塞模型来说也更复杂。 I/O Multiplexing Model非常著名的IO模型，可以支持大量并发IO。通过调用select或者pull并阻塞，而不是在实际调用系统IO时阻塞。使用select阻塞在第一阶段和Blocking I/O的阻塞不太一样，Blocking I/O阻塞在当前IO操作第一阶段，而I/O复用则可以注册多个I/O在select函数，当有一个I/O就绪时select函数就会返回，如果所有I/O处于第一阶段阻塞状态则select函数阻塞。 相比较Blocking I/O Model和Nonblocking I/O Model，I/O Multiplexing Model明显能在短时间内处理更多的I/O。如果使用多线程+Blocking I/O Model也能达到类似的效果，但是有可能消耗过多线程资源。 I/O Multiplexing Model对应java NIO的Selector等api Signal-Driven I/O Model该模型在java中没有对应，所以这里只做简单介绍。 该模型特点是第一阶段调用sigaction函数非阻塞返回，在第一阶段完成后发送信号SIGIO至进程，之后在signal handler中进行第二阶段处理。相当于对Nonblocking I/O Model的一种改进。 Asynchronous I/O ModelAsynchronous I/O Model相比较Signal-Driven I/O Model的区别在于通知的时机不同：Asynchronous I/O Model在第一和第二阶段都完成时通过信号通知进程操作完成。 Asynchronous I/O Model对应java中AsynchronousSocketChannel，AsynchronousServerSocketChannel 和 AsynchronousFileChannel等api。 各个模型比较","link":"/2019/05/20/IO-Modle/"},{"title":"[片段] Mybatis ResultSetHandler 实践","text":"这次拦截的方法是handleResultSets(Statement stmt)，用来批量解密用@Encrypted注解的String字段，可能还有一些坑。 12345678910111213141516171819202122232425262728293031323334353637@Overridepublic List&lt;Object&gt; handleResultSets(Statement stmt) throws SQLException { ErrorContext.instance().activity(\"handling results\").object(mappedStatement.getId()); final List&lt;Object&gt; multipleResults = new ArrayList&lt;Object&gt;(); int resultSetCount = 0; ResultSetWrapper rsw = getFirstResultSet(stmt); List&lt;ResultMap&gt; resultMaps = mappedStatement.getResultMaps(); int resultMapCount = resultMaps.size(); validateResultMapsCount(rsw, resultMapCount); while (rsw != null &amp;&amp; resultMapCount &gt; resultSetCount) { ResultMap resultMap = resultMaps.get(resultSetCount); handleResultSet(rsw, resultMap, multipleResults, null); rsw = getNextResultSet(stmt); cleanUpAfterHandlingResultSet(); resultSetCount++; } String[] resultSets = mappedStatement.getResultSets(); if (resultSets != null) { while (rsw != null &amp;&amp; resultSetCount &lt; resultSets.length) { ResultMapping parentMapping = nextResultMaps.get(resultSets[resultSetCount]); if (parentMapping != null) { String nestedResultMapId = parentMapping.getNestedResultMapId(); ResultMap resultMap = configuration.getResultMap(nestedResultMapId); handleResultSet(rsw, resultMap, null, parentMapping); } rsw = getNextResultSet(stmt); cleanUpAfterHandlingResultSet(); resultSetCount++; } } return collapseSingleResultList(multipleResults);} 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123package app.pooi.common.encrypt;import app.pooi.common.encrypt.anno.CipherSpi;import app.pooi.common.encrypt.anno.Encrypted;import lombok.Getter;import org.apache.ibatis.executor.resultset.ResultSetHandler;import org.apache.ibatis.plugin.*;import org.apache.ibatis.reflection.MetaObject;import org.apache.ibatis.reflection.SystemMetaObject;import java.lang.reflect.Field;import java.sql.Statement;import java.util.*;import java.util.function.Function;import java.util.logging.Logger;import java.util.stream.Collectors;@Intercepts({ @Signature(type = ResultSetHandler.class, method = \"handleResultSets\", args = {Statement.class}),})public class EncryptInterceptor implements Interceptor { private static final Logger logger = Logger.getLogger(EncryptInterceptor.class.getName()); private CipherSpi cipherSpi; public EncryptInterceptor(CipherSpi cipherSpi) { this.cipherSpi = cipherSpi; } @Override public Object intercept(Invocation invocation) throws Throwable { final Object proceed = invocation.proceed(); if (proceed == null) { return proceed; } List&lt;?&gt; results = (List&lt;?&gt;) proceed; if (results.isEmpty()) { return proceed; } final Object first = results.iterator().next(); final Class&lt;?&gt; modelClazz = first.getClass(); final List&lt;String&gt; fieldsNeedDecrypt = Arrays.stream(modelClazz.getDeclaredFields()) .filter(f -&gt; f.getAnnotation(Encrypted.class) != null) .filter(f -&gt; { boolean isString = f.getType() == String.class; if (!isString) { logger.warning(f.getName() + \"is not String, actual type is \" + f.getType().getSimpleName() + \" ignored\"); } return isString; }) .map(Field::getName) .collect(Collectors.toList()); final List&lt;List&lt;String&gt;&gt; partition = partition(fieldsNeedDecrypt, 20); for (Object r : results) { final MetaObject metaObject = SystemMetaObject.forObject(r); for (List&lt;String&gt; fields : partition) { final Map&lt;String, String&gt; fieldValueMap = fields.stream().collect(Collectors.toMap(Function.identity(), f -&gt; (String) metaObject.getValue(f))); final ArrayList&lt;String&gt; values = new ArrayList&lt;&gt;(fieldValueMap.values()); Map&lt;String, String&gt; decryptValues = cipherSpi.decrypt(values); fieldValueMap.entrySet() .stream() .map(e -&gt; Tuple2.of(e.getKey(), decryptValues.getOrDefault(e.getValue(), \"\"))) .forEach(e -&gt; metaObject.setValue(e.getT1(), e.getT2())); } } return results; } private &lt;T&gt; List&lt;List&lt;T&gt;&gt; partition(List&lt;T&gt; list, int batchCount) { if (!(batchCount &gt; 0)) { throw new IllegalArgumentException(\"batch count must greater than zero\"); } List&lt;List&lt;T&gt;&gt; partitionList = new ArrayList&lt;&gt;(list.size() / (batchCount + 1)); for (int i = 0; i &lt; list.size(); i += batchCount) { partitionList.add(list.stream().skip(i).limit(batchCount).collect(Collectors.toList())); } return partitionList; } @Override public Object plugin(Object target) { return Plugin.wrap(target, this); } @Override public void setProperties(Properties properties) { }}@Getterclass Tuple2&lt;T1, T2&gt; { private final T1 t1; private final T2 t2; Tuple2(T1 t1, T2 t2) { this.t1 = t1; this.t2 = t2; } static &lt;T1, T2&gt; Tuple2&lt;T1, T2&gt; of(T1 t1, T2 t2) { return new Tuple2&lt;&gt;(t1, t2); }}","link":"/2019/01/09/Mybatis ResultSetHandler_2019-01-09/"},{"title":"[片段] Mybatis ResultSetHandler 实践-续","text":"这次拦截的方法是handleResultSets(Statement stmt)，用来批量解密用@Encrypted注解的String字段。 上次的局限是只能批量解密一个对象的所有加密字段，对批量数据来说稍显不足，这个主要改进了这一点。 12345678910111213141516171819202122232425262728293031323334353637@Overridepublic List&lt;Object&gt; handleResultSets(Statement stmt) throws SQLException { ErrorContext.instance().activity(\"handling results\").object(mappedStatement.getId()); final List&lt;Object&gt; multipleResults = new ArrayList&lt;Object&gt;(); int resultSetCount = 0; ResultSetWrapper rsw = getFirstResultSet(stmt); List&lt;ResultMap&gt; resultMaps = mappedStatement.getResultMaps(); int resultMapCount = resultMaps.size(); validateResultMapsCount(rsw, resultMapCount); while (rsw != null &amp;&amp; resultMapCount &gt; resultSetCount) { ResultMap resultMap = resultMaps.get(resultSetCount); handleResultSet(rsw, resultMap, multipleResults, null); rsw = getNextResultSet(stmt); cleanUpAfterHandlingResultSet(); resultSetCount++; } String[] resultSets = mappedStatement.getResultSets(); if (resultSets != null) { while (rsw != null &amp;&amp; resultSetCount &lt; resultSets.length) { ResultMapping parentMapping = nextResultMaps.get(resultSets[resultSetCount]); if (parentMapping != null) { String nestedResultMapId = parentMapping.getNestedResultMapId(); ResultMap resultMap = configuration.getResultMap(nestedResultMapId); handleResultSet(rsw, resultMap, null, parentMapping); } rsw = getNextResultSet(stmt); cleanUpAfterHandlingResultSet(); resultSetCount++; } } return collapseSingleResultList(multipleResults);} 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130package app.pooi.common.encrypt;import app.pooi.common.encrypt.anno.CipherSpi;import app.pooi.common.encrypt.anno.Encrypted;import lombok.Getter;import org.apache.ibatis.executor.resultset.ResultSetHandler;import org.apache.ibatis.plugin.*;import org.apache.ibatis.reflection.MetaObject;import org.apache.ibatis.reflection.SystemMetaObject;import java.lang.reflect.Field;import java.sql.Statement;import java.util.*;import java.util.function.Function;import java.util.logging.Logger;import java.util.stream.Collectors;@Intercepts({ @Signature(type = ResultSetHandler.class, method = \"handleResultSets\", args = {Statement.class}),})public class DecryptInterceptor implements Interceptor { private static final Logger logger = Logger.getLogger(DecryptInterceptor.class.getName()); private CipherSpi cipherSpi; public DecryptInterceptor(CipherSpi cipherSpi) { this.cipherSpi = cipherSpi; } @Override public Object intercept(Invocation invocation) throws Throwable { final Object proceed = invocation.proceed(); if (proceed == null) { return proceed; } List&lt;?&gt; results = (List&lt;?&gt;) proceed; if (results.isEmpty()) { return proceed; } final Object first = results.iterator().next(); final Class&lt;?&gt; modelClazz = first.getClass(); final List&lt;String&gt; decryptFields = getDecryptFields(modelClazz); if (decryptFields.isEmpty()) { return proceed; } final List&lt;List&lt;String&gt;&gt; secret = Flux.fromIterable(results) .map(SystemMetaObject::forObject) .flatMapIterable(mo -&gt; decryptFields.stream().map(mo::getValue).collect(Collectors.toList())) .cast(String.class) .buffer(1000) .collectList() .block(); final Map&lt;String, String&gt; secretMap = secret.stream() .map(secrets -&gt; { try { return cipherSpi.batchDecrypt(secrets); } catch (Exception e) { e.printStackTrace(); return Maps.&lt;String, String&gt;newHashMap(); } }).reduce(Maps.newHashMap(), (m1, m2) -&gt; { m1.putAll(m2); return m1; }); secretMap.put(\"\", \"0\"); for (Object r : results) { final MetaObject metaObject = SystemMetaObject.forObject(r); decryptFields.forEach(f -&gt; metaObject.setValue(f, secretMap.get(metaObject.getValue(f)))); } return results; } @NotNull private List&lt;String&gt; getDecryptFields(Class&lt;?&gt; modelClazz) { return Arrays.stream(modelClazz.getDeclaredFields()) .filter(f -&gt; f.getAnnotation(Decrypted.class) != null) .filter(f -&gt; { boolean isString = f.getType() == String.class; if (!isString) { logger.warning(f.getName() + \"is not String, actual type is \" + f.getType().getSimpleName() + \" ignored\"); } return isString; }) .map(Field::getName) .collect(Collectors.toList()); } @Override public Object plugin(Object target) { return Plugin.wrap(target, this); } @Override public void setProperties(Properties properties) { }}@Getterclass Tuple2&lt;T1, T2&gt; { private final T1 t1; private final T2 t2; Tuple2(T1 t1, T2 t2) { this.t1 = t1; this.t2 = t2; } static &lt;T1, T2&gt; Tuple2&lt;T1, T2&gt; of(T1 t1, T2 t2) { return new Tuple2&lt;&gt;(t1, t2); }}","link":"/2019/04/04/Mybatis ResultSetHandler_2019-04-04 续/"},{"title":"[片段] Java收集方法参数+Spring DataBinder","text":"收集参数目前是使用了spring aop 来拦截方法调用，把方法参数包装成Map形式 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758@Target(ElementType.METHOD)@Retention(RetentionPolicy.RUNTIME)@Documentedpublic @interface CollectArguments {}@Aspectpublic class ArgumentsCollector { private static final ThreadLocal&lt;Map&lt;String, Object&gt;&gt; ARGUMENTS = ThreadLocal.withInitial(ImmutableMap::of); static Map&lt;String, Object&gt; getArgs() { return ARGUMENTS.get(); } private Object[] args(Object[] args, int exceptLength) { if (exceptLength == args.length) { return args; } return Arrays.copyOf(args, exceptLength); } @Pointcut(\"@annotation(CollectArguments)\") void collectArgumentsAnnotationPointCut() { } @Before(\"collectArgumentsAnnotationPointCut()\") public void doAccessCheck(JoinPoint joinPoint) { final String[] parameterNames = ((MethodSignature) joinPoint.getSignature()).getParameterNames(); final Object[] args = args(joinPoint.getArgs(), parameterNames.length); ARGUMENTS.set(Collections.unmodifiableMap((IntStream.range(0, parameterNames.length - 1) .mapToObj(idx -&gt; Tuple2.of(parameterNames[idx], args[idx])) .collect(HashMap::new, (m, t) -&gt; m.put(t.getT1(), t.getT2()), HashMap::putAll)))); } @After(\"collectArgumentsAnnotationPointCut()\") public void remove() { ARGUMENTS.remove(); } @Data private static class Tuple2&lt;T1, T2&gt; { private T1 t1; private T2 t2; Tuple2(T1 t1, T2 t2) { this.t1 = t1; this.t2 = t2; } public static &lt;T1, T2&gt; Tuple2&lt;T1, T2&gt; of(T1 t1, T2 t2) { return new Tuple2&lt;&gt;(t1, t2); } }} 通过Map构造对象123456789101112public class BinderUtil { BinderUtil() { } @SuppressWarnings(\"unchecked\") public static &lt;T&gt; T getTarget(Class&lt;T&gt; beanClazz) { final DataBinder binder = new DataBinder(BeanUtils.instantiate(beanClazz)); binder.bind(new MutablePropertyValues(ArgumentsCollector.getArgs())); return (T) binder.getTarget(); }}","link":"/2019/01/22/Spring_aop_argument_collector_2019-01-22/"},{"title":"resilience4j-retry源码阅读","text":"resilience4j 源码还是比较清晰简单的，比较适合阅读。 放一张主要类的结构图： Retry入口Retry接口是提供重试功能的入口，主要提供了方法模版，具体校验结构，失败后处理由Context子类实现。 1234567891011121314151617181920212223/** * Creates a retryable supplier. * * @param retry the retry context * @param supplier the original function * @param &lt;T&gt; the type of results supplied by this supplier * @return a retryable function */static &lt;T&gt; Supplier&lt;T&gt; decorateSupplier(Retry retry, Supplier&lt;T&gt; supplier) { return () -&gt; { Retry.Context&lt;T&gt; context = retry.context(); do try { T result = supplier.get(); final boolean validationOfResult = context.onResult(result); if (!validationOfResult) { context.onSuccess(); return result; } } catch (RuntimeException runtimeException) { context.onRuntimeError(runtimeException); } while (true); };} 这里摘抄了一段核心代码，作用是循环直到context.onResult(result)返回true为止，需要留意context.onResult/onRuntimeError/onError可能执行多次， onSuccess只会执行一次，这里每次进入重试都是一个新的context对象。 Retry.ContextImpl1234567891011121314151617181920212223public boolean onResult(T result) { if (null != resultPredicate &amp;&amp; resultPredicate.test(result)) { int currentNumOfAttempts = numOfAttempts.incrementAndGet(); if (currentNumOfAttempts &gt;= maxAttempts) { return false; } else { waitIntervalAfterFailure(currentNumOfAttempts, null); return true; } } return false;}public void onRuntimeError(RuntimeException runtimeException) { if (exceptionPredicate.test(runtimeException)) { lastRuntimeException.set(runtimeException); throwOrSleepAfterRuntimeException(); } else { failedWithoutRetryCounter.increment(); publishRetryEvent(() -&gt; new RetryOnIgnoredErrorEvent(getName(), runtimeException)); throw runtimeException; }} 先关注onResult，它负责判断是否需要继续重试，如果通过校验或者重试超过此数，会停止重试。 onRuntimeError/onError, 负责把catch的异常存储在lastRuntimeException中。 12345678910public void onSuccess() { int currentNumOfAttempts = numOfAttempts.get(); if (currentNumOfAttempts &gt; 0) { succeededAfterRetryCounter.increment(); Throwable throwable = Option.of(lastException.get()).getOrElse(lastRuntimeException.get()); publishRetryEvent(() -&gt; new RetryOnSuccessEvent(getName(), currentNumOfAttempts, throwable)); } else { succeededWithoutRetryCounter.increment(); }} onSuccess负责统计和发送事件。 总结总体来说retry比较简单，需要注意的点有一个如果设置了结果校验，如果一直校验不通过，将返回未通过的结果，而不是返回失败。","link":"/2019/04/18/resilience4j-retry源码阅读/"},{"title":"[感悟] 2018我的所见所闻所感","text":"1. 我就喜欢写代码行不行？如果刚刚投入工作，工作还无法完全胜任，那我的建议是多写写代码，打怪升级完成自己的年度绩效。 如果已经能够胜任当前工作，下一步应该集中在提效上。这一步和之前自己积累的经验和知识密不可分，只有真正了解代码懂代码，才能从同龄人中的“熟练工”脱颖而出，两者的提效虽然结果一样，但是本质却完全不一样。 如果已经摆脱了熟练工的称号，实际上已经完成了自我提效，提前完成自己的本职任务，下一步可以计划推动团队的效率。 如果太过专注技术，专心自己的一亩三分田，相当于给自己设限成毕业两三年的水平。这种人应该被打上不胜任的标签，在寒冬中很容易被优化掉。专注技术还有一个误区，就是容易把“实施细节”和“技术”两者混淆，特别在软件行业“实施细节”很容易随时代改变，基本三年就会大变样，而“技术”类似于“知识”不会过时。看到这里大家可以自己思考下，自己学到的到底是“实施细节”还是真正的“技术”。 2.如何推动团队进步恭喜你跨过了第一道坎，推动团队进步不是一个人的事，一般推动团队分为两部分：个人影响团队，团队自我驱动。 个人影响团队比较简单，就是把自我提效所积累的经验和知识共享给整个团队，完成的手段可以是博客分享，会议分享，文档分享。 团队自我驱动，实际上我把整个团队拟作了一个人，一个人找出别人的缺点很容易，但是找出一个团队的问题却没那么容易，同时也会受到公司和领导的局限，比如一些项目管理的领导就是二传手，只催你进度的那种，这时候就需要你主动找他讨论。 如何找到团队的缺点？可以通过下面两个大类的套路：管理手段和技术手段。 管理手段可以从知识管理、代码规范、需求分析三处着手： 知识管理： 建立知识库，避免重复的培训，重复的解答问题 代码规范：借助代码缺陷检查工具，具体到负责人提升代码规范 需求分析：避免低效，无效会议，避免各种妥协下产生的需求 技术手段比较简单： 重构升级：弃用老的架构，拥抱新技术，带领团队提升技术 内部造轮子：内部定制工具，帮助团队高效完成任务 找到了团队的缺点接下来可以制定度量，衡量团队的推进程度，可以从两个角度进行度量： 跟自己比较：比如这次做需求提测bug数减少，需求delay少了，满足需求不需要上线只要配置上线，等等 和别的团队比较：这个比较凶残，我也不知道用什么度量比，但是有的大公司就是这样做的。 3.总结与晋升从发现缺点到最后得到成果完成团队推进目标，是时候写个ppt在领导面前吹一波了，这个就不用我教了。","link":"/2018/12/21/【归档】2018我的所见所闻所感/"},{"title":"[片段] @CreatedBy / @ModifiedBy 拦截器实现","text":"拦截器实现： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859package app.pooi.common.entity;import app.pooi.common.entity.anno.CreatedBy;import app.pooi.common.entity.anno.ModifiedBy;import lombok.Data;import org.apache.ibatis.executor.Executor;import org.apache.ibatis.mapping.MappedStatement;import org.apache.ibatis.plugin.Intercepts;import org.apache.ibatis.plugin.Invocation;import org.apache.ibatis.plugin.Plugin;import org.apache.ibatis.plugin.Signature;import java.util.Arrays;import java.util.Properties;import java.util.function.Supplier;@Data@Intercepts({ @Signature(type = Executor.class, method = \"update\", args = {MappedStatement.class, Object.class}),})public class EntityInterceptor implements org.apache.ibatis.plugin.Interceptor { private Supplier&lt;Long&gt; auditorAware; @Override public Object intercept(Invocation invocation) throws Throwable { Executor executor = (Executor) invocation.getTarget(); MappedStatement ms = (MappedStatement) invocation.getArgs()[0]; Object o = invocation.getArgs()[1]; Arrays.stream(o.getClass().getDeclaredFields()) .forEach(field -&gt; { final CreatedBy createdBy = field.getAnnotation(CreatedBy.class); final ModifiedBy modifiedBy = field.getAnnotation(ModifiedBy.class); if (createdBy != null || modifiedBy != null) { field.setAccessible(true); try { field.set(o, auditorAware.get()); } catch (IllegalAccessException ignore) { } } }); return invocation.proceed(); } @Override public Object plugin(Object target) { return Plugin.wrap(target, this); } @Override public void setProperties(Properties properties) { }} 配置： 1234567891011121314@Configurationstatic class MybatisInterceptorConfig { @Bean public Interceptor[] configurationCustomizer(CipherSpi cipherSpi) { final EntityInterceptor entityInterceptor = new EntityInterceptor(); entityInterceptor.setAuditorAware(() -&gt; { final String header = ((ServletRequestAttributes) RequestContextHolder.getRequestAttributes()).getRequest().getHeader(XHeaders.LOGIN_USER_ID); return Long.valueOf(header); }); return new Interceptor[]{new DecryptInterceptor(cipherSpi), entityInterceptor}; }}","link":"/2019/02/11/【片段】@CreatedBy  @ModifiedBy/"},{"title":"[片段] SpringBoot Mybatis配置","text":"纯记录，供自己参考🤣。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140private final MybatisProperties properties;private final Interceptor[] interceptors;private final ResourceLoader resourceLoader;private final DatabaseIdProvider databaseIdProvider;private final List&lt;ConfigurationCustomizer&gt; configurationCustomizers;public DataSourceConfig(MybatisProperties properties, ObjectProvider&lt;Interceptor[]&gt; interceptorsProvider, ResourceLoader resourceLoader, ObjectProvider&lt;DatabaseIdProvider&gt; databaseIdProvider, ObjectProvider&lt;List&lt;ConfigurationCustomizer&gt;&gt; configurationCustomizersProvider) { this.properties = properties; this.interceptors = interceptorsProvider.getIfAvailable(); this.resourceLoader = resourceLoader; this.databaseIdProvider = databaseIdProvider.getIfAvailable(); this.configurationCustomizers = configurationCustomizersProvider.getIfAvailable();}/** * 普通数据源 * 主数据源，必须配置，spring启动时会执行初始化数据操作（无论是否真的需要），选择查找DataSource class类型的数据源 * * @return {@link DataSource} */@Primary@Bean(name = BEANNAME_DATASOURCE_COMMON)@ConfigurationProperties(prefix = \"com.lianjia.confucius.bridge.boot.datasource.common\")public DataSource createDataSourceCommon() { return DataSourceBuilder.create().build();}/** * 只读数据源 * * @return {@link DataSource} */@Bean(name = BEANNAME_DATASOURCE_READONLY)@ConfigurationProperties(prefix = \"com.lianjia.confucius.bridge.boot.datasource.readonly\")public DataSource createDataSourceReadonly() { return DataSourceBuilder.create().build();}private SqlSessionFactory sqlSessionFactory(DataSource dataSource) throws Exception { SqlSessionFactoryBean factory = new SqlSessionFactoryBean(); factory.setDataSource(dataSource); factory.setVfs(SpringBootVFS.class); if (StringUtils.hasText(this.properties.getConfigLocation())) { factory.setConfigLocation(this.resourceLoader.getResource(this.properties.getConfigLocation())); } org.apache.ibatis.session.Configuration configuration = this.properties.getConfiguration(); if (configuration == null &amp;&amp; !StringUtils.hasText(this.properties.getConfigLocation())) { configuration = new org.apache.ibatis.session.Configuration(); } if (configuration != null &amp;&amp; !CollectionUtils.isEmpty(this.configurationCustomizers)) { for (ConfigurationCustomizer customizer : this.configurationCustomizers) { customizer.customize(configuration); } } factory.setConfiguration(configuration); if (this.properties.getConfigurationProperties() != null) { factory.setConfigurationProperties(this.properties.getConfigurationProperties()); } if (!ObjectUtils.isEmpty(this.interceptors)) { factory.setPlugins(this.interceptors); } if (this.databaseIdProvider != null) { factory.setDatabaseIdProvider(this.databaseIdProvider); } if (StringUtils.hasLength(this.properties.getTypeAliasesPackage())) { factory.setTypeAliasesPackage(this.properties.getTypeAliasesPackage()); } if (StringUtils.hasLength(this.properties.getTypeHandlersPackage())) { factory.setTypeHandlersPackage(this.properties.getTypeHandlersPackage()); } if (!ObjectUtils.isEmpty(this.properties.resolveMapperLocations())) { factory.setMapperLocations(this.properties.resolveMapperLocations()); } return factory.getObject();}public SqlSessionTemplate sqlSessionTemplate(SqlSessionFactory sqlSessionFactory) { ExecutorType executorType = this.properties.getExecutorType(); if (executorType != null) { return new SqlSessionTemplate(sqlSessionFactory, executorType); } else { return new SqlSessionTemplate(sqlSessionFactory); }}@Bean@Primarypublic SqlSessionFactory primarySqlSessionFactory() throws Exception { return this.sqlSessionFactory(this.createDataSourceCommon());}@Beanpublic SqlSessionFactory secondarySqlSessionFactory() throws Exception { return this.sqlSessionFactory(this.createDataSourceReadonly());}/** * 实例普通的 sqlSession * * @return SqlSession * @throws Exception when any exception occured */@Bean(name = BEANNAME_SQLSESSION_COMMON)public SqlSession initSqlSessionCommon() throws Exception { return this.sqlSessionTemplate(this.primarySqlSessionFactory());}/** * 实例只读的 sqlSession * * @return SqlSession * @throws Exception when any exception occured */@Bean(name = BEANNAME_SQLSESSION_READONLY)public SqlSession initSqlSessionReadonly() throws Exception { return this.sqlSessionTemplate(this.secondarySqlSessionFactory());}@MapperScan(annotationClass = PrimaryMapper.class, sqlSessionTemplateRef = BEANNAME_SQLSESSION_COMMON, basePackageClasses = ITalentApplicationSpringBootStart.class)static class PrimaryMapperConfiguration {}@MapperScan(annotationClass = SecondaryMapper.class, sqlSessionTemplateRef = BEANNAME_SQLSESSION_READONLY, basePackageClasses = ITalentApplicationSpringBootStart.class)static class SecondaryMapperConfiguration {}","link":"/2019/03/13/【片段】SpringBoot Mybatis配置/"},{"title":"LeetCode二叉树排序树基础算法","text":"修剪二叉搜索树12345678910111213141516171819/** * Definition for a binary tree node. * public class TreeNode { * int val; * TreeNode left; * TreeNode right; * TreeNode(int x) { val = x; } * } */class Solution { public TreeNode trimBST(TreeNode root, int L, int R) { if (root == null) return null; if (root.val &lt; L) return trimBST(root.right, L, R); if (root.val &gt; R) return trimBST(root.left, L, R); root.left = trimBST(root.left, L, R); root.right = trimBST(root.right, L, R); return root; }} 二叉搜索树中第K小的元素12345678910111213141516171819202122232425262728293031/** * Definition for a binary tree node. * public class TreeNode { * int val; * TreeNode left; * TreeNode right; * TreeNode(int x) { val = x; } * } */class Solution { private int cnt; private int val; public int kthSmallest(TreeNode root, int k) { search(root, k); return val; } private void search(TreeNode root, int k) { if (root == null) return; // kthSmallest(root.left, k); cnt++; if (cnt == k) { val = root.val; return; } kthSmallest(root.right, k); }} 把二叉搜索树转换为累加树1234567891011121314151617181920212223242526272829303132/** * Definition for a binary tree node. * public class TreeNode { * int val; * TreeNode left; * TreeNode right; * TreeNode(int x) { val = x; } * } */class Solution { private int sum; public TreeNode convertBST(TreeNode root) { // 中序遍历 但是是从右往左遍历 // visit(root); return root; } private void visit(TreeNode root) { if (root == null) return; visit(root.right); sum += root.val; root.val = sum; visit(root.left); }} 二叉搜索树的最近公共祖先12345678910111213141516171819/** * Definition for a binary tree node. * public class TreeNode { * int val; * TreeNode left; * TreeNode right; * TreeNode(int x) { val = x; } * } */class Solution { public TreeNode lowestCommonAncestor(TreeNode root, TreeNode p, TreeNode q) { // 公共祖先在左边 if (root.val &gt; p.val &amp;&amp; root.val &gt; q.val) return lowestCommonAncestor(root.left, p, q); // 公共祖先在右边 if (root.val &lt; p.val &amp;&amp; root.val &lt; q.val) return lowestCommonAncestor(root.right, p, q); // 公共祖先在这 return root; }} 二叉树的最近公共祖先12345678910111213141516171819202122/** * Definition for a binary tree node. * public class TreeNode { * int val; * TreeNode left; * TreeNode right; * TreeNode(int x) { val = x; } * } */class Solution { public TreeNode lowestCommonAncestor(TreeNode root, TreeNode p, TreeNode q) { if (root == null || p == root || q == root) return root; TreeNode left = lowestCommonAncestor(root.left, p, q); TreeNode right = lowestCommonAncestor(root.right, p, q); // 左右都是父节点， 上一级就是公共父节点 if(left != null &amp;&amp; right != null) return root; if (left == null &amp;&amp; right == null) return null; if (left != null) return left; return right; }} 将有序数组转换为二叉搜索树二叉树中序遍历 1234567891011121314151617181920212223242526/** * Definition for a binary tree node. * public class TreeNode { * int val; * TreeNode left; * TreeNode right; * TreeNode(int x) { val = x; } * } */class Solution { public TreeNode sortedArrayToBST(int[] nums) { return build(nums, 0, nums.length -1); } private TreeNode build(int[] nums, int start, int end) { if(start&gt; end) return null; TreeNode node = new TreeNode(nums[(start+end)/2]); node.left = build(nums, start, (start+end)/2 -1); node.right = build(nums, (start+end)/2+1, end); return node; }} 有序链表转换二叉搜索树链表转数组 1234567891011121314151617181920212223242526272829303132333435363738394041/** * Definition for singly-linked list. * public class ListNode { * int val; * ListNode next; * ListNode(int x) { val = x; } * } *//** * Definition for a binary tree node. * public class TreeNode { * int val; * TreeNode left; * TreeNode right; * TreeNode(int x) { val = x; } * } */class Solution { public TreeNode sortedListToBST(ListNode head) { List&lt;Integer&gt; list = new LinkedList(); ListNode now = head; while (now != null) { list.add(now.val); now = now.next; } return build(list, 0, list.size() - 1); } private TreeNode build(List&lt;Integer&gt; nums, int start, int end) { if(start&gt; end) return null; TreeNode node = new TreeNode(nums.get((start+end)/2)); node.left = build(nums, start, (start+end)/2 -1); node.right = build(nums, (start+end)/2+1, end); return node; }} 还可以使用双指针找到链表中间节点，缺点是重复遍历节点 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162/** * Definition for singly-linked list. * public class ListNode { * int val; * ListNode next; * ListNode(int x) { val = x; } * } *//** * Definition for a binary tree node. * public class TreeNode { * int val; * TreeNode left; * TreeNode right; * TreeNode(int x) { val = x; } * } */class Solution { public TreeNode sortedListToBST(ListNode head) {// List&lt;Integer&gt; list = new LinkedList();// ListNode now = head;// while (now != null) {// list.add(now.val);// now = now.next;// } // return build(list, 0, list.size() - 1); return sortedListToBST(head, null); } private TreeNode sortedListToBST(ListNode head, ListNode tail) { if (head == tail) return null; ListNode mid = head, end = head; while (end != tail &amp;&amp; end.next != tail) { mid = mid.next; end = end.next.next; } TreeNode root = new TreeNode(mid.val); root.right = sortedListToBST(mid.next, tail); root.left = sortedListToBST(head, mid); return root; } private TreeNode build(List&lt;Integer&gt; nums, int start, int end) { if(start&gt; end) return null; TreeNode node = new TreeNode(nums.get((start+end)/2)); node.left = build(nums, start, (start+end)/2 -1); node.right = build(nums, (start+end)/2+1, end); return node; } } 两数之和 IV - 输入 BST自己写两次遍历搜索二叉树，注意要排除自身节点 123456789101112131415161718192021222324252627282930313233343536/** * Definition for a binary tree node. * public class TreeNode { * int val; * TreeNode left; * TreeNode right; * TreeNode(int x) { val = x; } * } */class Solution { private boolean res; private TreeNode r; private TreeNode current; public boolean findTarget(TreeNode root, int k) { r = root; visit(root, k); return res; } private void visit(TreeNode root, int val) { if (root == null) return; visit(root.left, val); current = root; if (find(r, val - root.val)) {res = true; return;} visit(root.right, val); } private boolean find(TreeNode root, int value) { if (root == null) return false; if (root == current) return false; if (root.val == value ) return true; return (value &gt; root.val) ? find(root.right, value): find(root.left, value); }} 正经思路， 中序遍历转化为排序数组， 使用双指针查找 1234567891011121314151617181920212223242526272829303132333435363738/** * Definition for a binary tree node. * public class TreeNode { * int val; * TreeNode left; * TreeNode right; * TreeNode(int x) { val = x; } * } */class Solution { private List&lt;Integer&gt; res = new ArrayList&lt;&gt;(64); public boolean findTarget(TreeNode root, int k) { visitTree(root); int low = 0, high = res.size() - 1; while (low &lt; high) { int sum = res.get(low) + res.get(high); if (sum == k) return true; if (sum &lt; k) low++; else high--; } return false; } private void visitTree(TreeNode root) { if (root == null) return; visitTree(root.left); res.add(root.val); visitTree(root.right); }}","link":"/2019/06/15/2019-06-30树算法 - 二叉排序树/"},{"title":"AbstractQueuedSynchronizer解析","text":"AbstractQueuedSynchronizer 数据结构123456789101112131415161718 /** * Head of the wait queue , lazily initialized . Except for * initialization , it is modified only via method setHead . Note : * If head exists , its waitStatus is guaranteed not to be * CANCELLED . */ private transient volatile Node head; /** * Tail of the wait queue , lazily initialized . Modified only via * method enq to add new wait node . */ private transient volatile Node tail; /** * The synchronization state . */ private volatile int state; 稍微注意下在线程争用锁是才会初始化链表 AbstractQueuedSynchronizer.Node 数据结构123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081/** * Status field , taking on only the values : * SIGNAL : The successor of this node is ( or will soon be ) * blocked ( via park ), so the current node must * unpark its successor when it releases or * cancels . To avoid races , acquire methods must * first indicate they need a signal , * then retry the atomic acquire , and then , * on failure , block . * CANCELLED : This node is cancelled due to timeout or interrupt . * Nodes never leave this state . In particular , * a thread with cancelled node never again blocks . * CONDITION : This node is currently on a condition queue . * It will not be used as a sync queue node * until transferred , at which time the status * will be set to 0. ( Use of this value here has * nothing to do with the other uses of the * field , but simplifies mechanics .) * PROPAGATE : A releaseShared should be propagated to other * nodes . This is set ( for head node only ) in * doReleaseShared to ensure propagation * continues , even if other operations have * since intervened . * 0: None of the above * * The values are arranged numerically to simplify use . * Non - negative values mean that a node doesn ' t need to * signal . So , most code doesn ' t need to check for particular * values , just for sign . * * The field is initialized to 0 for normal sync nodes , and * CONDITION for condition nodes . It is modified using CAS * ( or when possible , unconditional volatile writes ). */ volatile int waitStatus; /** * Link to predecessor node that current node / thread relies on * for checking waitStatus . Assigned during enqueuing , and nulled * out ( for sake of GC ) only upon dequeuing . Also , upon * cancellation of a predecessor , we short - circuit while * finding a non - cancelled one , which will always exist * because the head node is never cancelled : A node becomes * head only as a result of successful acquire . A * cancelled thread never succeeds in acquiring , and a thread only * cancels itself , not any other node . */ volatile Node prev; /** * Link to the successor node that the current node / thread * unparks upon release . Assigned during enqueuing , adjusted * when bypassing cancelled predecessors , and nulled out ( for * sake of GC ) when dequeued . The enq operation does not * assign next field of a predecessor until after attachment , * so seeing a null next field does not necessarily mean that * node is at end of queue . However , if a next field appears * to be null , we can scan prev ' s from the tail to * double - check . The next field of cancelled nodes is set to * point to the node itself instead of null , to make life * easier for isOnSyncQueue . */ volatile Node next; /** * The thread that enqueued this node . Initialized on * construction and nulled out after use . */ volatile Thread thread; /** * Link to next node waiting on condition , or the special * value SHARED . Because condition queues are accessed only * when holding in exclusive mode , we just need a simple * linked queue to hold nodes while they are waiting on * conditions . They are then transferred to the queue to * re - acquire . And because conditions can only be exclusive , * we save a field by using special value to indicate shared * mode . */ Node nextWaiter; AbstractQueuedSynchronizer** 的数据结构（盗用的图） AbstractQueuedSynchronizer 做了什么 ?内部维护state和CLH队列，负责在资源争用时线程入队，资源释放时唤醒队列中线程。 而实现类只需要实现 什么条件获取资源成功 和 什么条件释放资源 成功就可以了 所以，最简单的CountDownLatch使用AbstractQueuedSynchronizer实现非常简单： 申明AbstractQueuedSynchronizer的state数量（比如十个） await方法尝试获取资源，如果state&gt;0表示获取失败（ 什么条件获取资源成功 ，CountDownLatch实现），获取失败线程休眠（AbstractQueuedSynchronizer负责） countDown方法state-1，如果state==0表示资源释放成功( 什么条件释放资源成功 ，CountDownLatch实现)，唤醒队列中所有线程（AbstractQueuedSynchronizer负责） AbstractQueuedSynchronizer 怎么做的?顺着ReentrantLock lock、unlock看一遍我们就大致总结出AbstractQueuedSynchronizer工作原理了 先简单介绍下ReentrantLock特性：可重入，中断，有超时机制。 ReentrantLock lock() 流程 ( 再盗图 ) 黄色表示ReentrantLock实现，绿色表示AbstractQueuedSynchronizer内部实现 lock方法入口 直接调用 AbstractQueuedSynchronizer.acquire方法 tryAcquire addWaiter acquireQueued AbstractQueuedSynchronizer.acquire12345**public** final void acquire ( int arg) { **if** (! tryAcquire (arg) &amp;&amp; acquireQueued ( addWaiter ( Node . EXCLUSIVE ), arg)) selfInterrupt (); } 获取的锁的逻辑：直接获取成功则返回，如果没有获取成功入队休眠（对就是这么简单） 下面我们仔细一个一个方法看 ReentrantLock.tryAcquire我这里贴的时非公平的所获取，公平和不公平的区别在于公平锁老老实实的会进入队列排队，非公平锁会先检查资源是否可用，如果可用不管队列中的情况直接尝试获取锁。 123456789101112131415161718final boolean nonfairTryAcquire ( int acquires) { final Thread current = Thread . currentThread (); int c = getState (); if (c == 0 ) { if ( compareAndSetState ( 0 , acquires)) { setExclusiveOwnerThread (current); return true ; } } else if (current == getExclusiveOwnerThread ()) { int nextc = c + acquires; if (nextc &lt; 0 ) // overflow throw new Error ( \"Maximum lock count exceeded\" ); setState (nextc); return true ; } return false ; } ReentrantLock.tryAcquire读取到state==0时尝试占用锁，并保证同一线程可以重复占用。其他情况下获取资源失败。如果获取成功就没啥事了，不过关键不就是锁争用的时候是如何处理的吗？ AbstractQueuedSynchronizer.addWaiter(Node.EXCLUSIVE)12345678910111213141516private Node addWaiter ( Node mode) { Node node = new Node (mode); for (;;) { Node oldTail = tail; if (oldTail != null ) { node. setPrevRelaxed (oldTail); if ( compareAndSetTail (oldTail, node)) { oldTail. next = node; return node; } } else { initializeSyncQueue (); } } } 一旦锁争用，一定会初始化队列（因为排队的线程需要前驱节点唤醒，所以要初始化一个前驱节点），之后自旋成为队列尾节点。 简单来说就是获取不到锁就放进队列里维护起来，等锁释放的时候再用。 这里还有一个 很具有参考性的小细节 ：先设置新节点的前驱结点，自旋成为尾节点后设置前驱的后驱 AbstractQueuedSynchronizer.acquireQueued1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253final boolean acquireQueued ( final Node node, int arg) { boolean interrupted = false ; try { for (;;) { final Node p = node. predecessor (); if (p == head &amp;&amp; tryAcquire (arg)) { setHead (node); p. next = null ; // help GC return interrupted; } if ( shouldParkAfterFailedAcquire (p, node)) interrupted |= parkAndCheckInterrupt (); } } catch ( Throwable t) { cancelAcquire (node); if (interrupted) selfInterrupt (); throw t; } } private static boolean shouldParkAfterFailedAcquire ( Node pred, Node node) { int ws = pred. waitStatus ; if (ws == Node . SIGNAL ) /* * This node has already set status asking a release * to signal it, so it can safely park. */ return true ; if (ws &gt; 0 ) { /* * Predecessor was cancelled. Skip over predecessors and * indicate retry. */ do { node. prev = pred = pred. prev ; } while (pred. waitStatus &gt; 0 ); pred. next = node; } else { /* * waitStatus must be 0 or PROPAGATE. Indicate that we * need a signal, but don't park yet. Caller will need to * retry to make sure it cannot acquire before parking. */ pred. compareAndSetWaitStatus (ws, Node . SIGNAL ); } return false ; } private final boolean parkAndCheckInterrupt () { LockSupport . park ( this ); return Thread . interrupted (); } 前面只是维护下链表数据结构，这里负责找到合适的唤醒前驱，然后让线程休眠。 这里主要是一个循环过程： 检查是否能获取到锁，获取到则返回 失败则寻找前面最近的未放弃争用的前驱，把前驱的waitStatus设置为-1，并把放弃争用的节点抛弃 检查是否能休眠 使用Usafe.park休眠（不是wait） ReentrantLock lock 总结 ReentrantLock unlock()12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849public final boolean release ( int arg) { if ( tryRelease (arg)) { Node h = head; if (h != null &amp;&amp; h. waitStatus != 0 ) unparkSuccessor (h); return true ; } return false ; } protected final boolean tryRelease ( int releases) { int c = getState () - releases; if ( Thread . currentThread () != getExclusiveOwnerThread ()) throw new IllegalMonitorStateException (); boolean free = false ; if (c == 0 ) { free = true ; setExclusiveOwnerThread ( null ); } setState (c); return free; } private void unparkSuccessor ( Node node) { /* * If status is negative (i.e., possibly needing signal) try * to clear in anticipation of signalling. It is OK if this * fails or if status is changed by waiting thread. */ int ws = node. waitStatus ; if (ws &lt; 0 ) node. compareAndSetWaitStatus (ws, 0 ); /* * Thread to unpark is held in successor, which is normally * just the next node. But if cancelled or apparently null, * traverse backwards from tail to find the actual * non-cancelled successor. */ Node s = node. next ; if (s == null || s. waitStatus &gt; 0 ) { s = null ; for ( Node p = tail; p != node &amp;&amp; p != null ; p = p. prev ) if (p. waitStatus &lt;= 0 ) s = p; } if (s != null ) LockSupport . unpark (s. thread ); } unlock的代码特别简单： 每unlock一次state-1 state == 0 时资源成功释放 如果释放成功，唤醒第二个节点 如果第二个节点没引用或者放弃争用，从队尾开始寻找可以唤醒的线程","link":"/2018/07/02/AbstractQueuedSynchronizer解析/"},{"title":"[片段] 使用redis创建简易搜索引擎（核心篇）","text":"支持and查询、多选、多字段排序分页，缺少的功能：or 条件 核心类，有一些测试代码，将就一下。另外需要spring-data-redis 2.0版本以上 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321322323324325326327328329330331332333334335336337338339340341342343344345346347348349350351352353354355356357358359360361362363364365366367368369370371372373374375376377378379380381382package app.pooi.redissearch.search;import app.pooi.redissearch.search.anno.CreateIndex;import app.pooi.redissearch.search.anno.Field;import com.google.common.collect.Lists;import com.google.common.collect.Sets;import lombok.Data;import org.apache.commons.lang3.ArrayUtils;import org.springframework.dao.DataAccessException;import org.springframework.data.redis.connection.RedisZSetCommands;import org.springframework.data.redis.core.*;import org.springframework.data.redis.hash.Jackson2HashMapper;import org.springframework.stereotype.Service;import org.springframework.web.bind.annotation.*;import reactor.util.function.Tuple2;import reactor.util.function.Tuples;import java.util.*;import java.util.concurrent.TimeUnit;import java.util.function.Consumer;import java.util.function.Function;import java.util.regex.Matcher;import java.util.regex.Pattern;import java.util.stream.Collectors;import java.util.stream.Stream;import static app.pooi.redissearch.search.SearchCore.Util.*;@RestController@Servicepublic class SearchCore { private StringRedisTemplate redisTemplate; private Jackson2HashMapper hashMapper = new Jackson2HashMapper(true); @Data private static class Person { private Long id; private String name; private Integer age; private Long ctime; } @PostMapping(\"/person\") @CreateIndex( index = \"person\", documentId = \"#p0.id\", fields = { @Field(propertyName = \"name\", value = \"#p0.name\"), @Field(propertyName = \"age\", value = \"#p0.age\", sort = true), @Field(propertyName = \"ctime\", value = \"#p0.ctime\", sort = true) }) Person addPerson(Person person) { return person; } public SearchCore(StringRedisTemplate redisTemplate) { this.redisTemplate = redisTemplate; } public void indexMeta(String index, Map&lt;String, FieldMeta&gt; fieldMeta) { this.redisTemplate.opsForHash().putAll(genIdxMetaName(index), hashMapper.toHash(fieldMeta)); } @PostMapping(\"/index\") public int indexDocument( final String index, final String field, final String documentId, final String document) { return this.indexDocument(index, field, documentId, document, doc -&gt; Lists.newArrayList(doc.split(\"\"))); } public int indexDocument( final String index, final String field, final String documentId, final String document, final Function&lt;String, List&lt;String&gt;&gt; tokenizer) { final List&lt;String&gt; tokens = tokenizer != null ? tokenizer.apply(document) : Collections.singletonList(document); final String docKey = genDocIdxName(index, documentId); final List&lt;Object&gt; results = redisTemplate.executePipelined(new SessionCallback&lt;Integer&gt;() { @Override public Integer execute(RedisOperations operations) throws DataAccessException { final StringRedisTemplate template = (StringRedisTemplate) operations; final String[] idxs = tokens.stream() .map(word -&gt; genIdxName(index, field, word)) .peek(idx -&gt; ((StringRedisTemplate) operations).opsForSet().add(idx, documentId)) .toArray(String[]::new); template.opsForSet().add(docKey, idxs); return null; } }); return results.size(); } public int indexSortField( final String index, final String field, final String documentId, final Double document) { final String docKey = genDocIdxName(index, documentId); final List&lt;Object&gt; results = redisTemplate.executePipelined(new SessionCallback&lt;Integer&gt;() { @Override public Integer execute(RedisOperations operations) throws DataAccessException { final StringRedisTemplate template = (StringRedisTemplate) operations; final String idxName = genSortIdxName(index, field); template.opsForZSet().add(idxName, documentId, document); template.opsForSet().add(docKey, idxName); return null; } }); return results.size(); } @DeleteMapping(\"/index\") public int deleteDocumentIndex(final String index, final String documentId) { final String docKey = genDocIdxName(index, documentId); final Boolean hasKey = redisTemplate.hasKey(docKey); if (!hasKey) { return 0; } final List&lt;Object&gt; results = redisTemplate.executePipelined(new SessionCallback&lt;Integer&gt;() { @Override public Integer execute(RedisOperations operations) throws DataAccessException { final Set&lt;String&gt; idx = redisTemplate.opsForSet().members(docKey); ((StringRedisTemplate) operations).delete(idx); ((StringRedisTemplate) operations).delete(docKey); return null; } }); return results.size(); } @PatchMapping(\"/index\") public int updateDocumentIndex(final String index, final String field, final String documentId, final String document) { this.deleteDocumentIndex(index, documentId); return this.indexDocument(index, field, documentId, document); } public int updateSortField(final String index, final String field, final String documentId, final Double document) { this.deleteDocumentIndex(index, documentId); return this.indexSortField(index, field, documentId, document); } private Consumer&lt;SetOperations&lt;String, String&gt;&gt; operateAndStore(String method, String key, Collection&lt;String&gt; keys, String destKey) { switch (method) { case \"intersectAndStore\": return (so) -&gt; so.intersectAndStore(key, keys, destKey); case \"unionAndStore\": return (so) -&gt; so.unionAndStore(key, keys, destKey); case \"differenceAndStore\": return (so) -&gt; so.differenceAndStore(key, keys, destKey); default: return so -&gt; { }; } } private Consumer&lt;ZSetOperations&lt;String, String&gt;&gt; zOperateAndStore(String method, String key, Collection&lt;String&gt; keys, String destKey, final RedisZSetCommands.Weights weights) { switch (method) { case \"intersectAndStore\": return (so) -&gt; so.intersectAndStore(key, keys, destKey, RedisZSetCommands.Aggregate.SUM, weights); case \"unionAndStore\": return (so) -&gt; so.unionAndStore(key, keys, destKey, RedisZSetCommands.Aggregate.SUM, weights); default: return so -&gt; { }; } } private String common(String index, String method, List&lt;String&gt; keys, long ttl) { final String destKey = Util.genQueryIdxName(index); redisTemplate.executePipelined(new SessionCallback&lt;String&gt;() { @Override public &lt;K, V&gt; String execute(RedisOperations&lt;K, V&gt; operations) throws DataAccessException { operateAndStore(method, keys.stream().limit(1L).findFirst().get(), keys.stream().skip(1L).collect(Collectors.toList()), destKey) .accept(((StringRedisTemplate) operations).opsForSet()); ((StringRedisTemplate) operations).expire(destKey, ttl, TimeUnit.SECONDS); return null; } }); return destKey; } public String intersect(String index, List&lt;String&gt; keys, long ttl) { return common(index, \"intersectAndStore\", keys, ttl); } public String union(String index, List&lt;String&gt; keys, long ttl) { return common(index, \"unionAndStore\", keys, ttl); } public String diff(String index, List&lt;String&gt; keys, long ttl) { return common(index, \"differenceAndStore\", keys, ttl); } private static Tuple2&lt;Set&lt;Tuple2&lt;String, String&gt;&gt;, Set&lt;Tuple2&lt;String, String&gt;&gt;&gt; parse(String query) { final Pattern pattern = Pattern.compile(\"[+-]?([\\\\w\\\\d]+):(\\\\S+)\"); final Matcher matcher = pattern.matcher(query); Set&lt;Tuple2&lt;String, String&gt;&gt; unwant = Sets.newHashSet(); Set&lt;Tuple2&lt;String, String&gt;&gt; want = Sets.newHashSet(); while (matcher.find()) { String word = matcher.group(); String prefix = null; if (word.length() &gt; 1) { prefix = word.substring(0, 1); } final Tuple2&lt;String, String&gt; t = Tuples.of(matcher.group(1), matcher.group(2)); if (\"-\".equals(prefix)) { unwant.add(t); } else { want.add(t); } } return Tuples.of(want, unwant); } public String query( String index, String query) { final Tuple2&lt;Set&lt;Tuple2&lt;String, String&gt;&gt;, Set&lt;Tuple2&lt;String, String&gt;&gt;&gt; parseResult = parse(query); final Set&lt;Tuple2&lt;String, String&gt;&gt; want = parseResult.getT1(); final Set&lt;Tuple2&lt;String, String&gt;&gt; unwant = parseResult.getT2(); if (want.isEmpty()) { return \"\"; } final Map&lt;String, FieldMeta&gt; entries = (Map&lt;String, FieldMeta&gt;) hashMapper.fromHash(redisTemplate.&lt;String, Object&gt;opsForHash().entries(genIdxMetaName(index))); // union final List&lt;Tuple2&lt;String, String&gt;&gt; unionFields = want.stream() .filter(w -&gt; w.getT2().contains(\",\")) .filter(w -&gt; \"true\".equals(entries.get(w.getT1()).getSort())) .collect(Collectors.toList()); final List&lt;String&gt; unionIdx = unionFields.stream() .flatMap(w -&gt; Arrays.stream(w.getT2().split(\",\")).map(value -&gt; Tuples.of(w.getT1(), value))) .map(w -&gt; genIdxName(index, w.getT1(), w.getT2())) .collect(Collectors.toList()); final String unionResultId = unionIdx.isEmpty() ? \"\" : this.union(index, unionIdx, 30L); want.removeAll(unionFields); // intersect final List&lt;String&gt; intersectIdx = want.stream() .flatMap(t -&gt; { if (\"true\".equals(entries.get(t.getT1()).getSort())) return Stream.of(t); return Arrays.stream(t.getT2().split(\"\")).map(value -&gt; Tuples.of(t.getT1(), value)); }) .map(w -&gt; genIdxName(index, w.getT1(), w.getT2())) .collect(Collectors.toList()); if (!unionResultId.isEmpty()) intersectIdx.add(unionResultId); String intersectResult = this.intersect(index, intersectIdx, 30L); // diff return unwant.isEmpty() ? intersectResult : this.diff(index, Stream.concat(Stream.of(intersectResult), unwant.stream().map(w -&gt; genIdxName(index, w.getT1(), w.getT2()))).collect(Collectors.toList()), 30L); } @GetMapping(\"/query/{index}\") public Set&lt;String&gt; queryAndSort( @PathVariable(\"index\") String index, @RequestParam(\"param\") String query, @RequestParam(\"sort\") String sort, Integer start, Integer stop ) { final String[] sorts = sort.split(\" \"); final Map&lt;String, Integer&gt; map = Arrays.stream(sorts).collect( Collectors.toMap(f -&gt; { if (f.startsWith(\"+\") || f.startsWith(\"-\")) { f = f.substring(1); } return genSortIdxName(\"person\", f); }, field -&gt; field.startsWith(\"-\") ? -1 : 1) ); final int[] weights = map.values() .stream() .mapToInt(Integer::intValue) .toArray();// if (!sort.startsWith(\"+\") &amp;&amp; !sort.startsWith(\"-\")) {// sort = \"+\" + sort;// }// boolean desc = sort.startsWith(\"-\");// sort = sort.substring(1); String queryId = this.query(index, query); Long size; if (queryId.length() == 0 || (size = redisTemplate.opsForSet().size(queryId)) == null || size == 0) { return Collections.emptySet(); } final String resultId = genQueryIdxName(index);// String sortField = sort; redisTemplate.executePipelined(new SessionCallback&lt;Object&gt;() { @Override public &lt;K, V&gt; Object execute(RedisOperations&lt;K, V&gt; operations) throws DataAccessException { final StringRedisTemplate template = (StringRedisTemplate) operations;// template.opsForZSet().intersectAndStore(genSortIdxName(index, sortField), queryId, resultId); SearchCore.this.zOperateAndStore(\"intersectAndStore\", map.keySet().stream().limit(1L).findFirst().get(), Stream.concat(map.keySet().stream().skip(1L), Stream.of(queryId)).collect(Collectors.toList()), resultId, RedisZSetCommands.Weights.of(ArrayUtils.add(weights, 0))).accept(template.opsForZSet());// template.opsForZSet().size(resultId); template.expire(resultId, 30L, TimeUnit.SECONDS); return null; } }); // sort return redisTemplate.opsForZSet().range(resultId, start, stop); } static class Util { private Util() { } static String genIdxMetaName(String index) { return String.format(\"meta:idx:%s\", index); } static String genIdxName(String index, String field, String value) { return String.format(\"idx:%s:%s:%s\", index, field, value); } static String genSortIdxName(String index, String field) { return String.format(\"idx:%s:%s\", index, field); } static String genQueryIdxName(String index) { return String.format(\"idx:%s:q:%s\", index, UUID.randomUUID().toString()); } static String genDocIdxName(String index, String documentId) { return String.format(\"doc:%s:%s\", index, documentId); } }} 辅助类 123456789101112131415161718import lombok.Data;@Datapublic class FieldMeta { private String sort = \"false\"; private String splitFun = \"\"; public FieldMeta() { } public FieldMeta(boolean sort) { this.sort = Boolean.toString(sort); }} 做一个轻量级的搜索还是可以的。","link":"/2019/03/11/[片段]使用redis创建简易搜索引擎（核心篇）/"},{"title":"LeetCode二叉树基础算法","text":"树的高度104. Maximum Depth of Binary Tree (Easy) 递归计算二叉树左右两边深度，取最大值。 12345678910111213141516171819/** * Definition for a binary tree node. * public class TreeNode { * int val; * TreeNode left; * TreeNode right; * TreeNode(int x) { val = x; } * } */class Solution { public int maxDepth(TreeNode root) { if (root == null) return 0; int left = maxDepth(root.left); int right = maxDepth(root.right); return Math.max(left, right) + 1; }} 平衡树110. Balanced Binary Tree (Easy) 递归遍历二叉树左右子树深度 1234567891011121314151617181920212223242526/** * Definition for a binary tree node. * public class TreeNode { * int val; * TreeNode left; * TreeNode right; * TreeNode(int x) { val = x; } * } */class Solution { private boolean balance = true; public boolean isBalanced(TreeNode root) { visitTree(root); return balance; } private int visitTree(TreeNode root) { if (root == null) return 0; int left = visitTree(root.left); int right = visitTree(root.right); if (Math.abs(left - right) &gt; 1 ) this.balance = false; return Math.max(left, right) + 1; }} 两节点的最长路径543. Diameter of Binary Tree (Easy) 递归遍历二叉树左右子树深度， 路径就是两边子树深度之和 1234567891011121314151617181920212223242526/** * Definition for a binary tree node. * public class TreeNode { * int val; * TreeNode left; * TreeNode right; * TreeNode(int x) { val = x; } * } */class Solution { private int max; public int diameterOfBinaryTree(TreeNode root) { deep(root); return max; } private int deep(TreeNode root) { if (root == null) return 0; int left = deep(root.left); int right = deep(root.right); max = Math.max(max,left+right); return Math.max(left, right) + 1; }} 翻转树226. Invert Binary Tree (Easy) 递归交换左右子树的引用 123456789101112131415161718/** * Definition for a binary tree node. * public class TreeNode { * int val; * TreeNode left; * TreeNode right; * TreeNode(int x) { val = x; } * } */class Solution { public TreeNode invertTree(TreeNode root) { if (root == null) return root; TreeNode right =root.right; root.right = invertTree(root.left); root.left = invertTree(right); return root; }} 归并两棵树617. Merge Two Binary Trees (Easy) 递归时如果其中一个节点是空，可以直接复用该节点。如果新建节点，需要拷贝节点的左右子树引用，递归时会用到。 1234567891011121314151617181920/** * Definition for a binary tree node. * public class TreeNode { * int val; * TreeNode left; * TreeNode right; * TreeNode(int x) { val = x; } * } */class Solution { public TreeNode mergeTrees(TreeNode t1, TreeNode t2) { if (t1 == null &amp;&amp; t2 == null ) return null; if (t1 == null) return t2; if (t2 == null) return t1; TreeNode root = new TreeNode(t1.val + t2.val); root.left = mergeTrees(t1.left, t2.left); root.right = mergeTrees(t1.right, t2.right); return root; }} 判断路径和是否等于一个数Leetcode : 112. Path Sum (Easy) 递归查询子树和是否等于目标和 12345678910111213141516/** * Definition for a binary tree node. * public class TreeNode { * int val; * TreeNode left; * TreeNode right; * TreeNode(int x) { val = x; } * } */class Solution { public boolean hasPathSum(TreeNode root, int sum) { if (root == null) return false; if (root.val == sum &amp;&amp; root.left == null &amp;&amp; root.right == null) return true; return hasPathSum(root.left, sum - root.val) || hasPathSum(root.right, sum - root.val); }} 统计路径和等于一个数的路径数量437. Path Sum III (Easy) 双层递归 以当前节点为起点统计路径和 当前节点以下节点为起点统计路径和 以root为根节点的路径数量= 以root为起点统计路径和+root左节点为起点统计路径和+root右节点为起点统计路径和 123456789101112131415161718192021222324/** * Definition for a binary tree node. * public class TreeNode { * int val; * TreeNode left; * TreeNode right; * TreeNode(int x) { val = x; } * } */class Solution { public int pathSum(TreeNode root, int sum) { if (root == null) return 0; //结果数 等于 以当前root为父节点和 root以下为父节点结果数之和 return sum(root, sum) + pathSum(root.left, sum) + pathSum(root.right, sum); } // 计算以当前node为父节点能都多少路径数 private int sum(TreeNode node, int sum) { if (node == null) return 0; int count = 0; if (node.val == sum) count++; count += sum(node.left, sum - node.val) + sum(node.right, sum - node.val); return count; }} 子树572. Subtree of Another Tree (Easy) 12345678910111213141516171819202122/** * Definition for a binary tree node. * public class TreeNode { * int val; * TreeNode left; * TreeNode right; * TreeNode(int x) { val = x; } * } */class Solution { public boolean isSubtree(TreeNode s, TreeNode t) { if (s == null) return false; return isSubRoot(s, t) || isSubtree(s.left, t) || isSubtree(s.right, t); } public boolean isSubRoot(TreeNode node, TreeNode t) { if (node == null &amp;&amp; t == null) return true; if (node == null || t == null) return false; if (node.val != t.val) return false; return isSubRoot(node.left, t.left) &amp;&amp; isSubRoot(node.right, t.right); }} 树的对称101. Symmetric Tree (Easy) 12345678910111213141516171819202122/** * Definition for a binary tree node. * public class TreeNode { * int val; * TreeNode left; * TreeNode right; * TreeNode(int x) { val = x; } * } */class Solution { public boolean isSymmetric(TreeNode root) { if (root == null) return true; return isSymmetric(root.left, root.right); } public boolean isSymmetric(TreeNode left, TreeNode right) { if (left == null &amp;&amp; right == null) return true; if (left == null || right == null) return false; if (left.val != right.val) return false; return isSymmetric(left.left, right.right) &amp;&amp; isSymmetric(left.right, right.left); }} 最小路径111. Minimum Depth of Binary Tree (Easy) 和最大路径类似 123456789101112131415161718/** * Definition for a binary tree node. * public class TreeNode { * int val; * TreeNode left; * TreeNode right; * TreeNode(int x) { val = x; } * } */class Solution { public int minDepth(TreeNode root) { if (root == null) return 0; int left = minDepth(root.left); int right = minDepth(root.right); if (left == 0 || right == 0) return left + right + 1; return Math.min(left, right) + 1; }} 统计左叶子节点的和404. Sum of Left Leaves (Easy) 123456789101112131415161718/** * Definition for a binary tree node. * public class TreeNode { * int val; * TreeNode left; * TreeNode right; * TreeNode(int x) { val = x; } * } */class Solution { public int sumOfLeftLeaves(TreeNode root) { if (root == null) return 0; if (root.left != null &amp;&amp; root.left.left == null &amp;&amp; root.left.right == null) return root.left.val + sumOfLeftLeaves(root.right); return sumOfLeftLeaves(root.left) + sumOfLeftLeaves(root.right); }}` 相同节点值的最大路径长度687. Longest Univalue Path (Easy) 递归查找左右子树相同节点值最大路径，最大路径的计算：如果相等路径+1，如果不相等置为0。 12345678910111213141516171819202122232425262728/** * Definition for a binary tree node. * public class TreeNode { * int val; * TreeNode left; * TreeNode right; * TreeNode(int x) { val = x; } * } */class Solution { private int path = 0; public int longestUnivaluePath(TreeNode root) { visit(root); return path; } private int visit(TreeNode root) { if (root == null) return 0; int left = visit(root.left); int right = visit(root.right); left = (root.left != null &amp;&amp; root.val == root.left.val) ? left + 1 : 0; right = (root.right != null &amp;&amp; root.val == root.right.val)? right + 1 : 0; path = Math.max(path, left+right); return Math.max(left, right ); }} 间隔遍历337. House Robber III (Medium) 递归查询两种情况 如果从当前节点开始 从当前节点的子节点开始 1234567891011121314151617181920/** * Definition for a binary tree node. * public class TreeNode { * int val; * TreeNode left; * TreeNode right; * TreeNode(int x) { val = x; } * } */class Solution { public int rob(TreeNode root) { if (root == null) return 0; int val1 = root.val, val2 = 0; if (root.left != null) val1+= rob(root.left.left) + rob(root.left.right); if (root.right != null) val1+= rob(root.right.left) + rob(root.right.right); val2 = rob(root.left) + rob(root.right); return Math.max(val1, val2); }} 找出二叉树中第二小的节点Second Minimum Node In a Binary Tree (Easy) 第二小节点在子树节点上，如果子树值与根节点相等，继续向下查找 123456789101112131415161718192021/** * Definition for a binary tree node. * public class TreeNode { * int val; * TreeNode left; * TreeNode right; * TreeNode(int x) { val = x; } * } */class Solution { public int findSecondMinimumValue(TreeNode root) { if (root == null) return -1; if (root.left == null) return -1; int left = root.left.val, right = root.right.val; if (root.val == root.left.val) left = findSecondMinimumValue(root.left); if (root.val == root.right.val) right = findSecondMinimumValue(root.right); if (left != -1 &amp;&amp; right != -1) return Math.min(left, right); if (left &gt; -1) return left; return right; }} 二叉树的层平均值637. Average of Levels in Binary Tree (Easy) BFS 123456789101112131415161718192021222324252627282930/** * Definition for a binary tree node. * public class TreeNode { * int val; * TreeNode left; * TreeNode right; * TreeNode(int x) { val = x; } * } */class Solution { public List&lt;Double&gt; averageOfLevels(TreeNode root) { List&lt;Double&gt; ret = new ArrayList&lt;&gt;(); if (root == null) return ret; Queue&lt;TreeNode&gt; queue = new LinkedList&lt;&gt;(); queue.add(root); while(!queue.isEmpty()) { int count = queue.size(); double sum = 0d; for(int i = 0; i &lt; count; i++) { TreeNode node = queue.poll(); sum+= node.val; if (node.left != null) queue.add(node.left); if (node.right != null) queue.add(node.right); } ret.add(sum/count); } return ret; }} 找树左下角的值513. Find Bottom Left Tree Value (Easy) DFS 123456789101112131415161718192021222324252627282930/** * Definition for a binary tree node. * public class TreeNode { * int val; * TreeNode left; * TreeNode right; * TreeNode(int x) { val = x; } * } */class Solution { private int val = 0; public int findBottomLeftValue(TreeNode root) { if (root == null) return val; Queue&lt;TreeNode&gt; queue = new LinkedList&lt;&gt;(); queue.add(root); while(!queue.isEmpty()) { // 这一行的数量 int count = queue.size(); for (int i = 0; i &lt; count; i++) { TreeNode node = queue.poll(); if(i == 0) val = node.val; if (node.left != null) queue.add(node.left); if (node.right != null) queue.add(node.right); } } return val; }} 非递归实现二叉树的后序遍历入栈条件： 未访问过该节点出栈条件： 访问过该节点 12345678910111213141516171819202122232425262728293031323334353637383940414243/** * Definition for a binary tree node. * public class TreeNode { * int val; * TreeNode left; * TreeNode right; * TreeNode(int x) { val = x; } * } */class Solution { private List&lt;Integer&gt; res = new LinkedList&lt;&gt;(); private Stack&lt;TreeNode&gt; stack = new Stack&lt;&gt;(); private Set&lt;TreeNode&gt; visited = new HashSet&lt;&gt;(); public List&lt;Integer&gt; postorderTraversal(TreeNode root) { if (root == null) return res; stack.push(root); while (!stack.isEmpty()) { TreeNode node = stack.peek(); if ((node.left == null &amp;&amp; node.right == null) || visited.contains(node)) { TreeNode i = stack.pop(); res.add(i.val); } else { visited.add(node); if (node.right != null) stack.push(node.right); if (node.left != null) stack.push(node.left); } } return res; } private void visit(TreeNode root) { if(root == null) return; visit(root.left); visit(root.right); res.add(root.val); }} 非递归实现二叉树的前序遍历入栈条件： 无出栈条件： 直接出栈 1234567891011121314151617181920212223242526/** * Definition for a binary tree node. * public class TreeNode { * int val; * TreeNode left; * TreeNode right; * TreeNode(int x) { val = x; } * } */class Solution { private List&lt;Integer&gt; res = new LinkedList&lt;&gt;(); private Stack&lt;TreeNode&gt; stack = new Stack&lt;&gt;(); public List&lt;Integer&gt; preorderTraversal(TreeNode root) { if (root == null) return res; stack.push(root); while (!stack.isEmpty()) { TreeNode node = stack.pop(); res.add(node.val); if (node.right != null) stack.push(node.right); if (node.left != null) stack.push(node.left); } return res; }} 非递归实现二叉树的中序遍历入栈条件： 未访问过该节点出栈条件： 访问过该节点入栈顺序: right -&gt; middle -&gt; left 12345678910111213141516171819202122232425262728293031323334353637/** * Definition for a binary tree node. * public class TreeNode { * int val; * TreeNode left; * TreeNode right; * TreeNode(int x) { val = x; } * } */class Solution { private List&lt;Integer&gt; res = new LinkedList&lt;&gt;(); private Stack&lt;TreeNode&gt; stack = new Stack&lt;&gt;(); private Set&lt;TreeNode&gt; visited = new HashSet&lt;&gt;(); public List&lt;Integer&gt; inorderTraversal(TreeNode root) { if (root == null) return res; push(root); while (!stack.isEmpty()) { TreeNode node = stack.pop(); if ((node.left == null &amp;&amp; node.right == null) || visited.contains(node)) { res.add(node.val); } else { push(node); } } return res; } private void push(TreeNode root) { if (root == null) return; visited.add(root); if (root.right != null) stack.push(root.right); stack.push(root); if (root.left != null) stack.push(root.left); }}","link":"/2019/06/15/2019-06-15树算法/"}],"tags":[{"name":"工作","slug":"工作","link":"/tags/工作/"},{"name":"代码","slug":"代码","link":"/tags/代码/"},{"name":"算法","slug":"算法","link":"/tags/算法/"},{"name":"读书","slug":"读书","link":"/tags/读书/"},{"name":"java","slug":"java","link":"/tags/java/"},{"name":"感悟","slug":"感悟","link":"/tags/感悟/"},{"name":"linux","slug":"linux","link":"/tags/linux/"},{"name":"mybatis","slug":"mybatis","link":"/tags/mybatis/"},{"name":"spring","slug":"spring","link":"/tags/spring/"},{"name":"aop","slug":"aop","link":"/tags/aop/"},{"name":"团队","slug":"团队","link":"/tags/团队/"},{"name":"搜索","slug":"搜索","link":"/tags/搜索/"},{"name":"redis","slug":"redis","link":"/tags/redis/"}],"categories":[{"name":"工作","slug":"工作","link":"/categories/工作/"},{"name":"算法","slug":"算法","link":"/categories/算法/"},{"name":"读书","slug":"读书","link":"/categories/读书/"},{"name":"挑战","slug":"工作/挑战","link":"/categories/工作/挑战/"},{"name":"Java基础","slug":"Java基础","link":"/categories/Java基础/"},{"name":"字符串","slug":"算法/字符串","link":"/categories/算法/字符串/"},{"name":"总结","slug":"工作/总结","link":"/categories/工作/总结/"},{"name":"Java框架","slug":"Java框架","link":"/categories/Java框架/"},{"name":"中间件","slug":"中间件","link":"/categories/中间件/"},{"name":"Mybatis","slug":"Java框架/Mybatis","link":"/categories/Java框架/Mybatis/"},{"name":"Spring","slug":"Java框架/Spring","link":"/categories/Java框架/Spring/"},{"name":"resilience4j","slug":"中间件/resilience4j","link":"/categories/中间件/resilience4j/"},{"name":"二叉树","slug":"算法/二叉树","link":"/categories/算法/二叉树/"},{"name":"Java并发","slug":"Java并发","link":"/categories/Java并发/"},{"name":"搜索引擎","slug":"中间件/搜索引擎","link":"/categories/中间件/搜索引擎/"}]}